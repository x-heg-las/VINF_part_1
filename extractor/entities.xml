<?xml version='1.0' encoding='utf-8'?>
<root><repository><username>rust-lang</username><reponame>rust</reponame><readme>               The Rust Programming Language Quick Start Installing from Source Dependencies Building on a Unix-like system Build steps Configure and Make Building on Windows MinGW MSVC Specifying an ABI Building Documentation Notes Getting Help Contributing License Trademark      README.md     The Rust Programming Language  This is the main source code repository for Rust. It contains the compiler, standard library, and documentation. Note: this README is for users rather than contributors. If you wish to contribute to the compiler, you should read CONTRIBUTING.md instead. Quick Start Read "Installation" from The Book. Installing from Source The Rust build system uses a Python script called x.py to build the compiler, which manages the bootstrapping process. It lives at the root of the project. It also uses a file named config.toml to determine various configuration settings for the build. You can see a full list of options in config.example.toml. The x.py command can be run directly on most Unix systems in the following format: ./x.py &lt;subcommand&gt; [flags] This is how the documentation and examples assume you are running x.py. See the rustc dev guide if this does not work on your platform. More information about x.py can be found by running it with the --help flag or reading the rustc dev guide. Dependencies Make sure you have installed the dependencies:  python 3 or 2.7 git A C compiler (when building for the host, cc is enough; cross-compiling may need additional compilers) curl (not needed on Windows) pkg-config if you are compiling on Linux and targeting Linux libiconv (already included with glibc on Debian-based distros)  To build Cargo, you'll also need OpenSSL (libssl-dev or openssl-devel on most Unix distros). If building LLVM from source, you'll need additional tools:  g++, clang++, or MSVC with versions listed on LLVM's documentation ninja, or GNU make 3.81 or later (Ninja is recommended, especially on Windows) cmake 3.13.4 or later libstdc++-static may be required on some Linux distributions such as Fedora and Ubuntu  On tier 1 or tier 2 with host tools platforms, you can also choose to download LLVM by setting llvm.download-ci-llvm = true. Otherwise, you'll need LLVM installed and llvm-config in your path. See the rustc-dev-guide for more info. Building on a Unix-like system Build steps   Clone the source with git: git clone https://github.com/rust-lang/rust.git cd rust     Configure the build settings: ./configure If you plan to use x.py install to create an installation, it is recommended that you set the prefix value in the [install] section to a directory: ./configure --set install.prefix=&lt;path&gt;   Build and install: ./x.py build &amp;&amp; ./x.py install When complete, ./x.py install will place several programs into $PREFIX/bin: rustc, the Rust compiler, and rustdoc, the API-documentation tool. By default, it will also include Cargo, Rust's package manager. You can disable this behavior by passing --set build.extended=false to ./configure.   Configure and Make This project provides a configure script and makefile (the latter of which just invokes x.py). ./configure is the recommended way to programmatically generate a config.toml. make is not recommended (we suggest using x.py directly), but it is supported and we try not to break it unnecessarily. ./configure make &amp;&amp; sudo make install configure generates a config.toml which can also be used with normal x.py invocations. Building on Windows On Windows, we suggest using winget to install dependencies by running the following in a terminal: winget install -e Python.Python.3 winget install -e Kitware.CMake winget install -e Git.Git Then edit your system's PATH variable and add: C:\Program Files\CMake\bin. See this guide on editing the system PATH from the Java documentation. There are two prominent ABIs in use on Windows: the native (MSVC) ABI used by Visual Studio and the GNU ABI used by the GCC toolchain. Which version of Rust you need depends largely on what C/C++ libraries you want to interoperate with. Use the MSVC build of Rust to interop with software produced by Visual Studio and the GNU build to interop with GNU software built using the MinGW/MSYS2 toolchain. MinGW MSYS2 can be used to easily build Rust on Windows:   Download the latest MSYS2 installer and go through the installer.   Run mingw32_shell.bat or mingw64_shell.bat from the MSYS2 installation directory (e.g. C:\msys64), depending on whether you want 32-bit or 64-bit Rust. (As of the latest version of MSYS2 you have to run msys2_shell.cmd -mingw32 or msys2_shell.cmd -mingw64 from the command line instead.)   From this terminal, install the required tools: # Update package mirrors (may be needed if you have a fresh install of MSYS2) pacman -Sy pacman-mirrors  # Install build tools needed for Rust. If you're building a 32-bit compiler, # then replace "x86_64" below with "i686". If you've already got Git, Python, # or CMake installed and in PATH you can remove them from this list. # Note that it is important that you do **not** use the 'python2', 'cmake', # and 'ninja' packages from the 'msys2' subsystem. # The build has historically been known to fail with these packages. pacman -S git \             make \             diffutils \             tar \             mingw-w64-x86_64-python \             mingw-w64-x86_64-cmake \             mingw-w64-x86_64-gcc \             mingw-w64-x86_64-ninja   Navigate to Rust's source code (or clone it), then build it: python x.py setup user &amp;&amp; python x.py build &amp;&amp; python x.py install   MSVC MSVC builds of Rust additionally require an installation of Visual Studio 2017 (or later) so rustc can use its linker. The simplest way is to get Visual Studio, check the "C++ build tools" and "Windows 10 SDK" workload. (If you're installing CMake yourself, be careful that "C++ CMake tools for Windows" doesn't get included under "Individual components".) With these dependencies installed, you can build the compiler in a cmd.exe shell with: python x.py setup user python x.py build Right now, building Rust only works with some known versions of Visual Studio. If you have a more recent version installed and the build system doesn't understand, you may need to force rustbuild to use an older version. This can be done by manually calling the appropriate vcvars file before running the bootstrap. CALL "C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\vcvars64.bat" python x.py build Specifying an ABI Each specific ABI can also be used from either environment (for example, using the GNU ABI in PowerShell) by using an explicit build triple. The available Windows build triples are:  GNU ABI (using GCC)  i686-pc-windows-gnu x86_64-pc-windows-gnu   The MSVC ABI  i686-pc-windows-msvc x86_64-pc-windows-msvc    The build triple can be specified by either specifying --build=&lt;triple&gt; when invoking x.py commands, or by creating a config.toml file (as described in Building on a Unix-like system), and passing --set build.build=&lt;triple&gt; to ./configure. Building Documentation If you'd like to build the documentation, it's almost the same: ./x.py doc The generated documentation will appear under doc in the build directory for the ABI used. That is, if the ABI was x86_64-pc-windows-msvc, the directory will be build\x86_64-pc-windows-msvc\doc. Notes Since the Rust compiler is written in Rust, it must be built by a precompiled "snapshot" version of itself (made in an earlier stage of development). As such, source builds require an Internet connection to fetch snapshots, and an OS that can execute the available snapshot binaries. See https://doc.rust-lang.org/nightly/rustc/platform-support.html for a list of supported platforms. Only "host tools" platforms have a pre-compiled snapshot binary available; to compile for a platform without host tools you must cross-compile. You may find that other platforms work, but these are our officially supported build environments that are most likely to work. Getting Help See https://www.rust-lang.org/community for a list of chat platforms and forums. Contributing See CONTRIBUTING.md. License Rust is primarily distributed under the terms of both the MIT license and the Apache License (Version 2.0), with portions covered by various BSD-like licenses. See LICENSE-APACHE, LICENSE-MIT, and COPYRIGHT for details. Trademark The Rust Foundation owns and protects the Rust and Cargo trademarks and logos (the "Rust Trademarks"). If you want to use these names or brands, please read the media guide. Third-party logos may be subject to third-party copyrights and trademarks. See Licenses for details.   </readme><commitcount>236,484</commitcount><languages>Rust 96.9%	Shell 0.5%	JavaScript 0.5%	Fluent 0.4%	HTML 0.3%	Python 0.3%</languages><tags>language	rust	compiler	hacktoberfest</tags><about>Empowering everyone to build reliable and efficient software.</about><starcount>86.3k</starcount><watchcount>0</watchcount></repository><repository><username>ByteByteGoHq</username><reponame>system-design-101</reponame><readme>               System Design 101 Table of Contents Communication protocols REST API vs. GraphQL How does gRPC work? What is a webhook? How to improve API performance? HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) SOAP vs REST vs GraphQL vs RPC Code First vs. API First HTTP status codes What does API gateway do? How do we design effective and safe APIs? TCP/IP encapsulation Why is Nginx called a “reverse” proxy? What are the common load-balancing algorithms? URL, URI, URN - Do you know the differences? CI/CD CI/CD Pipeline Explained in Simple Terms Netflix Tech Stack (CI/CD Pipeline) Architecture patterns MVC, MVP, MVVM, MVVM-C, and VIPER 18 Key Design Patterns Every Developer Should Know Database A nice cheat sheet of different databases in cloud services 8 Data Structures That Power Your Databases How is an SQL statement executed in the database? CAP theorem Types of Memory and Storage Visualizing a SQL query SQL language Cache Data is cached everywhere Why is Redis so fast? How can Redis be used? Top caching strategies Microservice architecture What does a typical microservice architecture look like? Microservice Best Practices What tech stack is commonly used for microservices? Why is Kafka fast Payment systems How to learn payment systems? Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? How does VISA work when we swipe a credit card at a merchant’s shop? Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India DevOps DevOps vs. SRE vs. Platform Engineering. What is the difference? What is k8s (Kubernetes)? Docker vs. Kubernetes. Which one should we use? How does Docker work? GIT How Git Commands work How does Git Work? Git merge vs. Git rebase Cloud Services A nice cheat sheet of different cloud services (2023 edition) What is cloud native? Developer productivity tools Visualize JSON files Automatically turn code into architecture diagrams Linux Linux file system explained 18 Most-used Linux Commands You Should Know Security How does HTTPS work? Oauth 2.0 Explained With Simple Terms. Top 4 Forms of Authentication Mechanisms Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? How to store passwords safely in the database and how to validate a password? Explaining JSON Web Token (JWT) to a 10 year old Kid How does Google Authenticator (or other types of 2-factor authenticators) work? Real World Case Studies Netflix's Tech Stack Twitter Architecture 2022 Evolution of Airbnb’s microservice architecture over the past 15 years Monorepo vs. Microrepo. How will you design the Stack Overflow website? Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? How does Disney Hotstar capture 5 Billion Emojis during a tournament? How Discord Stores Trillions Of Messages How do video live streamings work on YouTube, TikTok live, or Twitch? License      README.md            【        👨🏻‍💻 YouTube    |         📮 Newsletter    】  System Design 101 Explain complex systems using visuals and simple terms. Whether you're preparing for a System Design Interview or you simply want to understand how systems work beneath the surface, we hope this repository will help you achieve that. Table of Contents  Communication protocols  REST API vs. GraphQL How does gRPC work? What is a webhook? How to improve API performance? HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) SOAP vs REST vs GraphQL vs RPC Code First vs. API First HTTP status codes What does API gateway do? How do we design effective and safe APIs? TCP/IP encapsulation Why is Nginx called a “reverse” proxy? What are the common load-balancing algorithms? URL, URI, URN - Do you know the differences?   CI/CD  CI/CD Pipeline Explained in Simple Terms Netflix Tech Stack (CI/CD Pipeline)   Architecture patterns  MVC, MVP, MVVM, MVVM-C, and VIPER 18 Key Design Patterns Every Developer Should Know   Database  A nice cheat sheet of different databases in cloud services 8 Data Structures That Power Your Databases How is an SQL statement executed in the database? CAP theorem Types of Memory and Storage Visualizing a SQL query SQL language   Cache  Data is cached everywhere Why is Redis so fast? How can Redis be used? Top caching strategies   Microservice architecture  What does a typical microservice architecture look like? Microservice Best Practices What tech stack is commonly used for microservices? Why is Kafka fast   Payment systems  How to learn payment systems? Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? How does VISA work when we swipe a credit card at a merchant’s shop? Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India   DevOps  DevOps vs. SRE vs. Platform Engineering. What is the difference? What is k8s (Kubernetes)? Docker vs. Kubernetes. Which one should we use? How does Docker work?   GIT  How Git Commands work How does Git Work? Git merge vs. Git rebase   Cloud Services  A nice cheat sheet of different cloud services (2023 edition) What is cloud native?   Developer productivity tools  Visualize JSON files Automatically turn code into architecture diagrams   Linux  Linux file system explained 18 Most-used Linux Commands You Should Know   Security  How does HTTPS work? Oauth 2.0 Explained With Simple Terms. Top 4 Forms of Authentication Mechanisms Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? How to store passwords safely in the database and how to validate a password? Explaining JSON Web Token (JWT) to a 10 year old Kid How does Google Authenticator (or other types of 2-factor authenticators) work?   Real World Case Studies  Netflix's Tech Stack Twitter Architecture 2022 Evolution of Airbnb’s microservice architecture over the past 15 years Monorepo vs. Microrepo. How will you design the Stack Overflow website? Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? How does Disney Hotstar capture 5 Billion Emojis during a tournament? How Discord Stores Trillions Of Messages How do video live streamings work on YouTube, TikTok live, or Twitch?    Communication protocols Architecture styles define how different components of an application programming interface (API) interact with one another. As a result, they ensure efficiency, reliability, and ease of integration with other systems by providing a standard approach to designing and building APIs. Here are the most used styles:      SOAP:  Mature, comprehensive, XML-based Best for enterprise applications    RESTful:  Popular, easy-to-implement, HTTP methods  Ideal for web services    GraphQL:  Query language, request specific data  Reduces network overhead, faster responses    gRPC:  Modern, high-performance, Protocol Buffers  Suitable for microservices architectures    WebSocket:  Real-time, bidirectional, persistent connections  Perfect for low-latency data exchange    Webhook:  Event-driven, HTTP callbacks, asynchronous  Notifies systems when events occur   REST API vs. GraphQL The diagram below shows a quick comparison between REST and GraphQL.      GraphQL is a query language for APIs developed by Meta. It provides a complete description of the data in the API and gives clients the power to ask for exactly what they need.   GraphQL servers sit in between the client and the backend services. GraphQL can aggregate multiple REST requests into one query. GraphQL server organizes the resources in a graph.   GraphQL supports queries, mutations (applying data modifications to resources), and subscriptions (receiving notifications on schema modifications).   How does gRPC work?    RPC (Remote Procedure Call) is called “remote” because it enables communications between remote services when services are deployed to different servers under microservice architecture. From the user’s point of view, it acts like a local function call. The diagram below illustrates the overall data flow for gRPC. Step 1: A REST call is made from the client. The request body is usually in JSON format. Steps 2 - 4: The order service (gRPC client) receives the REST call, transforms it, and makes an RPC call to the payment service. gPRC encodes the client stub into a binary format and sends it to the low-level transport layer. Step 5: gRPC sends the packets over the network via HTTP2. Because of binary encoding and network optimizations, gRPC is said to be 5X faster than JSON. Steps 6 - 8: The payment service (gRPC server) receives the packets from the network, decodes them, and invokes the server application. Steps 9 - 11: The result is returned from the server application, and gets encoded and sent to the transport layer. Steps 12 - 14: The order service receives the packets, decodes them, and sends the result to the client application. What is a webhook? The diagram below shows a comparison between polling and Webhook.     Assume we run an eCommerce website. The clients send orders to the order service via the API gateway, which goes to the payment service for payment transactions. The payment service then talks to an external payment service provider (PSP) to complete the transactions.  There are two ways to handle communications with the external PSP.  1. Short polling  After sending the payment request to the PSP, the payment service keeps asking the PSP about the payment status. After several rounds, the PSP finally returns with the status.  Short polling has two drawbacks:   Constant polling of the status requires resources from the payment service.  The External service communicates directly with the payment service, creating security vulnerabilities.   2. Webhook  We can register a webhook with the external service. It means: call me back at a certain URL when you have updates on the request. When the PSP has completed the processing, it will invoke the HTTP request to update the payment status. In this way, the programming paradigm is changed, and the payment service doesn’t need to waste resources to poll the payment status anymore. What if the PSP never calls back? We can set up a housekeeping job to check payment status every hour. Webhooks are often referred to as reverse APIs or push APIs because the server sends HTTP requests to the client. We need to pay attention to 3 things when using a webhook:  We need to design a proper API for the external service to call. We need to set up proper rules in the API gateway for security reasons. We need to register the correct URL at the external service.  How to improve API performance? The diagram below shows 5 common tricks to improve API performance.    Pagination This is a common optimization when the size of the result is large. The results are streaming back to the client to improve the service responsiveness. Asynchronous Logging Synchronous logging deals with the disk for every call and can slow down the system. Asynchronous logging sends logs to a lock-free buffer first and immediately returns. The logs will be flushed to the disk periodically. This significantly reduces the I/O overhead. Caching We can cache frequently accessed data into a cache. The client can query the cache first instead of visiting the database directly. If there is a cache miss, the client can query from the database. Caches like Redis store data in memory, so the data access is much faster than the database. Payload Compression The requests and responses can be compressed using gzip etc so that the transmitted data size is much smaller. This speeds up the upload and download. Connection Pool When accessing resources, we often need to load data from the database. Opening the closing db connections adds significant overhead. So we should connect to the db via a pool of open connections. The connection pool is responsible for managing the connection lifecycle. HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) What problem does each generation of HTTP solve? The diagram below illustrates the key features.      HTTP 1.0 was finalized and fully documented in 1996. Every request to the same server requires a separate TCP connection.   HTTP 1.1 was published in 1997. A TCP connection can be left open for reuse (persistent connection), but it doesn’t solve the HOL (head-of-line) blocking issue. HOL blocking - when the number of allowed parallel requests in the browser is used up, subsequent requests need to wait for the former ones to complete.   HTTP 2.0 was published in 2015. It addresses HOL issue through request multiplexing, which eliminates HOL blocking at the application layer, but HOL still exists at the transport (TCP) layer. As you can see in the diagram, HTTP 2.0 introduced the concept of HTTP “streams”: an abstraction that allows multiplexing different HTTP exchanges onto the same TCP connection. Each stream doesn’t need to be sent in order.   HTTP 3.0 first draft was published in 2020. It is the proposed successor to HTTP 2.0. It uses QUIC instead of TCP for the underlying transport protocol, thus removing HOL blocking in the transport layer.   QUIC is based on UDP. It introduces streams as first-class citizens at the transport layer. QUIC streams share the same QUIC connection, so no additional handshakes and slow starts are required to create new ones, but QUIC streams are delivered independently such that in most cases packet loss affecting one stream doesn't affect others. SOAP vs REST vs GraphQL vs RPC The diagram below illustrates the API timeline and API styles comparison. Over time, different API architectural styles are released. Each of them has its own patterns of standardizing data exchange. You can check out the use cases of each style in the diagram.    Code First vs. API First The diagram below shows the differences between code-first development and API-first development. Why do we want to consider API first design?     Microservices increase system complexity We have separate services to serve different functions of the system. While this kind of architecture facilitates decoupling and segregation of duty, we need to handle the various communications among services.  It is better to think through the system's complexity before writing the code and carefully defining the boundaries of the services.  Separate functional teams need to speak the same language The dedicated functional teams are only responsible for their own components and services. It is recommended that the organization speak the same language via API design.  We can mock requests and responses to validate the API design before writing code.  Improve software quality and developer productivity Since we have ironed out most of the uncertainties when the project starts, the overall development process is smoother, and the software quality is greatly improved.  Developers are happy about the process as well because they can focus on functional development instead of negotiating sudden changes. The possibility of having surprises toward the end of the project lifecycle is reduced. Because we have designed the API first, the tests can be designed while the code is being developed. In a way, we also have TDD (Test Driven Design) when using API first development. HTTP status codes    The response codes for HTTP are divided into five categories: Informational (100-199) Success (200-299) Redirection (300-399) Client Error (400-499) Server Error (500-599) What does API gateway do? The diagram below shows the details.    Step 1 - The client sends an HTTP request to the API gateway. Step 2 - The API gateway parses and validates the attributes in the HTTP request. Step 3 - The API gateway performs allow-list/deny-list checks. Step 4 - The API gateway talks to an identity provider for authentication and authorization. Step 5 - The rate limiting rules are applied to the request. If it is over the limit, the request is rejected. Steps 6 and 7 - Now that the request has passed basic checks, the API gateway finds the relevant service to route to by path matching. Step 8 - The API gateway transforms the request into the appropriate protocol and sends it to backend microservices. Steps 9-12: The API gateway can handle errors properly, and deals with faults if the error takes a longer time to recover (circuit break). It can also leverage ELK (Elastic-Logstash-Kibana) stack for logging and monitoring. We sometimes cache data in the API gateway. How do we design effective and safe APIs? The diagram below shows typical API designs with a shopping cart example.    Note that API design is not just URL path design. Most of the time, we need to choose the proper resource names, identifiers, and path patterns. It is equally important to design proper HTTP header fields or to design effective rate-limiting rules within the API gateway. TCP/IP encapsulation How is data sent over the network? Why do we need so many layers in the OSI model?    The diagram below shows how data is encapsulated and de-encapsulated when transmitting over the network. Step 1: When Device A sends data to Device B over the network via the HTTP protocol, it is first added an HTTP header at the application layer. Step 2: Then a TCP or a UDP header is added to the data. It is encapsulated into TCP segments at the transport layer. The header contains the source port, destination port, and sequence number. Step 3: The segments are then encapsulated with an IP header at the network layer. The IP header contains the source/destination IP addresses. Step 4: The IP datagram is added a MAC header at the data link layer, with source/destination MAC addresses. Step 5: The encapsulated frames are sent to the physical layer and sent over the network in binary bits. Steps 6-10: When Device B receives the bits from the network, it performs the de-encapsulation process, which is a reverse processing of the encapsulation process. The headers are removed layer by layer, and eventually, Device B can read the data. We need layers in the network model because each layer focuses on its own responsibilities. Each layer can rely on the headers for processing instructions and does not need to know the meaning of the data from the last layer. Why is Nginx called a “reverse” proxy? The diagram below shows the differences between a 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐱𝐲 and a 𝐫𝐞𝐯𝐞𝐫𝐬𝐞 𝐩𝐫𝐨𝐱𝐲.    A forward proxy is a server that sits between user devices and the internet. A forward proxy is commonly used for:  Protect clients Avoid browsing restrictions Block access to certain content  A reverse proxy is a server that accepts a request from the client, forwards the request to web servers, and returns the results to the client as if the proxy server had processed the request. A reverse proxy is good for:  Protect servers Load balancing Cache static contents Encrypt and decrypt SSL communications  What are the common load-balancing algorithms? The diagram below shows 6 common algorithms.     Static Algorithms    Round robin The client requests are sent to different service instances in sequential order. The services are usually required to be stateless.   Sticky round-robin This is an improvement of the round-robin algorithm. If Alice’s first request goes to service A, the following requests go to service A as well.   Weighted round-robin The admin can specify the weight for each service. The ones with a higher weight handle more requests than others.   Hash This algorithm applies a hash function on the incoming requests’ IP or URL. The requests are routed to relevant instances based on the hash function result.    Dynamic Algorithms    Least connections A new request is sent to the service instance with the least concurrent connections.   Least response time A new request is sent to the service instance with the fastest response time.   URL, URI, URN - Do you know the differences? The diagram below shows a comparison of URL, URI, and URN.     URI  URI stands for Uniform Resource Identifier. It identifies a logical or physical resource on the web. URL and URN are subtypes of URI. URL locates a resource, while URN names a resource. A URI is composed of the following parts: scheme:[//authority]path[?query][#fragment]  URL  URL stands for Uniform Resource Locator, the key concept of HTTP. It is the address of a unique resource on the web. It can be used with other protocols like FTP and JDBC.  URN  URN stands for Uniform Resource Name. It uses the urn scheme. URNs cannot be used to locate a resource. A simple example given in the diagram is composed of a namespace and a namespace-specific string. If you would like to learn more detail on the subject, I would recommend W3C’s clarification. CI/CD CI/CD Pipeline Explained in Simple Terms    Section 1 - SDLC with CI/CD The software development life cycle (SDLC) consists of several key stages: development, testing, deployment, and maintenance. CI/CD automates and integrates these stages to enable faster and more reliable releases. When code is pushed to a git repository, it triggers an automated build and test process. End-to-end (e2e) test cases are run to validate the code. If tests pass, the code can be automatically deployed to staging/production. If issues are found, the code is sent back to development for bug fixing. This automation provides fast feedback to developers and reduces the risk of bugs in production. Section 2 - Difference between CI and CD Continuous Integration (CI) automates the build, test, and merge process. It runs tests whenever code is committed to detect integration issues early. This encourages frequent code commits and rapid feedback. Continuous Delivery (CD) automates release processes like infrastructure changes and deployment. It ensures software can be released reliably at any time through automated workflows. CD may also automate the manual testing and approval steps required before production deployment. Section 3 - CI/CD Pipeline A typical CI/CD pipeline has several connected stages:  The developer commits code changes to the source control CI server detects changes and triggers the build Code is compiled, and tested (unit, integration tests) Test results reported to the developer On success, artifacts are deployed to staging environments Further testing may be done on staging before release CD system deploys approved changes to production  Netflix Tech Stack (CI/CD Pipeline)    Planning: Netflix Engineering uses JIRA for planning and Confluence for documentation. Coding: Java is the primary programming language for the backend service, while other languages are used for different use cases. Build: Gradle is mainly used for building, and Gradle plugins are built to support various use cases. Packaging: Package and dependencies are packed into an Amazon Machine Image (AMI) for release. Testing: Testing emphasizes the production culture's focus on building chaos tools. Deployment: Netflix uses its self-built Spinnaker for canary rollout deployment. Monitoring: The monitoring metrics are centralized in Atlas, and Kayenta is used to detect anomalies. Incident report: Incidents are dispatched according to priority, and PagerDuty is used for incident handling. Architecture patterns MVC, MVP, MVVM, MVVM-C, and VIPER These architecture patterns are among the most commonly used in app development, whether on iOS or Android platforms. Developers have introduced them to overcome the limitations of earlier patterns. So, how do they differ?     MVC, the oldest pattern, dates back almost 50 years Every pattern has a "view" (V) responsible for displaying content and receiving user input Most patterns include a "model" (M) to manage business data "Controller," "presenter," and "view-model" are translators that mediate between the view and the model ("entity" in the VIPER pattern)  18 Key Design Patterns Every Developer Should Know Patterns are reusable solutions to common design problems, resulting in a smoother, more efficient development process. They serve as blueprints for building better software structures. These are some of the most popular patterns:     Abstract Factory: Family Creator - Makes groups of related items. Builder: Lego Master - Builds objects step by step, keeping creation and appearance separate. Prototype: Clone Maker - Creates copies of fully prepared examples. Singleton: One and Only - A special class with just one instance. Adapter: Universal Plug - Connects things with different interfaces. Bridge: Function Connector - Links how an object works to what it does. Composite: Tree Builder - Forms tree-like structures of simple and complex parts. Decorator: Customizer - Adds features to objects without changing their core. Facade: One-Stop-Shop - Represents a whole system with a single, simplified interface. Flyweight: Space Saver - Shares small, reusable items efficiently. Proxy: Stand-In Actor - Represents another object, controlling access or actions. Chain of Responsibility: Request Relay - Passes a request through a chain of objects until handled. Command: Task Wrapper - Turns a request into an object, ready for action. Iterator: Collection Explorer - Accesses elements in a collection one by one. Mediator: Communication Hub - Simplifies interactions between different classes. Memento: Time Capsule - Captures and restores an object's state. Observer: News Broadcaster - Notifies classes about changes in other objects. Visitor: Skillful Guest - Adds new operations to a class without altering it.  Database A nice cheat sheet of different databases in cloud services    Choosing the right database for your project is a complex task. Many database options, each suited to distinct use cases, can quickly lead to decision fatigue. We hope this cheat sheet provides high-level direction to pinpoint the right service that aligns with your project's needs and avoid potential pitfalls. Note: Google has limited documentation for their database use cases. Even though we did our best to look at what was available and arrived at the best option, some of the entries may need to be more accurate. 8 Data Structures That Power Your Databases The answer will vary depending on your use case. Data can be indexed in memory or on disk. Similarly, data formats vary, such as numbers, strings, geographic coordinates, etc. The system might be write-heavy or read-heavy. All of these factors affect your choice of database index format.    The following are some of the most popular data structures used for indexing data:  Skiplist: a common in-memory index type. Used in Redis Hash index: a very common implementation of the “Map” data structure (or “Collection”) SSTable: immutable on-disk “Map” implementation LSM tree: Skiplist + SSTable. High write throughput B-tree: disk-based solution. Consistent read/write performance Inverted index: used for document indexing. Used in Lucene Suffix tree: for string pattern search R-tree: multi-dimension search, such as finding the nearest neighbor  How is an SQL statement executed in the database? The diagram below shows the process. Note that the architectures for different databases are different, the diagram demonstrates some common designs.    Step 1 - A SQL statement is sent to the database via a transport layer protocol (e.g.TCP). Step 2 - The SQL statement is sent to the command parser, where it goes through syntactic and semantic analysis, and a query tree is generated afterward. Step 3 - The query tree is sent to the optimizer. The optimizer creates an execution plan. Step 4 - The execution plan is sent to the executor. The executor retrieves data from the execution. Step 5 - Access methods provide the data fetching logic required for execution, retrieving data from the storage engine. Step 6 - Access methods decide whether the SQL statement is read-only. If the query is read-only (SELECT statement), it is passed to the buffer manager for further processing. The buffer manager looks for the data in the cache or data files. Step 7 - If the statement is an UPDATE or INSERT, it is passed to the transaction manager for further processing. Step 8 - During a transaction, the data is in lock mode. This is guaranteed by the lock manager. It also ensures the transaction’s ACID properties. CAP theorem The CAP theorem is one of the most famous terms in computer science, but I bet different developers have different understandings. Let’s examine what it is and why it can be confusing.    CAP theorem states that a distributed system can't provide more than two of these three guarantees simultaneously. Consistency: consistency means all clients see the same data at the same time no matter which node they connect to. Availability: availability means any client that requests data gets a response even if some of the nodes are down. Partition Tolerance: a partition indicates a communication break between two nodes. Partition tolerance means the system continues to operate despite network partitions. The “2 of 3” formulation can be useful, but this simplification could be misleading.   Picking a database is not easy. Justifying our choice purely based on the CAP theorem is not enough. For example, companies don't choose Cassandra for chat applications simply because it is an AP system. There is a list of good characteristics that make Cassandra a desirable option for storing chat messages. We need to dig deeper.   “CAP prohibits only a tiny part of the design space: perfect availability and consistency in the presence of partitions, which are rare”. Quoted from the paper: CAP Twelve Years Later: How the “Rules” Have Changed.   The theorem is about 100% availability and consistency. A more realistic discussion would be the trade-offs between latency and consistency when there is no network partition. See PACELC theorem for more details.   Is the CAP theorem actually useful? I think it is still useful as it opens our minds to a set of tradeoff discussions, but it is only part of the story. We need to dig deeper when picking the right database. Types of Memory and Storage    Visualizing a SQL query    SQL statements are executed by the database system in several steps, including:  Parsing the SQL statement and checking its validity Transforming the SQL into an internal representation, such as relational algebra Optimizing the internal representation and creating an execution plan that utilizes index information Executing the plan and returning the results  The execution of SQL is highly complex and involves many considerations, such as:  The use of indexes and caches The order of table joins Concurrency control Transaction management  SQL language In 1986, SQL (Structured Query Language) became a standard. Over the next 40 years, it became the dominant language for relational database management systems. Reading the latest standard (ANSI SQL 2016) can be time-consuming. How can I learn it?    There are 5 components of the SQL language:  DDL: data definition language, such as CREATE, ALTER, DROP DQL: data query language, such as SELECT DML: data manipulation language, such as INSERT, UPDATE, DELETE DCL: data control language, such as GRANT, REVOKE TCL: transaction control language, such as COMMIT, ROLLBACK  For a backend engineer, you may need to know most of it. As a data analyst, you may need to have a good understanding of DQL. Select the topics that are most relevant to you. Cache Data is cached everywhere This diagram illustrates where we cache data in a typical architecture.    There are multiple layers along the flow.  Client apps: HTTP responses can be cached by the browser. We request data over HTTP for the first time, and it is returned with an expiry policy in the HTTP header; we request data again, and the client app tries to retrieve the data from the browser cache first. CDN: CDN caches static web resources. The clients can retrieve data from a CDN node nearby. Load Balancer: The load Balancer can cache resources as well. Messaging infra: Message brokers store messages on disk first, and then consumers retrieve them at their own pace. Depending on the retention policy, the data is cached in Kafka clusters for a period of time. Services: There are multiple layers of cache in a service. If the data is not cached in the CPU cache, the service will try to retrieve the data from memory. Sometimes the service has a second-level cache to store data on disk. Distributed Cache: Distributed cache like Redis holds key-value pairs for multiple services in memory. It provides much better read/write performance than the database. Full-text Search: we sometimes need to use full-text searches like Elastic Search for document search or log search. A copy of data is indexed in the search engine as well. Database: Even in the database, we have different levels of caches:   WAL(Write-ahead Log): data is written to WAL first before building the B tree index Bufferpool: A memory area allocated to cache query results Materialized View: Pre-compute query results and store them in the database tables for better query performance Transaction log: record all the transactions and database updates Replication Log: used to record the replication state in a database cluster  Why is Redis so fast? There are 3 main reasons as shown in the diagram below.     Redis is a RAM-based data store. RAM access is at least 1000 times faster than random disk access. Redis leverages IO multiplexing and single-threaded execution loop for execution efficiency. Redis leverages several efficient lower-level data structures.  Question: Another popular in-memory store is Memcached. Do you know the differences between Redis and Memcached? You might have noticed the style of this diagram is different from my previous posts. Please let me know which one you prefer. How can Redis be used?    There is more to Redis than just caching. Redis can be used in a variety of scenarios as shown in the diagram.   Session We can use Redis to share user session data among different services.   Cache We can use Redis to cache objects or pages, especially for hotspot data.   Distributed lock We can use a Redis string to acquire locks among distributed services.   Counter We can count how many likes or how many reads for articles.   Rate limiter We can apply a rate limiter for certain user IPs.   Global ID generator We can use Redis Int for global ID.   Shopping cart We can use Redis Hash to represent key-value pairs in a shopping cart.   Calculate user retention We can use Bitmap to represent the user login daily and calculate user retention.   Message queue We can use List for a message queue.   Ranking We can use ZSet to sort the articles.   Top caching strategies Designing large-scale systems usually requires careful consideration of caching. Below are five caching strategies that are frequently utilized.    Microservice architecture What does a typical microservice architecture look like?    The diagram below shows a typical microservice architecture.  Load Balancer: This distributes incoming traffic across multiple backend services. CDN (Content Delivery Network): CDN is a group of geographically distributed servers that hold static content for faster delivery. The clients look for content in CDN first, then progress  to backend services. API Gateway: This handles incoming requests and routes them to the relevant services. It talks to the identity provider and service discovery. Identity Provider: This handles authentication and authorization for users. Service Registry &amp; Discovery: Microservice registration and discovery happen in this component, and the API gateway looks for relevant services in this component to talk to. Management: This component is responsible for monitoring the services. Microservices: Microservices are designed and deployed in different domains. Each domain has its own database. The API gateway talks to the microservices via REST API or other protocols, and the microservices within the same domain talk to each other using RPC (Remote Procedure Call).  Benefits of microservices:  They can be quickly designed, deployed, and horizontally scaled. Each domain can be independently maintained by a dedicated team. Business requirements can be customized in each domain and better supported, as a result.  Microservice Best Practices A picture is worth a thousand words: 9 best practices for developing microservices.    When we develop microservices, we need to follow the following best practices:  Use separate data storage for each microservice Keep code at a similar level of maturity Separate build for each microservice Assign each microservice with a single responsibility Deploy into containers Design stateless services Adopt domain-driven design Design micro frontend Orchestrating microservices  What tech stack is commonly used for microservices? Below you will find a diagram showing the microservice tech stack, both for the development phase and for production.    ▶️ 𝐏𝐫𝐞-𝐏𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧  Define API - This establishes a contract between frontend and backend. We can use Postman or OpenAPI for this. Development - Node.js or react is popular for frontend development, and java/python/go for backend development. Also, we need to change the configurations in the API gateway according to API definitions. Continuous Integration - JUnit and Jenkins for automated testing. The code is packaged into a Docker image and deployed as microservices.  ▶️ 𝐏𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧  NGinx is a common choice for load balancers. Cloudflare provides CDN (Content Delivery Network). API Gateway - We can use spring boot for the gateway, and use Eureka/Zookeeper for service discovery. The microservices are deployed on clouds. We have options among AWS, Microsoft Azure, or Google GCP. Cache and Full-text Search - Redis is a common choice for caching key-value pairs. ElasticSearch is used for full-text search. Communications - For services to talk to each other, we can use messaging infra Kafka or RPC. Persistence - We can use MySQL or PostgreSQL for a relational database, and Amazon S3 for object store. We can also use Cassandra for the wide-column store if necessary. Management &amp; Monitoring - To manage so many microservices, the common Ops tools include Prometheus, Elastic Stack, and Kubernetes.  Why is Kafka fast There are many design decisions that contributed to Kafka’s performance. In this post, we’ll focus on two. We think these two carried the most weight.     The first one is Kafka’s reliance on Sequential I/O. The second design choice that gives Kafka its performance advantage is its focus on efficiency: zero copy principle.  The diagram illustrates how the data is transmitted between producer and consumer, and what zero-copy means.  Step 1.1 - 1.3: Producer writes data to the disk Step 2: Consumer reads data without zero-copy  2.1 The data is loaded from disk to OS cache 2.2 The data is copied from OS cache to Kafka application 2.3 Kafka application copies the data into the socket buffer 2.4 The data is copied from socket buffer to network card 2.5 The network card sends data out to the consumer  Step 3: Consumer reads data with zero-copy  3.1: The data is loaded from disk to OS cache 3.2 OS cache directly copies the data to the network card via sendfile() command 3.3 The network card sends data out to the consumer Zero copy is a shortcut to save the multiple data copies between application context and kernel context. Payment systems How to learn payment systems?    Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? The diagram below shows the economics of the credit card payment flow.    1.  The cardholder pays a merchant $100 to buy a product. 2. The merchant benefits from the use of the credit card with higher sales volume and needs to compensate the issuer and the card network for providing the payment service. The acquiring bank sets a fee with the merchant, called the “merchant discount fee.” 3 - 4. The acquiring bank keeps $0.25 as the acquiring markup, and $1.75 is paid to the issuing bank as the interchange fee. The merchant discount fee should cover the interchange fee. The interchange fee is set by the card network because it is less efficient for each issuing bank to negotiate fees with each merchant. 5.  The card network sets up the network assessments and fees with each bank, which pays the card network for its services every month. For example, VISA charges a 0.11% assessment, plus a $0.0195 usage fee, for every swipe. 6.  The cardholder pays the issuing bank for its services. Why should the issuing bank be compensated?  The issuer pays the merchant even if the cardholder fails to pay the issuer. The issuer pays the merchant before the cardholder pays the issuer. The issuer has other operating costs, including managing customer accounts, providing statements, fraud detection, risk management, clearing &amp; settlement, etc.  How does VISA work when we swipe a credit card at a merchant’s shop?    VISA, Mastercard, and American Express act as card networks for the clearing and settling of funds. The card acquiring bank and the card issuing bank can be – and often are – different. If banks were to settle transactions one by one without an intermediary, each bank would have to settle the transactions with all the other banks. This is quite inefficient. The diagram below shows VISA’s role in the credit card payment process. There are two flows involved. Authorization flow happens when the customer swipes the credit card. Capture and settlement flow happens when the merchant wants to get the money at the end of the day.  Authorization Flow  Step 0: The card issuing bank issues credit cards to its customers. Step 1: The cardholder wants to buy a product and swipes the credit card at the Point of Sale (POS) terminal in the merchant’s shop. Step 2: The POS terminal sends the transaction to the acquiring bank, which has provided the POS terminal. Steps 3 and 4: The acquiring bank sends the transaction to the card network, also called the card scheme. The card network sends the transaction to the issuing bank for approval. Steps 4.1, 4.2 and 4.3: The issuing bank freezes the money if the transaction is approved. The approval or rejection is sent back to the acquirer, as well as the POS terminal.  Capture and Settlement Flow  Steps 1 and 2: The merchant wants to collect the money at the end of the day, so they hit ”capture” on the POS terminal. The transactions are sent to the acquirer in batch. The acquirer sends the batch file with transactions to the card network. Step 3: The card network performs clearing for the transactions collected from different acquirers, and sends the clearing files to different issuing banks. Step 4: The issuing banks confirm the correctness of the clearing files, and transfer money to the relevant acquiring banks. Step 5: The acquiring bank then transfers money to the merchant’s bank. Step 4: The card network clears the transactions from different acquiring banks. Clearing is a process in which mutual offset transactions are netted, so the number of total transactions is reduced. In the process, the card network takes on the burden of talking to each bank and receives service fees in return. Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India What’s UPI? UPI is an instant real-time payment system developed by the National Payments Corporation of India. It accounts for 60% of digital retail transactions in India today. UPI = payment markup language + standard for interoperable payments    DevOps DevOps vs. SRE vs. Platform Engineering. What is the difference? The concepts of DevOps, SRE, and Platform Engineering have emerged at different times and have been developed by various individuals and organizations.    DevOps as a concept was introduced in 2009 by Patrick Debois and Andrew Shafer at the Agile conference. They sought to bridge the gap between software development and operations by promoting a collaborative culture and shared responsibility for the entire software development lifecycle. SRE, or Site Reliability Engineering, was pioneered by Google in the early 2000s to address operational challenges in managing large-scale, complex systems. Google developed SRE practices and tools, such as the Borg cluster management system and the Monarch monitoring system, to improve the reliability and efficiency of their services. Platform Engineering is a more recent concept, building on the foundation of SRE engineering. The precise origins of Platform Engineering are less clear, but it is generally understood to be an extension of the DevOps and SRE practices, with a focus on delivering a comprehensive platform for product development that supports the entire business perspective. It's worth noting that while these concepts emerged at different times. They are all related to the broader trend of improving collaboration, automation, and efficiency in software development and operations. What is k8s (Kubernetes)? K8s is a container orchestration system. It is used for container deployment and management. Its design is greatly impacted by Google’s internal system Borg.    A k8s cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers, and a cluster usually runs multiple nodes, providing fault tolerance and high availability.  Control Plane Components    API Server The API server talks to all the components in the k8s cluster. All the operations on pods are executed by talking to the API server.   Scheduler The scheduler watches pod workloads and assigns loads on newly created pods.   Controller Manager The controller manager runs the controllers, including Node Controller, Job Controller, EndpointSlice Controller, and ServiceAccount Controller.   Etcd etcd is a key-value store used as Kubernetes' backing store for all cluster data.    Nodes    Pods A pod is a group of containers and is the smallest unit that k8s administers. Pods have a single IP address applied to every container within the pod.   Kubelet An agent that runs on each node in the cluster. It ensures containers are running in a Pod.   Kube Proxy Kube-proxy is a network proxy that runs on each node in your cluster. It routes traffic coming into a node from the service. It forwards requests for work to the correct containers.   Docker vs. Kubernetes. Which one should we use?    What is Docker ? Docker is an open-source platform that allows you to package, distribute, and run applications in isolated containers. It focuses on containerization, providing lightweight environments that encapsulate applications and their dependencies. What is Kubernetes ? Kubernetes, often referred to as K8s, is an open-source container orchestration platform. It provides a framework for automating the deployment, scaling, and management of containerized applications across a cluster of nodes. How are both different from each other ? Docker: Docker operates at the individual container level on a single operating system host. You must manually manage each host and setting up networks, security policies, and storage for multiple related containers can be complex. Kubernetes: Kubernetes operates at the cluster level. It manages multiple containerized applications across multiple hosts, providing automation for tasks like load balancing, scaling, and ensuring the desired state of applications. In short, Docker focuses on containerization and running containers on individual hosts, while Kubernetes specializes in managing and orchestrating containers at scale across a cluster of hosts. How does Docker work? The diagram below shows the architecture of Docker and how it works when we run “docker build”, “docker pull” and “docker run”.    There are 3 components in Docker architecture:   Docker client The docker client talks to the Docker daemon.   Docker host The Docker daemon listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.   Docker registry A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use.   Let’s take the “docker run” command as an example.  Docker pulls the image from the registry. Docker creates a new container. Docker allocates a read-write filesystem to the container. Docker creates a network interface to connect the container to the default network. Docker starts the container.  GIT How Git Commands work To begin with, it's essential to identify where our code is stored. The common assumption is that there are only two locations - one on a remote server like Github and the other on our local machine. However, this isn't entirely accurate. Git maintains three local storages on our machine, which means that our code can be found in four places:     Working directory: where we edit files Staging area: a temporary location where files are kept for the next commit Local repository: contains the code that has been committed Remote repository: the remote server that stores the code  Most Git commands primarily move files between these four locations. How does Git Work? The diagram below shows the Git workflow.    Git is a distributed version control system. Every developer maintains a local copy of the main repository and edits and commits to the local copy. The commit is very fast because the operation doesn’t interact with the remote repository. If the remote repository crashes, the files can be recovered from the local repositories. Git merge vs. Git rebase What are the differences?    When we merge changes from one Git branch to another, we can use ‘git merge’ or ‘git rebase’. The diagram below shows how the two commands work. Git merge This creates a new commit G’ in the main branch. G’ ties the histories of both main and feature branches. Git merge is non-destructive. Neither the main nor the feature branch is changed. Git rebase Git rebase moves the feature branch histories to the head of the main branch. It creates new commits E’, F’, and G’ for each commit in the feature branch. The benefit of rebase is that it has a linear commit history. Rebase can be dangerous if “the golden rule of git rebase” is not followed. The Golden Rule of Git Rebase Never use it on public branches! Cloud Services A nice cheat sheet of different cloud services (2023 edition)    What is cloud native? Below is a diagram showing the evolution of architecture and processes since the 1980s.    Organizations can build and run scalable applications on public, private, and hybrid clouds using cloud native technologies. This means the applications are designed to leverage cloud features, so they are resilient to load and easy to scale. Cloud native includes 4 aspects:   Development process This has progressed from waterfall to agile to DevOps.   Application Architecture The architecture has gone from monolithic to microservices. Each service is designed to be small, adaptive to the limited resources in cloud containers.   Deployment &amp; packaging The applications used to be deployed on physical servers. Then around 2000, the applications that were not sensitive to latency were usually deployed on virtual servers. With cloud native applications, they are packaged into docker images and deployed in containers.   Application infrastructure The applications are massively deployed on cloud infrastructure instead of self-hosted servers.   Developer productivity tools Visualize JSON files Nested JSON files are hard to read. JsonCrack generates graph diagrams from JSON files and makes them easy to read. Additionally, the generated diagrams can be downloaded as images.    Automatically turn code into architecture diagrams    What does it do?  Draw the cloud system architecture in Python code. Diagrams can also be rendered directly inside the Jupyter Notebooks. No design tools are needed. Supports the following providers: AWS, Azure, GCP, Kubernetes, Alibaba Cloud, Oracle Cloud, etc.  Github repo Linux Linux file system explained    The Linux file system used to resemble an unorganized town where individuals constructed their houses wherever they pleased. However, in 1994, the Filesystem Hierarchy Standard (FHS) was introduced to bring order to the Linux file system. By implementing a standard like the FHS, software can ensure a consistent layout across various Linux distributions. Nonetheless, not all Linux distributions strictly adhere to this standard. They often incorporate their own unique elements or cater to specific requirements. To become proficient in this standard, you can begin by exploring. Utilize commands such as "cd" for navigation and "ls" for listing directory contents. Imagine the file system as a tree, starting from the root (/). With time, it will become second nature to you, transforming you into a skilled Linux administrator. 18 Most-used Linux Commands You Should Know Linux commands are instructions for interacting with the operating system. They help manage files, directories, system processes, and many other aspects of the system. You need to become familiar with these commands in order to navigate and maintain Linux-based systems efficiently and effectively. This diagram below shows popular Linux commands:     ls - List files and directories cd - Change the current directory mkdir - Create a new directory rm - Remove files or directories cp - Copy files or directories mv - Move or rename files or directories chmod - Change file or directory permissions grep - Search for a pattern in files find - Search for files and directories tar - manipulate tarball archive files vi - Edit files using text editors cat - display the content of files top - Display processes and resource usage ps - Display processes information kill - Terminate a process by sending a signal du - Estimate file space usage ifconfig - Configure network interfaces ping - Test network connectivity between hosts  Security How does HTTPS work? Hypertext Transfer Protocol Secure (HTTPS) is an extension of the Hypertext Transfer Protocol (HTTP.) HTTPS transmits encrypted data using Transport Layer Security (TLS.) If the data is hijacked online, all the hijacker gets is binary code.    How is the data encrypted and decrypted? Step 1 - The client (browser) and the server establish a TCP connection. Step 2 - The client sends a “client hello” to the server. The message contains a set of necessary encryption algorithms (cipher suites) and the latest TLS version it can support. The server responds with a “server hello” so the browser knows whether it can support the algorithms and TLS version. The server then sends the SSL certificate to the client. The certificate contains the public key, host name, expiry dates, etc. The client validates the certificate. Step 3 - After validating the SSL certificate, the client generates a session key and encrypts it using the public key. The server receives the encrypted session key and decrypts it with the private key. Step 4 - Now that both the client and the server hold the same session key (symmetric encryption), the encrypted data is transmitted in a secure bi-directional channel. Why does HTTPS switch to symmetric encryption during data transmission? There are two main reasons:   Security: The asymmetric encryption goes only one way. This means that if the server tries to send the encrypted data back to the client, anyone can decrypt the data using the public key.   Server resources: The asymmetric encryption adds quite a lot of mathematical overhead. It is not suitable for data transmissions in long sessions.   Oauth 2.0 Explained With Simple Terms. OAuth 2.0 is a powerful and secure framework that allows different applications to securely interact with each other on behalf of users without sharing sensitive credentials.    The entities involved in OAuth are the User, the Server, and the Identity Provider (IDP). What Can an OAuth Token Do? When you use OAuth, you get an OAuth token that represents your identity and permissions. This token can do a few important things: Single Sign-On (SSO): With an OAuth token, you can log into multiple services or apps using just one login, making life easier and safer. Authorization Across Systems: The OAuth token allows you to share your authorization or access rights across various systems, so you don't have to log in separately everywhere. Accessing User Profile: Apps with an OAuth token can access certain parts of your user profile that you allow, but they won't see everything. Remember, OAuth 2.0 is all about keeping you and your data safe while making your online experiences seamless and hassle-free across different applications and services. Top 4 Forms of Authentication Mechanisms      SSH Keys: Cryptographic keys are used to access remote systems and servers securely   OAuth Tokens: Tokens that provide limited access to user data on third-party applications   SSL Certificates: Digital certificates ensure secure and encrypted communication between servers and clients   Credentials: User authentication information is used to verify and grant access to various systems and services   Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? These terms are all related to user identity management. When you log into a website, you declare who you are (identification). Your identity is verified (authentication), and you are granted the necessary permissions (authorization). Many solutions have been proposed in the past, and the list keeps growing.    From simple to complex, here is my understanding of user identity management:   WWW-Authenticate is the most basic method. You are asked for the username and password by the browser. As a result of the inability to control the login life cycle, it is seldom used today.   A finer control over the login life cycle is session-cookie. The server maintains session storage, and the browser keeps the ID of the session. A cookie usually only works with browsers and is not mobile app friendly.   To address the compatibility issue, the token can be used. The client sends the token to the server, and the server validates the token. The downside is that the token needs to be encrypted and decrypted, which may be time-consuming.   JWT is a standard way of representing tokens. This information can be verified and trusted because it is digitally signed. Since JWT contains the signature, there is no need to save session information on the server side.   By using SSO (single sign-on), you can sign on only once and log in to multiple websites. It uses CAS (central authentication service) to maintain cross-site information   By using OAuth 2.0, you can authorize one website to access your information on another website.   How to store passwords safely in the database and how to validate a password?    Things NOT to do   Storing passwords in plain text is not a good idea because anyone with internal access can see them.   Storing password hashes directly is not sufficient because it is pruned to precomputation attacks, such as rainbow tables.   To mitigate precomputation attacks, we salt the passwords.   What is salt? According to OWASP guidelines, “a salt is a unique, randomly generated string that is added to each password as part of the hashing process”. How to store a password and salt?  the hash result is unique to each password. The password can be stored in the database using the following format: hash(password + salt).  How to validate a password? To validate a password, it can go through the following process:  A client enters the password. The system fetches the corresponding salt from the database. The system appends the salt to the password and hashes it. Let’s call the hashed value H1. The system compares H1 and H2, where H2 is the hash stored in the database. If they are the same, the password is valid.  Explaining JSON Web Token (JWT) to a 10 year old Kid    Imagine you have a special box called a JWT. Inside this box, there are three parts: a header, a payload, and a signature. The header is like the label on the outside of the box. It tells us what type of box it is and how it's secured. It's usually written in a format called JSON, which is just a way to organize information using curly braces { } and colons : . The payload is like the actual message or information you want to send. It could be your name, age, or any other data you want to share. It's also written in JSON format, so it's easy to understand and work with. Now, the signature is what makes the JWT secure. It's like a special seal that only the sender knows how to create. The signature is created using a secret code, kind of like a password. This signature ensures that nobody can tamper with the contents of the JWT without the sender knowing about it. When you want to send the JWT to a server, you put the header, payload, and signature inside the box. Then you send it over to the server. The server can easily read the header and payload to understand who you are and what you want to do. How does Google Authenticator (or other types of 2-factor authenticators) work? Google Authenticator is commonly used for logging into our accounts when 2-factor authentication is enabled. How does it guarantee security? Google Authenticator is a software-based authenticator that implements a two-step verification service. The diagram below provides detail.    There are two stages involved:  Stage 1 - The user enables Google two-step verification Stage 2 - The user uses the authenticator for logging in, etc.  Let’s look at these stages. Stage 1 Steps 1 and 2: Bob opens the web page to enable two-step verification. The front end requests a secret key. The authentication service generates the secret key for Bob and stores it in the database. Step 3: The authentication service returns a URI to the front end. The URI is composed of a key issuer, username, and secret key. The URI is displayed in the form of a QR code on the web page. Step 4: Bob then uses Google Authenticator to scan the generated QR code. The secret key is stored in the authenticator. Stage 2 Steps 1 and 2: Bob wants to log into a website with Google two-step verification. For this, he needs the password. Every 30 seconds, Google Authenticator generates a 6-digit password using TOTP (Time-based One Time Password) algorithm. Bob uses the password to enter the website. Steps 3 and 4: The frontend sends the password Bob enters to the backend for authentication. The authentication service reads the secret key from the database and generates a 6-digit password using the same TOTP algorithm as the client. Step 5: The authentication service compares the two passwords generated by the client and the server, and returns the comparison result to the frontend. Bob can proceed with the login process only if the two passwords match. Is this authentication mechanism safe?   Can the secret key be obtained by others? We need to make sure the secret key is transmitted using HTTPS. The authenticator client and the database store the secret key, and we need to make sure the secret keys are encrypted.   Can the 6-digit password be guessed by hackers? No. The password has 6 digits, so the generated password has 1 million potential combinations. Plus, the password changes every 30 seconds. If hackers want to guess the password in 30 seconds, they need to enter 30,000 combinations per second.   Real World Case Studies Netflix's Tech Stack This post is based on research from many Netflix engineering blogs and open-source projects. If you come across any inaccuracies, please feel free to inform us.    Mobile and web: Netflix has adopted Swift and Kotlin to build native mobile apps. For its web application, it uses React. Frontend/server communication: Netflix uses GraphQL. Backend services: Netflix relies on ZUUL, Eureka, the Spring Boot framework, and other technologies. Databases: Netflix utilizes EV cache, Cassandra, CockroachDB, and other databases. Messaging/streaming: Netflix employs Apache Kafka and Fink for messaging and streaming purposes. Video storage: Netflix uses S3 and Open Connect for video storage. Data processing: Netflix utilizes Flink and Spark for data processing, which is then visualized using Tableau. Redshift is used for processing structured data warehouse information. CI/CD: Netflix employs various tools such as JIRA, Confluence, PagerDuty, Jenkins, Gradle, Chaos Monkey, Spinnaker, Atlas, and more for CI/CD processes. Twitter Architecture 2022 Yes, this is the real Twitter architecture. It is posted by Elon Musk and redrawn by us for better readability.    Evolution of Airbnb’s microservice architecture over the past 15 years Airbnb’s microservice architecture went through 3 main stages.    Monolith (2008 - 2017) Airbnb began as a simple marketplace for hosts and guests. This is built in a Ruby on Rails application - the monolith. What’s the challenge?  Confusing team ownership + unowned code Slow deployment  Microservices (2017 - 2020) Microservice aims to solve those challenges. In the microservice architecture, key services include:  Data fetching service Business logic data service Write workflow service UI aggregation service Each service had one owning team  What’s the challenge? Hundreds of services and dependencies were difficult for humans to manage. Micro + macroservices (2020 - present) This is what Airbnb is working on now. The micro and macroservice hybrid model focuses on the unification of APIs. Monorepo vs. Microrepo. Which is the best? Why do different companies choose different options?    Monorepo isn't new; Linux and Windows were both created using Monorepo. To improve scalability and build speed, Google developed its internal dedicated toolchain to scale it faster and strict coding quality standards to keep it consistent. Amazon and Netflix are major ambassadors of the Microservice philosophy. This approach naturally separates the service code into separate repositories. It scales faster but can lead to governance pain points later on. Within Monorepo, each service is a folder, and every folder has a BUILD config and OWNERS permission control. Every service member is responsible for their own folder. On the other hand, in Microrepo, each service is responsible for its repository, with the build config and permissions typically set for the entire repository. In Monorepo, dependencies are shared across the entire codebase regardless of your business, so when there's a version upgrade, every codebase upgrades their version. In Microrepo, dependencies are controlled within each repository. Businesses choose when to upgrade their versions based on their own schedules. Monorepo has a standard for check-ins. Google's code review process is famously known for setting a high bar, ensuring a coherent quality standard for Monorepo, regardless of the business. Microrepo can either set its own standard or adopt a shared standard by incorporating best practices. It can scale faster for business, but the code quality might be a bit different. Google engineers built Bazel, and Meta built Buck. There are other open-source tools available, including Nix, Lerna, and others. Over the years, Microrepo has had more supported tools, including Maven and Gradle for Java, NPM for NodeJS, and CMake for C/C++, among others. How will you design the Stack Overflow website? If your answer is on-premise servers and monolith (on the right), you would likely fail the interview, but that's how it is built in reality!    What people think it should look like The interviewer is probably expecting something on the left side.  Microservice is used to decompose the system into small components. Each service has its own database. Use cache heavily. The service is sharded. The services talk to each other asynchronously through message queues. The service is implemented using Event Sourcing with CQRS. Showing off knowledge in distributed systems such as eventual consistency, CAP theorem, etc.  What it actually is Stack Overflow serves all the traffic with only 9 on-premise web servers, and it’s on monolith! It has its own servers and does not run on the cloud. This is contrary to all our popular beliefs these days. Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? The diagram below shows the architecture comparison before and after the migration.    What is Amazon Prime Video Monitoring Service? Prime Video service needs to monitor the quality of thousands of live streams. The monitoring tool automatically analyzes the streams in real time and identifies quality issues like block corruption, video freeze, and sync problems. This is an important process for customer satisfaction. There are 3 steps: media converter, defect detector, and real-time notification.   What is the problem with the old architecture? The old architecture was based on Amazon Lambda, which was good for building services quickly. However, it was not cost-effective when running the architecture at a high scale. The two most expensive operations are:     The orchestration workflow - AWS step functions charge users by state transitions and the orchestration performs multiple state transitions every second.   Data passing between distributed components - the intermediate data is stored in Amazon S3 so that the next stage can download. The download can be costly when the volume is high.     Monolithic architecture saves 90% cost A monolithic architecture is designed to address the cost issues. There are still 3 components, but the media converter and defect detector are deployed in the same process, saving the cost of passing data over the network. Surprisingly, this approach to deployment architecture change led to 90% cost savings!   This is an interesting and unique case study because microservices have become a go-to and fashionable choice in the tech industry. It's good to see that we are having more discussions about evolving the architecture and having more honest discussions about its pros and cons. Decomposing components into distributed microservices comes with a cost.   What did Amazon leaders say about this? Amazon CTO Werner Vogels: “Building evolvable software systems is a strategy, not a religion. And revisiting your architecture with an open mind is a must.”   Ex Amazon VP Sustainability Adrian Cockcroft: “The Prime Video team had followed a path I call Serverless First…I don’t advocate Serverless Only”. How does Disney Hotstar capture 5 Billion Emojis during a tournament?      Clients send emojis through standard HTTP requests. You can think of Golang Service as a typical Web Server. Golang is chosen because it supports concurrency well. Threads in Golang are lightweight.   Since the write volume is very high, Kafka (message queue) is used as a buffer.   Emoji data are aggregated by a streaming processing service called Spark. It aggregates data every 2 seconds, which is configurable. There is a trade-off to be made based on the interval. A shorter interval means emojis are delivered to other clients faster but it also means more computing resources are needed.   Aggregated data is written to another Kafka.   The PubSub consumers pull aggregated emoji data from Kafka.   Emojis are delivered to other clients in real-time through the PubSub infrastructure. The PubSub infrastructure is interesting. Hotstar considered the following protocols: Socketio, NATS, MQTT, and gRPC, and settled with MQTT.   A similar design is adopted by LinkedIn which streams a million likes/sec. How Discord Stores Trillions Of Messages The diagram below shows the evolution of message storage at Discord:    MongoDB ➡️ Cassandra ➡️ ScyllaDB In 2015, the first version of Discord was built on top of a single MongoDB replica. Around Nov 2015, MongoDB stored 100 million messages and the RAM couldn’t hold the data and index any longer. The latency became unpredictable. Message storage needs to be moved to another database. Cassandra was chosen. In 2017, Discord had 12 Cassandra nodes and stored billions of messages. At the beginning of 2022, it had 177 nodes with trillions of messages. At this point, latency was unpredictable, and maintenance operations became too expensive to run. There are several reasons for the issue:  Cassandra uses the LSM tree for the internal data structure. The reads are more expensive than the writes. There can be many concurrent reads on a server with hundreds of users, resulting in hotspots. Maintaining clusters, such as compacting SSTables, impacts performance. Garbage collection pauses would cause significant latency spikes  ScyllaDB is Cassandra compatible database written in C++. Discord redesigned its architecture to have a monolithic API, a data service written in Rust, and ScyllaDB-based storage. The p99 read latency in ScyllaDB is 15ms compared to 40-125ms in Cassandra. The p99 write latency is 5ms compared to 5-70ms in Cassandra. How do video live streamings work on YouTube, TikTok live, or Twitch? Live streaming differs from regular streaming because the video content is sent via the internet in real-time, usually with a latency of just a few seconds. The diagram below explains what happens behind the scenes to make this possible.    Step 1: The raw video data is captured by a microphone and camera. The data is sent to the server side. Step 2: The video data is compressed and encoded. For example, the compressing algorithm separates the background and other video elements. After compression, the video is encoded to standards such as H.264. The size of the video data is much smaller after this step. Step 3: The encoded data is divided into smaller segments, usually seconds in length, so it takes much less time to download or stream. Step 4: The segmented data is sent to the streaming server. The streaming server needs to support different devices and network conditions. This is called ‘Adaptive Bitrate Streaming.’ This means we need to produce multiple files at different bitrates in steps 2 and 3. Step 5: The live streaming data is pushed to edge servers supported by CDN (Content Delivery Network.) Millions of viewers can watch the video from an edge server nearby. CDN significantly lowers data transmission latency. Step 6: The viewers’ devices decode and decompress the video data and play the video in a video player. Steps 7 and 8: If the video needs to be stored for replay, the encoded data is sent to a storage server, and viewers can request a replay from it later. Standard protocols for live streaming include:  RTMP (Real-Time Messaging Protocol): This was originally developed by Macromedia to transmit data between a Flash player and a server. Now it is used for streaming video data over the internet. Note that video conferencing applications like Skype use RTC (Real-Time Communication) protocol for lower latency. HLS (HTTP Live Streaming): It requires the H.264 or H.265 encoding. Apple devices accept only HLS format. DASH (Dynamic Adaptive Streaming over HTTP): DASH does not support Apple devices. Both HLS and DASH support adaptive bitrate streaming.  License This work is licensed under CC BY-NC-ND 4.0   </readme><commitcount>6</commitcount><languages /><tags>computer-science	aws	cloud-computing	software-engineering	software-development	interview-questions	coding-interviews	software-architecture	system-design	system-design-interview</tags><about>Explain complex systems using visuals and simple terms. Help you prepare for system design interviews.</about><starcount>13.1k</starcount><watchcount>0</watchcount></repository><repository><username>OpenBMB</username><reponame>XAgent</reponame><readme>               XAgent 📖 Introduction  XAgent 🧰 ToolServer ✨ Quickstart 🛠️ Build and Setup ToolServer 🎮 Setup and Run XAgent 🎬 Demo Case 1. Data Analysis: Demonstrating the Effectiveness of Dual-Loop Mechanism Case 2. Recommendation: A New Paradigm of Human-Agent Interaction Case 3. Training Model: A Sophisticated Tool User 📊 Evaluation 🖌️ Blog Citation      README.md       XAgent             English •     中文   Tutorial •   Demo •   Blog •   Citation  📖 Introduction XAgent is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks. It is designed to be a general-purpose agent that can be applied to a wide range of tasks. Currently, XAgent is still in its early stage, and we are working hard to improve it. 🏆 Our goal is to create a super-intelligent agent that can solve any given task! We welcome diverse forms of collaborations, including full-time, part-time roles, and more. If you are interested in the frontiers of agents and want to join us in realizing true autonomous agents, please contact us at xagentteam@gmail.com.  XAgent XAgent is designed with the following features:  Autonomy: XAgent can automatically solve various tasks without human participation. Safety: XAgent is designed to run safely. All actions are constrained inside a docker container. Run it anyway! Extensibility: XAgent is designed to be extensible. You can easily add new tools to enhance agent's abilities and even new agents！ GUI: XAgent provides a friendly GUI for users to interact with the agent. You can also use the command line interface to interact with the agent. Cooperation with Human: XAgent can collaborate with you to tackle tasks. It not only has the capability to follow your guidance in solving complex tasks on the go but also can seek your assistance when it encounters challenges.  XAgent is composed of three parts:  🤖 Dispatcher is responsible for dynamically instantiating and dispatching tasks to different agents. It allows us to add new agents and improve the agents' abilities. 🧐 Planner is responsible for generating and rectifying plans for tasks. It divides a task into subtasks and generates milestones for them, allowing agents to solve tasks step by step. 🦾 Actor is responsible for conducting actions to achieve goals and finish subtasks. The actor utilizes various tools to solve subtasks, and it can also collaborate with human to solve tasks.  🧰 ToolServer ToolServer is the server that provides XAgent with powerful and safe tools to solve tasks. It is a docker container that provides a safe environment for XAgent to run. Currently, ToolServer provides the following tools:  📝 File Editor provides a text editing tool that can write, read, and modify files. 📘 Python Notebook provides an interactive Python notebook that can run Python code to validate ideas, draw figures, etc. 🌏 Web Browser provides a web browser that can search and visit webpages. 🖥️ Shell provides a bash shell tool that can execute any shell commands, even install programs and host services. 🧩 Rapid API provides a tool to retrieve APIs from Rapid API and call them, which provides a wide range of APIs for XAgent to use. See ToolBench to get more information about the Rapid API collections. You can also easily add new tools to ToolServer to enhance XAgent's abilities.   ✨ Quickstart 🛠️ Build and Setup ToolServer ToolServer is where XAgent's action takes place. It is a docker container that provides a safe environment for XAgent to run. So you should install docker and docker-compose first. After that, you should build the docker image for ToolServer and start the docker container. cd ToolServer docker-compose up Refer here for detailed information about our ToolServer. 🎮 Setup and Run XAgent After setting up ToolServer, you can start to run XAgent.  Install requirements (Require Python &gt;= 3.10)  pip install -r requirements.txt  Configure XAgent  You should configure XAgent in config.yml before running it. At least one OpenAI key is provided in config.yml, which is used to access OpenAI API. We highly recommend using gpt-4-32k to run XAgent, gpt-4 is also OK for most simple tasks. In any case, at least one gpt-3.5-turbo-16k API key should be provided as a backup model. We do not test or recommend using gpt-3.5-turbo to run XAgent due to very limited context length, you should not try to run XAgent on that.  Run XAgent  python run.py --task "put your task here" --model "gpt-4" You can use argument --upload_files to select files you want to submit to XAgent. The local workspace for your XAgent is in local_workspace, where you can find all the files generated by XAgent throughout the running process. Besides, in running_records you can find all the intermediate steps information, e.g. task statuses, LLM's input-output pairs, used tools, etc. After execution, the full workspace in ToolServerNode will be copied to running_records for your convenience. If you want to load from a existing record, set record_dir in config,default to Null. All the runs will set to a record automatically, We remove api key and other unsafe items, so you can share your run with other people by sharing the records.  Run XAgent with GUI  cd XAgentServer docker-compose up  # also, you can start with python # python3.10+ is required cd XAgentServer pip install -r requirements.txt cd .. python start_server.py  # you can run web page with following command as well: cd ../XAgentWeb npm install npm run dev Build the docker image for XAgent-Server and start the docker container. You will see the XAgent Server listening on port 8090. You could visit http://localhost:5173 to interact with XAgent by using web ui. Refer here for the detailed information about our GUI Demo.  🎬 Demo Here we also show some cases of solving tasks by XAgent: You can check our live demo on XAgent Official Website. We also provide a video demo and show cases of using XAgent here:  Case 1. Data Analysis: Demonstrating the Effectiveness of Dual-Loop Mechanism We start with a case of aiding users in intricate data analysis. Here, our user submitted an iris.zip file to XAgent, seeking assistance in data analysis. XAgent swiftly broke down the task into four sub-tasks: (1) data inspection and comprehension, (2) verification of the system's Python environment for relevant data analysis libraries, (3) crafting data analysis code for data processing and analysis, and (4) compiling an analytical report based on the Python code's execution results. Here is a figure drawn by XAgent.  Case 2. Recommendation: A New Paradigm of Human-Agent Interaction Empowered with the unique capability to actively seek human assistance and collaborate in problem-solving, XAgent continues to redefine the boundaries of human-agent cooperation. As depicted in screenshot below, a user sought XAgent's aid in recommending some great restaurants for a friendly gathering, yet failed to provide specific details. Recognizing the insufficiency of the provided information, XAgent employed the AskForHumanHelp tool, prompting human intervention to elicit the user's preferred location, budget constraints, culinary preferences, and any dietary restrictions. Armed with this valuable feedback, XAgent seamlessly generated tailored restaurant recommendations, ensuring a personalized and satisfying experience for the user and their friends.  Case 3. Training Model: A Sophisticated Tool User XAgent not only tackles mundane tasks but also serves as an invaluable aid in complex tasks such as model training. Here we show a scenario where a user desires to analyze movie reviews and evaluate the public sentiment surrounding particular films. In response, XAgent promptly initiates the process by downloading the IMDB dataset to train a cutting-edge BERT model (see screenshot below), harnessing the power of deep learning. Armed with this trained BERT model, XAgent seamlessly navigates the intricate nuances of movie reviews, offering insightful predictions regarding the public's perception of various films.    📊 Evaluation We conduct human preference evaluation to evaluate XAgent's performance. We prepare over 50 real-world complex tasks for evaluation, which can be categorized into 5 classes: Search and Report, Coding and Developing, Data Analysis, Math, and Life Assistant. We compare the results of XAgent with AutoGPT, which shows a total win of XAgent over AutoGPT. All running records will be released soon.  We report a significant improvement of XAgent over AutoGPT in terms of human preference. We also evaluate XAgent on the following benchmarks:   🖌️ Blog Our blog is available at here!  Citation If you find our repo useful, please kindly consider citing: @misc{xagent2023,       title={XAgent: An Autonomous Agent for Complex Task Solving},        author={XAgent Team},       year={2023}, }    </readme><commitcount>77</commitcount><languages>TypeScript 42.3%	Python 42.0%	Vue 14.5%	CSS 0.6%	Dockerfile 0.3%	JavaScript 0.1%</languages><tags /><about>An Autonomous LLM Agent for Complex Task Solving</about><starcount>1.7k</starcount><watchcount>0</watchcount></repository><repository><username>ionic-team</username><reponame>ionic-framework</reponame><readme>                  Ionic  Quickstart ·      Documentation    · Contribute · Blog   Community:   Discord · Forums · Twitter Packages Getting Started Migration Guides Examples Contributing Future Goals Earlier Versions      README.md             Ionic     Ionic is an open source app development toolkit for building modern, fast, top-quality cross-platform native and Progressive Web Apps from a single codebase with JavaScript and the Web.     Ionic is based on Web Components, which enables significant performance, usability, and feature improvements alongside support for popular web frameworks like Angular, React, and Vue.                 Quickstart  ·       Documentation     ·  Contribute  ·  Blog    Community:   Discord  ·  Forums  ·  Twitter  Packages    Project Package Version Downloads Links     Core @ionic/core   README.md   Angular @ionic/angular   README.md   Vue @ionic/vue   README.md   React @ionic/react   README.md    Looking for the ionic-angular package? Ionic 3 has been moved to the ionic-v3 repo. See Earlier Versions. Getting Started Start a new project by following our quick Getting Started guide. We would love to hear from you! If you have any feedback or run into issues using our framework, please file an issue on this repository. Migration Guides Already have an Ionic app? These guides will help you migrate to the latest versions.  Migrate from v6 to v7 Migrate from v5 to v6 Migrate from v4 to v5 Migrate from v3 to v4  Examples The Ionic Conference App is a full featured Ionic app. It is the perfect starting point for learning and building your own app.  Angular Ionic Conference App React Ionic Conference App  Contributing Thanks for your interest in contributing! Read up on our guidelines for contributing and then look through our issues with a help wanted label. Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms. Future Goals As Ionic Framework components migrate to the web component standard, a goal of ours is to have Ionic Framework easily work within all of the popular frameworks. Earlier Versions The source code for earlier versions of the Ionic Framework may exist in other repositories. Please open issues and pull requests in their respective repositories.  Ionic 2/3: Moved to ionic-team/ionic-v3 Ionic 1: Moved to ionic-team/ionic-v1    </readme><commitcount>13,258</commitcount><languages>TypeScript 55.9%	HTML 29.0%	SCSS 11.5%	JavaScript 2.0%	Vue 1.0%	CSS 0.5%</languages><tags>react	javascript	ios	angular	mobile	framework	typescript	web	ionic	pwa	vue	material-design	webcomponents	frontend	stencil	capacitor	stenciljs</tags><about>A powerful cross-platform UI toolkit for building native-quality iOS, Android, and Progressive Web Apps with HTML, CSS, and JavaScript.</about><starcount>49.6k</starcount><watchcount>0</watchcount></repository><repository><username>cpacker</username><reponame>MemGPT</reponame><readme>               MemGPT 🤖 Create perpetual chatbots with self-editing memory! 🗃️ Chat with your data - talk to your SQL database or your local files! 📄 You can also talk to docs - for example ask about LlamaIndex! Quick setup What is MemGPT? Running MemGPT locally main.py flags Interactive CLI commands Example applications Use MemGPT to talk to your Database! Loading local files into archival memory Enhance with embeddings search Talking to LlamaIndex API Docs Support Datasets 🚀 Project Roadmap      README.md      MemGPT  Try out our MemGPT chatbot on Discord!     🤖 Create perpetual chatbots with self-editing memory!       🗃️ Chat with your data - talk to your SQL database or your local files! SQL Database    Local files      📄 You can also talk to docs - for example ask about LlamaIndex!     ChatGPT (GPT-4) when asked the same question:        (Question from run-llama/llama_index#7756)     Quick setup Join Discord and message the MemGPT bot (in the #memgpt channel). Then run the following commands (messaged to "MemGPT Bot"):  /profile (to create your profile) /key (to enter your OpenAI key) /create (to create a MemGPT chatbot)  Make sure your privacy settings on this server are open so that MemGPT Bot can DM you:  MemGPT → Privacy Settings → Direct Messages set to ON    You can see the full list of available commands when you enter / into the message box.    What is MemGPT? Memory-GPT (or MemGPT in short) is a system that intelligently manages different memory tiers in LLMs in order to effectively provide extended context within the LLM's limited context window. For example, MemGPT knows when to push critical information to a vector database and when to retrieve it later in the chat, enabling perpetual conversations. Learn more about MemGPT in our paper. Running MemGPT locally Install dependencies: pip install -r requirements.txt Extra step for Windows: # only needed on Windows pip install pyreadline Add your OpenAI API key to your environment: # on Linux/Mac export OPENAI_API_KEY=YOUR_API_KEY # on Windows set OPENAI_API_KEY=YOUR_API_KEY To run MemGPT for as a conversation agent in CLI mode, simply run main.py: python3 main.py To create a new starter user or starter persona (that MemGPT gets initialized with), create a new .txt file in /memgpt/humans/examples or /memgpt/personas/examples, then use the --persona or --human flag when running main.py. For example: # assuming you created a new file /memgpt/humans/examples/me.txt python main.py --human me.txt main.py flags --persona   load a specific persona file --human   load a specific human file --first   allows you to send the first message in the chat (by default, MemGPT will send the first message) --debug   enables debugging output --archival_storage_faiss_path=&lt;ARCHIVAL_STORAGE_FAISS_PATH&gt;   load in document database (backed by FAISS index) --archival_storage_files="&lt;ARCHIVAL_STORAGE_FILES_GLOB_PATTERN&gt;"   pre-load files into archival memory --archival_storage_files_compute_embeddings="&lt;ARCHIVAL_STORAGE_FILES_GLOB_PATTERN&gt;"   pre-load files into archival memory and also compute embeddings for embedding search --archival_storage_sqldb=&lt;SQLDB_PATH&gt;   load in SQL database  Interactive CLI commands While using MemGPT via the CLI you can run various commands: //   enter multiline input mode (type // again when done) /exit   exit the CLI /save   save a checkpoint of the current agent/conversation state /load   load a saved checkpoint /dump   view the current message log (see the contents of main context) /memory   print the current contents of agent memory /pop   undo the last message in the conversation /heartbeat   send a heartbeat system message to the agent /memorywarning   send a memory warning system message to the agent  Example applications  Use MemGPT to talk to your Database! MemGPT's archival memory let's you load your database and talk to it! To motivate this use-case, we have included a toy example. Consider the test.db already included in the repository.    id name age     1 Alice 30   2 Bob 25   3 Charlie 35    To talk to this database, run: python main.py  --archival_storage_sqldb=memgpt/personas/examples/sqldb/test.db And then you can input the path to your database, and your query. Please enter the path to the database. test.db ... Enter your message: How old is Bob? ... 🤖 Bob is 25 years old.   Loading local files into archival memory  MemGPT enables you to chat with your data locally -- this example gives the workflow for loading documents into MemGPT's archival memory. To run our example where you can search over the SEC 10-K filings of Uber, Lyft, and Airbnb,   Download the .txt files from Hugging Face and place them in memgpt/personas/examples/preload_archival.   In the root MemGPT directory, run python3 main.py --archival_storage_files="memgpt/personas/examples/preload_archival/*.txt" --persona=memgpt_doc --human=basic   If you would like to load your own local files into MemGPT's archival memory, run the command above but replace --archival_storage_files="memgpt/personas/examples/preload_archival/*.txt" with your own file glob expression (enclosed in quotes). Enhance with embeddings search In the root MemGPT directory, run python3 main.py --archival_storage_files_compute_embeddings="&lt;GLOB_PATTERN&gt;" --persona=memgpt_doc --human=basic This will generate embeddings, stick them into a FAISS index, and write the index to a directory, and then output:   To avoid computing embeddings next time, replace --archival_storage_files_compute_embeddings=&lt;GLOB_PATTERN&gt; with     --archival_storage_faiss_path=&lt;DIRECTORY_WITH_EMBEDDINGS&gt; (if your files haven't changed).  If you want to reuse these embeddings, run python3 main.py --archival_storage_faiss_path="&lt;DIRECTORY_WITH_EMBEDDINGS&gt;" --persona=memgpt_doc --human=basic   Talking to LlamaIndex API Docs MemGPT also enables you to chat with docs -- try running this example to talk to the LlamaIndex API docs!   a. Download LlamaIndex API docs and FAISS index from Hugging Face. # Make sure you have git-lfs installed (https://git-lfs.com) git lfs install git clone https://huggingface.co/datasets/MemGPT/llamaindex-api-docs mv llamaindex-api-docs -- OR -- b. Build the index:  Build llama_index API docs with make text. Instructions here. Copy over the generated _build/text folder to memgpt/personas/docqa. Generate embeddings and FAISS index. cd memgpt/personas/docqa python3 scrape_docs.py python3 generate_embeddings_for_docs.py all_docs.jsonl python3 build_index.py --embedding_files all_docs.embeddings.jsonl --output_index_file all_docs.index      In the root MemGPT directory, run python3 main.py --archival_storage_faiss_path=&lt;ARCHIVAL_STORAGE_FAISS_PATH&gt; --persona=memgpt_doc --human=basic where ARCHIVAL_STORAGE_FAISS_PATH is the directory where all_docs.jsonl and all_docs.index are located. If you downloaded from Hugging Face, it will be memgpt/personas/docqa/llamaindex-api-docs. If you built the index yourself, it will be memgpt/personas/docqa.    Support If you have any further questions, or have anything to share, we are excited to hear your feedback!  By default MemGPT will use gpt-4, so your API key will require gpt-4 API access For issues and feature requests, please open a GitHub issue or message us on our #support channel on Discord  Datasets Datasets used in our paper can be downloaded at Hugging Face. 🚀 Project Roadmap   Release MemGPT Discord bot demo (perpetual chatbot)  Add additional workflows (load SQL/text into MemGPT external context)  CLI UI improvements  Integration tests  Integrate with AutoGen  Add official gpt-3.5-turbo support  Add support for other LLM backends  Release MemGPT family of open models (eg finetuned Mistral)    </readme><commitcount>131</commitcount><languages>Python 100.0%</languages><tags>chat	chatbot	gpt	gpt-4	llm	llm-agent</tags><about>Teaching LLMs memory management for unbounded context 📚🦙</about><starcount>3.1k</starcount><watchcount>0</watchcount></repository><repository><username>Alex313031</username><reponame>thorium</reponame><readme>            Thorium Chromium fork for linux named after radioactive element No. 90. Windows/MacOS/RasPi/Android/Other builds see below. Other Builds             FEATURES &amp; DIFFERENCES BETWEEN CHROMIUM AND THORIUM  Building  Debugging       README.md         Thorium  Chromium fork for linux named after radioactive element No. 90. Windows/MacOS/RasPi/Android/Other builds see below.  Always built with the latest stable version of Chromium. Intended to behave like and have the featureset of Google Chrome, with differences/patches/enhancements listed below. Includes Widevine, All Codecs, Chrome Plugins, as well as thinLTO, CFlag, LDFlag, LLVM Loop, and PGO compiler optimizations. It is built with SSE4, AVX, and AES, so it won't launch on CPU's below 2nd gen Core or AMD FX, but benefits from Advanced Vector EXtensions. If your CPU lacks AVX, you can use builds from Thorium Special.  Other Builds         – Windows builds are here &gt; Thorium Win   – AVX2 Builds for Windows and Linux &gt; Thorium AVX2   – MacOS (M1 and X64) builds are located at &gt; Thorium Mac   – Android (arm32 &amp; arm64) builds are located at &gt; Thorium Android I might also occasionally post x86 builds.   – Raspberry Pi builds are located at &gt; Thorium Raspi For the Pi 3B/3B+ and Pi 4/400.   – Special builds are located at &gt; Thorium Special You can find SSE3 builds for CPUs without AVX here.   – Thorium Website with deb repo for auto-updating on Linux &gt; https://thorium.rocks/   – NEW: Windows 7 / 8 / 8.1 / Server 2012 builds in &gt; Thorium Win7 FEATURES &amp; DIFFERENCES BETWEEN CHROMIUM AND THORIUM     Various compiler flags that improve performance and target AVX CPU's (read PATCHES.md) Experimental MPEG-DASH support. HEVC/H.265 support on Linux and Windows. JPEG XL Image File Format turned on by default. Enable Live Caption (SODA) on all builds. Experimental PDF annotation support (called "Ink" on ChromiumOS). # DISABLED FOR NOW BECAUSE OF CRASHES. Patches from Debian including font rendering patch, VAAPI Patch, Intel HD support patch, native notifications patch, title bar patch, and... the VDPAU Patch!! (Rejoice Nvidia users) VAAPI on Wayland Patch (Thanks AUR and @pierro78) Audio Sandbox patch. DoH (DNS over HTTPS) patches from Bromite. Enable Do Not Track by default patch from Vanadium. Show full URLs including trailing slashes in address bar by default. Disable FLOC patch. Disable annoying Google API Key Infobar warning (you can still use API Keys to enable sync) from Ungoogled Chromium. Disable annoying Default Browser Infobar warning. Adds DuckDuckGo, Brave Search, Ecosia, Ask.com, and Yandex.com in US and other locales, along with the normal search engines. Always use the local NTP (New Tab Page) regardless of search engine. Fix icons when distilling page content in Reader Mode. Enable new Menu UI when right clicking the Reload button. (Adds 'Normal Reload', 'Hard Reload', and 'Clear Cache and Hard Reload') Home button and Chrome Labs shown by Default. Prefetch settings updated to respect privacy. Patches for GN and chrome_sandbox when building. Remove the addition of the Chrome APT sources.list during installation. Widevine CDM Patch for Linux. GTK auto dark mode patch Various new flags either developed from scratch, or added from Ungoogled Chromium. See PATCHES.md Enable Parallel Downloading by Default. Inclusion of  pak a utility for packing and unpacking the *.pak files in Thorium or any other Chromium based browser. Logo and Branding/Naming changed to the Thorium logo, Thorium name, and "Alex313031" being appended to "The Chromium Authors" in credits, etc. .desktop file includes useful cmdline flags that enable experimental or useful features. (See PATCHES.md) Includes installer patches and files to include ChromeDriver and thorium_shell (content_shell), with a .desktop file being provided for thorium_shell (named thorium-shell.desktop and shows in desktop environments as Thorium Content Shell). These are also included in the Windows releases, but it doesn't make a shorcut, although a .png and .ico is in the install directory for you to make your own shortcut with an icon. You can also run content_shell with the command thorium-shell (custom wrapper for it, located in /usr/bin/). You can run ChromeDriver at /usr/bin/chromedriver or chromedriver.exe on Windows. Also, patches for abseil library and mini_installer when building with AVX on Windows. Right clicking the launcher after install gives three additional desktop actions, one to open thorium-shell, another to open in Safe Mode which disables any flags one has set in chrome://flags until the next launch, and lastly to open in Dark Mode which appends the --force-dark-mode flag.    For more info, read the PATCHES.md file. Known bugs are in the BUGS.md file. A list of Chromium command line flags can be found at &gt; https://peter.sh/experiments/chromium-command-line-switches  Building   See &gt; https://github.com/Alex313031/thorium/blob/main/docs/BUILDING.md Debugging  See &gt; https://github.com/Alex313031/thorium/tree/main/infra/DEBUG#readme   − https://www.reddit.com/r/ChromiumBrowser/ is a subreddit I made for Thorium and general Thorium/Chromium discussion, https://thorium.rocks/ is the website I made for it, and https://alex313031.blogspot.com/ is a blog I made relating to Thorium/ThoriumOS.   − I also build ChromiumOS (now called ThoriumOS) with Thorium, Codecs, Widevine, linux-firmware/modules, and extra packages at &gt; https://github.com/Alex313031/ChromiumOS/  − Thanks to https://github.com/robrich999/ for some info and fixes that went into this project.  − Thanks to https://github.com/midzer/ for support and helping with builds.   − Also thanks to https://github.com/bromite/bromite, https://github.com/saiarcot895/chromium-ubuntu-build, https://github.com/Eloston/ungoogled-chromium, https://github.com/GrapheneOS/Vanadium, and https://github.com/iridium-browser/iridium-browser for patch code.   − The pak_src dir, and the binaries in pack_src/bin are credited to @freeer https://github.com/myfreeer/chrome-pak-customizer/  NOTE: libpepflashplayer.so is included for posterity and can be used to enable Adobe Flash on older Chromium releases. ʘ‿ʘ Thanks for using Thorium!      </readme><commitcount>4,019</commitcount><languages>C++ 68.1%	HTML 19.2%	Python 4.2%	Shell 3.2%	JavaScript 3.0%	C 1.4%</languages><tags>linux	chrome	aes	chrome-devtools	chromium	avx	thorium	chromedriver	web-browser	content-shell	webbrowser	web-platform	avx-instructions	chromium-browser	jxl	jpeg-xl	jpegxl	libjxl	thorium-browser	thoriumos</tags><about>Chromium fork named after radioactive element No. 90. Windows and MacOS/Raspi/Android/Special builds are in different repositories, links are towards the top of the README.md.</about><starcount>2k</starcount><watchcount>0</watchcount></repository><repository><username>cloudcommunity</username><reponame>Free-Certifications</reponame><readme>            Free Certifications Security Project Management Marketing Database Others      README.md     Free Certifications A curated list of free courses &amp; certifications. Don't forget to star ⭐ this repository. The offers on top of the table are time-limited and will expire soon. So, hurry up and grab them first! 🎉 Brought to you by the Cloud Study Network - a global tech community that shares knowledge, goodies and good vibes! 🎉    Technology Provider Description Link Expiration     Gitlab Certification Gitlab Free Certifications paths and badges Link Unknown   OCI Oracle Oracle Cloud Infrastructure, Oracle Cloud Infrastructure, Oracle Cloud Infrastructure Link September 31, 2023   AI Fundamentals Databricks Generative AI Fundamentals Link Unlimited   OCI Oracle Oracle Cloud Infrastructure 2023 Foundations Associate Link Unlimited   OCI Oracle Oracle Cloud Data Management 2023 Foundations Associate Link Unlimited   Azure Pluralsight Free subscription at Pluralsight for Microsoft Azure courses Link 01-Jan-2025   Cloud Networking Aviatrix Aviatrix, Free ACE - Multicloud Networking associate Course &amp; Certificate code ACEMULTICLOUD Link Limited Time   Microsoft Build: Azure Developer Challenge Learn how to design, build, test, and maintain cloud applications and services on Microsoft Azure. Link 21-Jun-2022   Cloud Networking Aviatrix Aviatrix, Free ACE - Multicloud Networking associate Course &amp; Certificate code ACEMULTICLOUD Link Limited Time   JumpCloud JumpCloud Free JumpCloud Core Certification (worth $150). Link Limited Time   Google Cloud QwikLabs Extra 30 days for Google Cloud labs on Qwiklabs. Link Unknown   Automated Testing Test Automation University Free certification courses by the Test Automation University. Link Unknown   Security Juniper Networks Courses and certifications for free by Juniper Networks (instead of 150 euros). Link Unknown   Google Cloud Qwiklabs Claim 30 days of free Qwiklabs and access to the featured labs. Link Unknown   Huawei Networking Huawei Academy Free courses &amp; exams from Huawei Academy for the HCIA, HCIP, and HCIE certifications. Link Unknown   Huawei Networking Huawei Academ Free course &amp; certification (HCIA level, $200 value). Link Unknown   Programming JetBrains Free courses by JetBrains Academy for learning Java, Kotlin &amp; Python. Link Unknown   Cloud Monitoring Elastic Free access to 11 Elastic Stack courses ($200 value each). Link Unknown   Diverse Udacity One free month of access to nanodegree programs by Udacity ($400 value). Credit card / PayPal required. Don’t forget to cancel in time. Link Unknown   Alibaba Cloud Coursera “Architecting on Alibaba Cloud Specialization” at Coursera. Link Unknown   DevOps The Linux Foundation The Linux Foundation offers 23 free courses with finalizing exams &amp; confirmations. Link Unknown   DevOps CloudBees University Free training through CloudBees University (Jenkins, DevOps). Link Unknown   Data Analytics Sumo Logic 6 free training courses and certifications by Sumo Logic. Link Unlimited   Web Development Freecodecamp 6 Free Code Camp learning courses &amp; certifications, incl. RWD, JavaScript, APIs, React… Link Unlimited   Diverse Udemy ~670 free courses at Udemy, incl. certificates. Link Unlimited   cPanel Cpanel University Free cPanel Professional Certification (CPP) awarded simply by successfully completing the full series of video lessons. There is no final certification exam required for the CPP status. Link Unlimited   Plesk Plesk University Free Plesk Obsidian Professional Certification Link Unlimited   SolusIO Plesk University Free SolusIO Professional Certification Link Unlimited   Analytics Google Analytics Academy Google Analytics Academy free courses with certificates Link Unlimited   SD-WAN Silver Peak Silver Peak , offers Free Training &amp; Certification Exam for SD-WAN Profissional Link Unknown   Data Science IBM Cognitive Class Data science courses with proof of completion and badge Link Unlimited   IT sector Bitdegree gain or improve digital skills on our eLearning platform Link Unknown   SysTrack Lakeside Software Free SysTrack certification and badge upon completion of self-paced courses through their online learning platform. They offer three courses: SysTrack Technician, SysTrack Engineer, and SysTrack Dashboard Designer. Link Unknown   Eggplant Eggplant Free Eggplant courses and certifications. Link Unknown   AWS AWS Free full-time AWS training with certification and launch a career in cloud computing with AWS re/Start for unemployed and underemployed individuals. Link Unknown   AWS AWS AWS Cloud Quest: Cloud Practitioner Link Unknown   AWS AWS AWS Solutions Architect - Knowledge Badge Readiness Path Link Unknown   API API Academy Free API Designer and API Security Architect certifications. Link Unknown   API Gravitee.io Event-Native API Management Fundamentals Certification Link Unknown   API Gravitee.io Event-Native API Management Professional Certification Link Unknown   Zerto Zerto Free Zerto Associate Certification on Zerto University. Link Unknown   Calico Calico Certified Calico Operator: Level 1. Link Unknown   Calico Calico Certified Calico Operator: AWS Expert Link Unknown   Calico Calico Certified Calico Operator: Azure Expert Link Unknown   Calico Calico Certified Calico Operator: eBPF Link Unknown   Stepik Stepik Several free courses with certificates are available for reference in design, computer science, mathematics, etc. Link Unknown   LoRaWAN TheThingsNetwork The Things Fundamentals, The Things Advanced, The Things Security, and The Things Network Management. Link Unknown   DevOps-Automation CHEF FREE Chef Principles Certification Exam Link Unknown   Observability New Relic FREE Full Stack Observability Exam Link Unknown   Programmability New Relic FREE Programmability Certification Exam Link Unknown   Chaos Gremlin FREE Gremlin Certified Chaos Engineering practitioner Certification Link Unknown   Neo4j Neo4j, Inc. Become a Neo4j Certified Professional Link Unknown   Confluent Confluent Fundamentals Accreditation Link Unknown   Confluence Confluence Fundamentals Accreditation Link Unknown   Gatling Gatling Academy Become an ace on Gatling’s Load Testing tool! Link Unknown   Appium HeadSpin Appium Advanced Certifications Link Unknown   Selenium LambdaTest Selenium Advanced Certifications Link Unknown   DeepLearning AI Deep Learning Deep Learning Specialization. Master Deep Learning, and Break into AI Link Unknown   Kubernetes Kasten.io by Veeam Free Kubernetes Training Link Unknown   GraphQL &amp; Apollo Odyssey by Apollo Apollo Graph Developer - Associate Certification Link Unknown   Kyverno Nirmata Kyverno Fundamentals Certification Link Unknown   Cilium Isovalent Getting Started with Cilium Link Unknown   Cilium Isovalent Cilium Service Mesh Link Unknown   Cilium Isovalent Cilium Cluster Mesh Link Unknown   Cilium Isovalent Cilium Enterprise: Network Policies Link Unknown   Cilium Isovalent Cilium LoadBalancer IPAM and BGP Service Advertisement Link Unknown   Cilium Isovalent Cilium Enterprise: Zero Trust Visibility Link Unknown   Cilium Isovalent Cilium IPv6 Networking and Observability Link Unknown   Cilium Isovalent Cilium Gateway API Link Unknown   Cilium Isovalent HTTP Golden Signals with Hubble and Grafana Link Unknown   Cilium Isovalent Advanced Gateway API Use Cases Link Unknown   Cilium Isovalent Getting Started with Tetragon Link Unknown   Cilium Isovalent Mutual Authentication with Cilium Link Unknown   Cilium Isovalent Cilium Transparent Encryption with IPSec and WireGuard Link Unknown   Cilium Isovalent Isovalent Enterprise for Cilium: Security Visibility Link Unknown   AWS Skill Builder AWS A repository of over 700 training lessons to help you learn AWS, refine your knowledge of AWS services, and improve your skills so you can put them into practice or apply the knowledge during the many AWS certifications. Link Unknown   Integration Boomi Professional API Management Certification Link Unknown   Integration Boomi Professional API Design Certification Link Unknown   Integration Boomi Associate Developer Certification Link Unknown   Integration Boomi Associate EDI for X12 Certification Link Unknown   Integration Boomi Associate Flow Essentials Certification Link Unknown   Integration Boomi Associate Master Data Hub Certification Link Unknown   Integration Boomi Development and Application Architecture Certification Link Unknown   Integration Boomi Professional Developer Certification Link Unknown   Integration Boomi Professional Flow Developer Certification Link Unknown   Integration Boomi Associate Administrator Certification Link Unknown   Integration Boomi Professional Linux Operational Administrator Certification Link Unknown   Integration Boomi Professional Windows Operational Administrator Certification Link Unknown   Apache Kafka &amp; OpenShift RedHat Free Course On Event-Driven Architecture with Apache Kafka and Red Hat OpenShift Application Services Technical Overview Link Unknown   OpenStack RedHat Free Course On Red Hat OpenStack Technical Overview Link Unknown   Ansible RedHat Free Course On Ansible Basics: Automation Technical Overview Link Unknown   RedHat Agile RedHat Free Course On Red Hat Agile Integration Technical Overview Link Unknown   Kubernetes and OpenShift RedHat Free Course On Containers, Kubernetes and Red Hat OpenShift Technical Overview Link Unknown   Cloud-Native RedHat Free Course On Developing Cloud-Native Applications with Microservices Architectures Link Unknown   Virtualization RedHat Free Course On Virtualization and Infrastructure Migration Technical Overview Link Unknown   RHE Linux RedHat Free Course On Red Hat Enterprise Linux Technical Overview Link Unknown   Ansible Automation for SAP RedHat Free Course On Red Hat Ansible Automation for SAP Technical Overview Link Unknown   Red Hat Satellite RedHat Free Course On Red Hat Satellite Technical Overview Link Unknown   Containers RedHat Free Course On Running Containers with Red Hat Technical Overview Link Unknown   Site Reliability Engineering RedHat Free Course On Transitional approach to implementing pragmatic Site Reliability Engineering (SRE) Technical Overview Link Unknown    Security    Provider Description Link Expiration     Palo Alto Networks Free certification exam coupon code upon completion of certification prep training during the event. Link 18-Nov-2020   Fortinet Free Network Security training courses &amp; certifications by Fortinet / NSE Institute. Link Unlimited   The Academic Council Of uLektz Free Cyber Security training and certification. Link Unknown   APMG International Free Certified Cyber Professional (CCP) Specialism Pilot certification for eligible individuals (see link for info). Link Unknown   Cisco Networking Academy Free Introduction to Cybersecurity course with Networking Academy badge for completing this course. Link Unlimited   SkillFront Free ISO/IEC 27001 Information Security Associate™ Link Unlimited   ISC² Voucher 100%, Certified in Cybersecurity℠(CC) Exam Voucher: CC1M12312023 Link 31-Dec-2023   AttackIQ Foundations of Purple Teaming Link Unlimited   AttackIQ Foundations of Breach &amp; Attack Simulation Link Unlimited   AttackIQ Foundations of Operationalizing MITRE ATT&amp;CK v13 Link Unlimited    Project Management    Provider Description Link Expiration     Certiprof Free “Scrum Foundations Professional Certificate (SFPC)” certification. Available in en, pt-br &amp; es. Use the code “COVID19Support” Link Unknown   Six Sigma Online Free Six Sigma White Belt Training &amp; Certification. Link Unknown   OHSC Free Project Management course and certificate by Oxford Home Study Centre (OHSC). Link Unlimited   Msicertified Free Project Management Essentials Certified (PMEC) training &amp; certification. Link Unlimited   6sigmastudy Free Six Sigma Yellow Belt course &amp; certification. Link Unlimited   ScrumStudy Free “Scrum Fundamentals Certified (SFC™)” training course &amp; certification Link Unlimited   SkillFront Free Certified Associate In Scrum Fundamentals™ (CASF™) Link Unlimited    Marketing    Provider Description Link Expiration     Google Fundamentals of Digital Marketing free course &amp; certificate by Google. Link Unlimited   Microsoft Microsoft Advertising certification and training. Link Unlimited   SMstudy 4 free marketing-related fundamental certifications by SMstudy. Link Unlimited   DMAC Free Facebook &amp; Instagram Marketing course and certification by DMAC (Digital Marketing Academy of Canada). Link Limited Time   Hootsuite [Students only] Free Hootsuite Platform Certification (worth $99) and Social Marketing Certification (worth $199) through Hootsuite's Student Program. Link Unlimited   HubSpot Academy Free marketing &amp; sales courses with certification. Link Unlimited   SEMrush Free Online Digital Marketing Courses and Exams. Link Unlimited   Microsoft Microsoft Advertising Certified Professional. Link Unlimited    Database    Provider Description Link Expiration     Exasol Free Exasol training courses and certifications (€150 value each). Link 30-Jun-2020   Mongodb 12 free MongoDB courses with proof of completion. Link Unlimited   CockcroachLab University Free Cert - the core concepts behind distributed databases and give you all the tools you need to get started with CockroachDB Link Unknown   CockcroachLab University Free Cert - you will build a full-stack ride-sharing application in Python using the popular SQLAlchemy ORM and CockroachDB Link Unknown   Liquibase Learn all about Liquibase fundamentals from free online courses by Liquibase experts and see how to apply them in the real world. Link Unknown   Redis Redis Certified Developer is our professional certification program for software developers who regularly work with Redis. Link Unknown   Databricks Free Training: Databricks Lakehouse Fundamentals. Link Unknown    Others    Provider Description Link Expiration     Salesforce Free Salesforce courses with career learning paths and superbadges. Also, free half-day Salesforce Certification preparation webinar, offering a $70 discount coupon for any $200 exam for all attendees. Link Unlimited   Revenera Revenera Certification, free of charge to approved members of the legal community. Link Unlimited   Kahoot! Kahoot! Certified for schools, this free program is designed to help teachers become the ultimate Kahoot! superheroes. Link Unlimited   Explain Everything Free Explain Everything course and certification. Link Unlimited   SkillFront Free SkillFront Entrepreneur Program™: Foundations Of Business And Entrepreneurship™ Link Unlimited   CertiProf Free Remote Work and Virtual Collaboration - RWVCPC Link Unknown   CertiProf Business Model Canvas Essentials Professional Certification - BMCEPC Link Unknown   CertiProf Business Intelligence Foundation Professional Certification - BIFPC (Spanish Only) Link Unknown   Slack Free Slack Skill Learning Paths and Badges (issued by accredible.com - these are not certifications but badges for skill specialists) Link Unknown   Reuters Reuters Training Course: Introduction to Digital Journalism Link Unknown   OpenSAP openSAP is SAP's free learning platform for everyone interested in learning about SAP's latest innovations and how to survive in the digital economy. Link Unknown   Kami Free online training courses to build your Kami skills and grow as a leader in your professional learning community Link 31-Oct-2021   Miro Miro essentials Link Unknown   Miro Collaborative meetings Link Unknown   Miro Mapping and diagramming Link Unknown   Cloud App Maker, Microsoft Microsoft low-code Cloud App Maker Certification: Register and complete the learning path within the start and end date to get a Free voucher to take the Microsoft Associate level certification Link 2023   EF SET 15-minute English quiz to validate reading skills (English grammar and vocabulary) and listening skills, aligned to CEFR levels. Link Unlimited   EF SET 50-minute test to receive your personalized English certificate to add to your LinkedIn profile or CV, aligned to CEFR levels. Link Unlimited   Microsoft The Microsoft Licensing Specialist certifications demonstrate competence in a particular area of Microsoft product licensing. You will require an intermediate level of knowledge to achieve these certifications. Link Unlimited      </readme><commitcount>238</commitcount><languages /><tags>learning	awesome	free	awesome-list	freebie	certification	exams	hacktoberfest	free-learning	awesome-lists	freebies	free-course	free-courses	certifications	free-certifications	free-coupon	free-certification	free-coupons	free-voucher	free-vouchers</tags><about>A curated list of free courses &amp; certifications.</about><starcount>8.9k</starcount><watchcount>0</watchcount></repository><repository><username>DarkFlippers</username><reponame>unleashed-firmware</reponame><readme>               Welcome to the Flipper Zero Unleashed Firmware repo! This firmware is a fork fromflipperdevices/flipperzero-firmware Most stable custom firmware focused on new features and improvements of original firmware components, with almost no UI changes This software is for experimental purposes only and is not meant for any illegal activity/purposes.  We do not condone illegal activity and strongly encourage keeping transmissions to legal/valid uses allowed by law.  Also, this software is made without any support from Flipper Devices and is in no way related to the official devs. FAQ (frequently asked questions) Dev builds (unstable) Releases in Telegram What's changed Current modified and new Sub-GHz protocols list: Please support development of the project Community apps included 🎲 Download Extra plugins for Unleashed List of Extra pack | List of Base (Deafult) pack Official Flipper Zero Apps Catalog web version or mobile app Instructions First lock official docs docs.flipper.net How to install - versions info: n,,e... Firmware &amp; Development - How to build | Project-structure - CLion IDE - How to setup workspace for flipper firmware development by Savely Krasovsky - "Hello world!" - plugin tutorial English by DroomOne | Russian by Pavel Yakovlev - How to write your own app. Docs by atmanos ⚠️outdated API Firmware &amp; main Apps feature - System: How to change Flipper name - BadUSB: How to add new keyboard layouts - Infrared: How to make captures to add them into Universal IR remotes Sub-GHz - External Radio: How to connect CC1101 module - Transmission is blocked? How to extend Sub-GHz frequency range - How to add extra Sub-GHz frequencies - How to use Flipper as new remote (Nice FlorS, BFT Mitto, Somfy Telis, Aprimatic, AN-Motors, etc..) - Configure Sub-GHz Remote App Not recomeded, please use embedded configurator Plugins - TOTP (Authenticator): config description - Mifare Nested plugin: How to recover keys - Barcode Generator: How to use - Multi Converter: How to use - WAV Player: sample files &amp; how to convert - Sub-GHz playlist: generator script Plugins that works with external hardware [GPIO] - Unitemp - Temperature sensors reader: How to use &amp; supported sensors - [NMEA] GPS: How to use - i2c Tools How to use - [NRF24] plugins: How to use - [WiFi] Scanner: How to use | Web Flasher - [ESP8266] Deauther: How to use | Web Flasher - [ESP32] WiFi Marauder: How to use docs by UberGuidoZ | Marauder repo - [ESP32-CAM] Camera Suite: How to use - How to Upload .bin to ESP32/ESP8266: Windows | FAP "ESP flasher" - [GPIO] SentrySafe plugin: How to use Where I can find IR, Sub-GHz, ... files, DBs, and other stuff? UberGuidoZ Playground - Large collection of files - Github Awesome Flipper Zero - Github Links Project structure      ReadMe.md                        Welcome to the Flipper Zero Unleashed Firmware repo! This firmware is a fork from flipperdevices/flipperzero-firmware  Most stable custom firmware focused on new features and improvements of original firmware components, with almost no UI changes  This software is for experimental purposes only and is not meant for any illegal activity/purposes.  We do not condone illegal activity and strongly encourage keeping transmissions to legal/valid uses allowed by law.  Also, this software is made without any support from Flipper Devices and is in no way related to the official devs.  FAQ (frequently asked questions) Follow this link to find answers to most asked questions Dev builds (unstable)  https://dev.unleashedflip.com/ https://t.me/kotnehleb  Releases in Telegram  https://t.me/unleashed_fw  What's changed   Sub-GHz lib &amp; hal  Regional TX restrictions removed Extra Sub-GHz frequencies Frequency range can be extended in settings file (Warning: It can damage Flipper's hardware) Many rolling code protocols now have the ability to save &amp; send captured signals FAAC SLH (Spa) &amp; BFT Mitto (keeloq secure with seed) manual creation External CC1101 module support (by quen0n)    Sub-GHz Main App  Save last used frequency (by derskythe) New frequency analyzer (by ClusterM) Press OK in frequency analyzer to use detected frequency in Read modes (by derskythe) Long press OK button in Sub-GHz Frequency analyzer to switch to Read menu (by derskythe) New option to use timestamps + protocol name when you saving file, instead of random name - Enable in Radio Settings -&gt; Time in names = ON Read mode UI improvements (shows time when signal was received) (by @wosk) External CC1101 module support (Hardware SPI used) Hold right in received signal list to delete selected signal Custom buttons for Keeloq / Alutech AT4N / Nice Flor S / Somfy Telis / Security+ 2.0 / CAME Atomo - now you can use arrow buttons to send signal with different button code Add manually menu extended with new protocols FAAC SLH, BFT Mitto / Somfy Telis / Nice Flor S / CAME Atomo, etc.. manual creation with programming new remote into receiver (use button 0xF for BFT Mitto, 0x8 (Prog) on Somfy Telis) Debug mode counter increase settings (+1 -&gt; +5, +10, default: +1) Debug PIN output settings for protocol development    Sub-GHz apps by unleashed team  Sub-GHz Bruteforce - static code brute-force plugin |  Time delay (between signals) setting (hold Up in main screen(says Up to Save)) + configure repeats in protocols list by pressing right button on selected protocol Load your own file and select bytes you want to bruteforce or use preconfigured options in protocols list   Sub-GHz Remote - remote control for 5 sub-ghz files | bind one file for each button  use the built-in constructor or make config file by following this instruction      Infrared  Recompiled IR TV Universal Remote for ALL buttons Universal remotes for Projectors, Fans, A/Cs and Audio(soundbars, etc.) -&gt; Also always updated and verified by our team Infrared -&gt; RCA Protocol Infrared -&gt; Debug TX PIN output settings    NFC/RFID/iButton  LFRFID/iButton Fuzzer plugins Extra Mifare Classic keys Add manually -&gt; Mifare Classic with custom UID Picopass/iClass plugin (now with emulation support!) included in releases    Quality of life &amp; other features  Customizable Flipper name Update! Now can be changed in Settings-&gt;Desktop (by @xMasterX and @Willy-JL) Text Input UI element -&gt; Cursor feature (by @Willy-JL) Byte Input Mini editor -&gt; Press UP multiple times until the nibble editor appears Clock on Desktop -&gt; Settings -&gt; Desktop -&gt; Show Clock Battery percentage display with different styles Settings -&gt; Desktop -&gt; Battery View More games in Dummy Mode -&gt; click or hold any of arrow buttons Lock device with pin(or regular lock if pin not set) by holding UP button on main screen (by an4tur0r) BadBT plugin (BT version of BadKB) (by Willy-JL, ClaraCrazy, XFW contributors) - (See in Applications-&gt;Tools) - (aka BadUSB via Bluetooth) BadUSB -&gt; Keyboard layouts (by rien &gt; dummy-decoy) Custom community plugins and games added + all known working apps can be downloaded in extra pack in every release Other small fixes and changes throughout See other changes in readme below    Also check the changelog in releases for latest updates! Current modified and new Sub-GHz protocols list: Thanks to Official team (to their SubGHz Developer, Skorp) for implementing decoders for these protocols in OFW. Keeloq [Not ALL systems supported for decode or emulation yet!] - Supported manufacturers list Encoders or sending made by @xMasterX:  Nero Radio 57bit (+ 56bit encoder improvements) CAME 12bit/24bit encoder fixes (Fixes now merged in OFW) Keeloq: HCS101 Keeloq: AN-Motors Keeloq: JCM Tech Keeloq: MHouse Keeloq: Nice Smilo Keeloq: DTM Neo Keeloq: FAAC RC,XT Keeloq: Mutancode Keeloq: Normstahl Keeloq: Beninca + Allmatic Keeloq: Stilmatic Keeloq: CAME Space Keeloq: Aprimatic (model TR and similar) Keeloq: Centurion Nova (thanks Carlos !)  Encoders or sending made by @Eng1n33r(first implementation in Q2 2022) &amp; @xMasterX (current version):  CAME Atomo -&gt; Update! check out new instructions Nice Flor S -&gt; How to create new remote - instructions FAAC SLH (Spa) -&gt; Update!!! Check out new instructions Keeloq: BFT Mitto -&gt; Update! Check out new instructions Star Line Security+ v1 &amp; v2 (encoders was made in OFW)  Encoders made by @assasinfil &amp; @xMasterX:  Somfy Telis -&gt; How to create new remote - instructions Somfy Keytis KingGates Stylo 4k Alutech AT-4N -&gt; How to create new remote - instructions Nice ON2E (Nice One) -&gt; How to create new remote - instructions  Please support development of the project The majority of this project is developed and maintained by me, @xMasterX. I'm unemployed, and the only income I receive is from your donations. Our team is small and the guys are working on this project as much as they can solely based on the enthusiasm they have for this project and the community.  @gid9798 - SubGHz, Plugins, many other things @assasinfil - SubGHz protocols @Svaarich - UI design and animations @amec0e &amp; @Leptopt1los - Infrared assets Community moderators in Telegram, Discord, and Reddit And of course our GitHub community. Your PRs are a very important part of this firmware and open-source development.  The amount of work done on this project is huge and we need your support, no matter how large or small. Even if you just say, "Thank you Unleashed firmware developers!" somewhere. Doing so will help us continue our work and will help drive us to make the firmware better every time. Also, regarding our releases, every build has and always will be free and open-source. There will be no paywall releases or closed-source apps within the firmware. As long as I am working on this project it will never happen. You can support us by using links or addresses below:    Service Remark Link/Wallet     Patreon  https://patreon.com/mmxdev   Boosty patreon alternative https://boosty.to/mmxdev   cloudtips only RU payments accepted https://pay.cloudtips.ru/p/7b3e9d65   YooMoney only RU payments accepted https://yoomoney.ru/fundraise/XA49mgQLPA0.221209   USDT (TRC20) TSXcitMSnWXUFqiUfEXrTVpVewXy2cYhrs   BCH  qquxfyzntuqufy2dx0hrfr4sndp0tucvky4sw8qyu3   ETH (BSC/ERC20-Tokens) darkflippers.eth (or 0xFebF1bBc8229418FF2408C07AF6Afa49152fEc6a)   BTC  bc1q0np836jk9jwr4dd7p6qv66d04vamtqkxrecck9   DOGE  D6R6gYgBn5LwTNmPyvAQR6bZ9EtGgFCpvv   LTC  ltc1q3ex4ejkl0xpx3znwrmth4lyuadr5qgv8tmq8z9   XMR (Monero) 41xUz92suUu1u5Mu4qkrcs52gtfpu9rnZRdBpCJ244KRHf6xXSvVFevdf2cnjS7RAeYr5hn9MsEfxKoFDRSctFjG5fv1Mhn   TON  EQCOqcnYkvzOZUV_9bPE_8oTbOrOF03MnF-VcJyjisTZmpGf    Community apps included 🎲 Download Extra plugins for Unleashed List of Extra pack | List of Base (Deafult) pack See full list and sources here: xMasterX/all-the-plugins Official Flipper Zero Apps Catalog web version or mobile app Instructions First lock official docs docs.flipper.net How to install - versions info: n, ,e... Firmware &amp; Development - How to build | Project-structure - CLion IDE - How to setup workspace for flipper firmware development by Savely Krasovsky - "Hello world!" - plugin tutorial English by DroomOne  | Russian by Pavel Yakovlev - How to write your own app. Docs by atmanos ⚠️outdated API Firmware &amp; main Apps feature - System: How to change Flipper name - BadUSB: How to add new keyboard layouts - Infrared: How to make captures to add them into Universal IR remotes Sub-GHz - External Radio: How to connect CC1101 module - Transmission is blocked? How to extend Sub-GHz frequency range - How to add extra Sub-GHz frequencies - How to use Flipper as new remote (Nice FlorS, BFT Mitto, Somfy Telis, Aprimatic, AN-Motors, etc..) - Configure Sub-GHz Remote App Not recomeded, please use embedded configurator Plugins - TOTP (Authenticator): config description - Mifare Nested plugin: How to recover keys - Barcode Generator: How to use - Multi Converter: How to use - WAV Player: sample files &amp; how to convert - Sub-GHz playlist: generator script Plugins that works with external hardware [GPIO] - Unitemp - Temperature sensors reader: How to use &amp; supported sensors - [NMEA] GPS: How to use - i2c Tools How to use - [NRF24] plugins: How to use - [WiFi] Scanner: How to use | Web Flasher - [ESP8266] Deauther: How to use | Web Flasher - [ESP32] WiFi Marauder: How to use docs by UberGuidoZ | Marauder repo - [ESP32-CAM] Camera Suite: How to use - How to Upload .bin to ESP32/ESP8266: Windows | FAP "ESP flasher" - [GPIO] SentrySafe plugin: How to use   Where I can find IR, Sub-GHz, ... files, DBs, and other stuff? UberGuidoZ Playground - Large collection of files - Github Awesome Flipper Zero - Github   Links  Official Docs: docs.flipper.net Official Forum: forum.flipperzero.one  Project structure  applications    - Applications and services used in firmware assets          - Assets used by applications and services furi            - Furi Core: OS-level primitives and helpers debug           - Debug tool: GDB-plugins, SVD-file and etc documentation   - Documentation generation system configs and input files firmware        - Firmware source code lib             - Our and 3rd party libraries, drivers and etc... site_scons      - Build helpers scripts         - Supplementary scripts and python libraries home  Also, pay attention to the ReadMe.md files inside those directories.   </readme><commitcount>5,575</commitcount><languages>C 97.4%	Python 1.1%	HTML 0.9%	PHP 0.3%	C++ 0.3%	Assembly 0.0%</languages><tags>flipper	custom	firmware	jailbreak	unofficial	unlocked	cfw	custom-firmware	unleashed	keeloq	flipper-plugins	rolling-codes	alternative-firmware	flipperzero	flipper-zero	darkflippers</tags><about>Flipper Zero Unleashed Firmware</about><starcount>11.5k</starcount><watchcount>0</watchcount></repository><repository><username>TheRealJoelmatic</username><reponame>RemoveAdblockThing</reponame><readme>               Remove the Adblock Popup from YOUTUBE Introduction Table of Contents: Installation How to bypass the video player ban from using an ad blocker Usage Contribution License      README.md     Remove the Adblock Popup from YOUTUBE  Introduction This repository contains a userscript designed to remove the annoying "Ad blocker are not allowed on Youtube" popup. Follow the instructions below to get started. We also have an undetected way of skipping ads if you get blocked from the video player. Table of Contents:  Introduction Installation How to bypass the video player ban from using an ad blocker Usage Contribution License  Installation   Install Tampermonkey: If you haven't already, you need to install the Tampermonkey browser extension. You can find it for various browsers:  Tampermonkey for Chrome Tampermonkey for Firefox Tampermonkey for Edge Tampermonkey for Opera/OperaGX    Open Tampermonkey Dashboard: Click on the Tampermonkey extension icon in your browser and select "Dashboard."   Create a New Script: In the Tampermonkey Dashboard, click on the "Utilities" tab and select "Create a new script."   Install the Script: Click Here and Press Install.   Enable the Script: Enable the script by clicking the switch next to the script name in the Tampermonkey Dashboard.   How to bypass the video player ban from using an ad blocker   Install the script: Install the script using the instructions above   Disable your adblocker: whitelist YouTube on any Chrome extensions that block ads or any third-party things that disable ads. Don't worry our script removes ads. (This only removes video player ads)   Usage Once you've installed and enabled the userscript, it should start working automatically Enjoy. Contribution If you have any suggestions, bug reports, or want to contribute to this userscript, feel free to create issues or pull requests in this GitHub repository. Contributors  AngelPuzzle oSumAtrIX Albedo-13 SleepingPig35 marjansimic Pecunia201  License This project is licensed under the MIT License.   </readme><commitcount>53</commitcount><languages>JavaScript 100.0%</languages><tags>youtube	adblock	tampermonkey	tampermonkey-userscript	remove-not-allowed</tags><about>Removes The "Ad blocker are not allowed on Youtube"</about><starcount>1.7k</starcount><watchcount>0</watchcount></repository><repository><username>refinedev</username><reponame>refine</reponame><readme>               What is refine? What do you mean by "headless" ? Headless in Routing 🔥 Try refine Use cases Key Features Quick Start Next Steps Stargazers Contribution Our ♥️ Contributors License      README.md                Home Page |     Discord |     Examples |     Blog |     Documentation    Build your React-based CRUD applications, without constraints.An open-source, headless web application framework developed with flexibility in mind.                 What is refine? refine is a meta React framework that enables the rapid✨ development of a wide range of web applications. From internal tools to admin panels, B2B apps, and dashboards, it serves as a comprehensive solution for building any type of CRUD application. refine's internal hooks and components simplify the development process and eliminate repetitive tasks by providing industry-standard solutions for crucial aspects of a project, including authentication, access control, routing, networking, state management, and i18n. refine is headless by design, thereby offering unlimited styling and customization options. What do you mean by "headless" ? Instead of being limited to a set of pre-styled components, refine provides collections of helper hooks, components, providers, and more. Since business logic and the UI are completely decoupled, you can customize the UI without constraints. It means that refine just works seamlessly with any custom designs or UI frameworks. Thanks to it's headless architecture, you can use popular CSS frameworks like TailwindCSS or even create your own styles from scratch. refine also provides integrations with Ant Design, Material UI, Mantine, and Chakra UI to get you started quickly. These libraries are a set of components that are nicely integrated with the headless @refinedev/core package. Headless in Routing For the routing, refine's headless approach shines too. It doesn't tie you to a single routing method or library. Instead, it offers a simple routing interface with built-in integrations for popular libraries. This means you can use refine seamlessly on different platforms like React Native, Electron, Next.js, Remix, etc. without any extra setup steps. 🔥 Try refine refine's browser-based app scaffolder lets you create refine app by making step-by-step selections directly in your browser You can choose the libraries and frameworks you want to work with, and the tool will generate a downloadable boilerplate code for you. This allows you to preview, modify, and download the project immediately, thereby streamlining the development process.       Use cases refine shines on data-intensive⚡ applications like admin panels, dashboards and internal tools. Thanks to the built-in SSR support, refine can also power customer-facing applications like storefronts. You can take a look at some live examples that can be built using refine from scratch:  Fully-functional Admin Panel Win95 Style Admin panel 🪟 Medium Clone - Real World Example Multitenancy Example Storefront  👉 Refer to most popular real use case examples 👉 More refine powered different usage scenarios can be found here Key Features ⚙️ Zero-config, one-minute setup with a single CLI command 🔌 Connectors for 15+ backend services including REST API, GraphQL, NestJs CRUD, Airtable, Strapi, Strapi v4, Strapi GraphQL, Supabase, Hasura, Appwrite, Nestjs-Query, Firebase, and Directus. 🌐 SSR support with Next.js or Remix 🔍 Auto-generated CRUD UIs from your API data structure ⚛ Perfect state management &amp; mutations with React Query 🔀 Advanced routing with any router library of your choice 🔐 Providers for seamless authentication and access control flows ⚡ Out-of-the-box support for live / real-time applications 📄 Easy audit logs &amp; document versioning 💬 Support for any i18n framework 💪 Future-proof, robust architecture ⌛️ Built-in CLI with time-saving features 💻 refine Devtools - dive deeper into your app and provide useful insights ✅ Full test coverage Quick Start The fastest way to get started with refine is by using the create refine-app CLI tool or the browser-based app scaffolder. For this example, we'll use the CLI tool. Simply run the following command to create a new refine project configured with Ant Design as the default UI framework: npm create refine-app@latest -- -o refine-antd  Once the setup is complete, navigate to the project folder and start your project with: npm run dev   Your refine application will be accessible at http://localhost:5173:   Let's consume a public fake REST API and add two resources (blog_posts and categories) to our project. Replace the contents of src/App.tsx with the following code: import { Refine } from "@refinedev/core"; import {     notificationProvider,     ErrorComponent,     ThemedLayout, } from "@refinedev/antd"; import routerProvider, { NavigateToResource } from "@refinedev/react-router-v6"; import dataProvider from "@refinedev/simple-rest";  import { BrowserRouter, Routes, Route, Outlet } from "react-router-dom";  import { AntdInferencer } from "@refinedev/inferencer/antd";  import "@refinedev/antd/dist/reset.css";  const App: React.FC = () =&gt; {     return (         &lt;BrowserRouter&gt;             &lt;Refine                 routerProvider={routerProvider}                 dataProvider={dataProvider("https://api.fake-rest.refine.dev")}                 notificationProvider={notificationProvider}                 resources={[                     {                         name: "blog_posts",                         list: "/blog-posts",                         show: "/blog-posts/show/:id",                         create: "/blog-posts/create",                         edit: "/blog-posts/edit/:id",                         meta: { canDelete: true },                     },                     {                         name: "categories",                         list: "/categories",                         show: "/categories/show/:id",                     },                 ]}             &gt;                 &lt;Routes&gt;                     &lt;Route                         element={                             &lt;ThemedLayout&gt;                                 &lt;Outlet /&gt;                             &lt;/ThemedLayout&gt;                         }                     &gt;                         &lt;Route index element={&lt;NavigateToResource /&gt;} /&gt;                         &lt;Route path="blog-posts"&gt;                             &lt;Route index element={&lt;AntdInferencer /&gt;} /&gt;                             &lt;Route                                 path="show/:id"                                 element={&lt;AntdInferencer /&gt;}                             /&gt;                             &lt;Route path="create" element={&lt;AntdInferencer /&gt;} /&gt;                             &lt;Route                                 path="edit/:id"                                 element={&lt;AntdInferencer /&gt;}                             /&gt;                         &lt;/Route&gt;                         &lt;Route path="categories"&gt;                             &lt;Route index element={&lt;AntdInferencer /&gt;} /&gt;                             &lt;Route                                 path="show/:id"                                 element={&lt;AntdInferencer /&gt;}                             /&gt;                         &lt;/Route&gt;                         &lt;Route path="*" element={&lt;ErrorComponent /&gt;} /&gt;                     &lt;/Route&gt;                 &lt;/Routes&gt;             &lt;/Refine&gt;         &lt;/BrowserRouter&gt;     ); };  export default App;  🚀 Thanks to the refine Inferencer package, it guesses the configuration to use for the list, show, create, and edit pages based on the data fetched from the API and generates the pages automatically. Now, you should see the output as a table populated with blog_posts &amp; category data:   You can get the auto-generated page codes by clicking the Show Code button on each page. Afterward, simply pass the pages to the resources array by replacing them with the Inferencer components. Next Steps 👉 Jump to Tutorial to continue your work and turn the example into a full-blown CRUD application. 👉 Visit the Learn the Basics page to get informed about the fundamental concepts. 👉 Read more on Advanced Tutorials  for different usage scenarios. 👉 See the real-life Finefoods Demo project. 👉 Play with interactive examples. Stargazers  Contribution 👉 Refer to the contribution docs for more information. If you have any doubts related to the project or want to discuss something, then join our Discord server. Our ♥️ Contributors    License Licensed under the MIT License, Copyright © 2021-present Refinedev   </readme><commitcount>5,686</commitcount><languages>TypeScript 97.3%	JavaScript 2.5%	CSS 0.1%	HTML 0.1%	Handlebars 0.0%	Smarty 0.0%</languages><tags>react	javascript	graphql	internal-tools	crud	admin	typescript	nextjs	headless	developer-tools	admin-ui	react-framework	frontend-framework	hacktoberfest	ant-design	low-code	internal-applications	nestjs	react-hooks	custom-internal</tags><about>Build your React-based CRUD applications, without constraints. 🌟 Star to support our work!</about><starcount>15.4k</starcount><watchcount>0</watchcount></repository><repository><username>PWhiddy</username><reponame>PokemonRedExperiments</reponame><readme>            Train RL agents to play Pokemon Red! Watch the Video on Youtube! Running the Pretrained Model Interactively 🎮 Training the Model 🏋️ Tracking Training Progress 📈 Extra 🐜      README.md     Train RL agents to play Pokemon Red!    Watch the Video on Youtube!    Running the Pretrained Model Interactively 🎮 🐍 Python 3.10 is recommended. Other versions may work but have not been tested. You also need to install ffmpeg and have it available in the command line.  Copy your legally obtained Pokemon Red ROM into the base directory. You can find this using google, it should be 1MB. Rename it to PokemonRed.gb if it is not already. The sha1 sum should be ea9bcae617fdf159b045185467ae58b2e4a48b9a, which you can verify by running shasum PokemonRed.gb. Move into the baselines/ directory: cd baselines Install dependencies: pip install -r requirements.txt It may be necessary in some cases to separately install the SDL libraries. Run: python run_pretrained_interactive.py  By default the game will terminate after 32K steps, or ~1 hour. You can increase this by adjusting the ep_length variable, but it will also use more memory. Interact with the emulator using the arrow keys and the a and s keys (A and B buttons). You can pause the AI's input during the game by editing agent_enabled.txt Note: the Pokemon.gb file MUST be in the main directory and your current directory MUST be the baselines/ directory in order for this to work. Training the Model 🏋️ Note: By default this can use up to ~100G of RAM. You can decrease this by reducing the num_cpu or ep_length, but it may affect the results. Also, the model behavior may become degenerate for up to the first 50 training iterations or so before starting to improve. This could likely be fixed with better hyperparameters but I haven't had the time or resources to sweep these.  Previous steps 1-3 Run: python run_baseline_parallel.py  Tracking Training Progress 📈 You can view the current state of each emulator, plot basic stats, and compare to previous runs using the VisualizeProgress.ipynb notebook. Extra 🐜 Map visualization code can be found in visualization/ directory.   </readme><commitcount>164</commitcount><languages>Jupyter Notebook 98.4%	Python 1.6%</languages><tags /><about>Playing Pokemon Red with Reinforcement Learning</about><starcount>4.4k</starcount><watchcount>0</watchcount></repository><repository><username>Alex313031</username><reponame>Thorium-Win</reponame><readme>            Thorium for Windows  Chromium fork for Windows named after radioactive element No. 90; Windows builds of https://github.com/Alex313031/Thorium Building       README.md         Thorium for Windows  Repo to serve Windows builds of Thorium!  IMPORTANT: PLEASE USE THE NEW WINDOWS 7/8 REPO HERE FOR WINDOWS 7/8/8.1 RELEASES &gt; https://github.com/Alex313031/thorium-win7  NEW: AVX2 Builds are now being served at https://github.com/Alex313031/Thorium-AVX2   This repo only hosts Windows builds. The other Thorium repos : Main repo that hosts the source code for all platforms and serves linux builds &gt; https://github.com/Alex313031/Thorium Windows AVX2 repo that serves AVX2 Windows builds &gt; https://github.com/Alex313031/Thorium-Win-AVX2 MacOS builds for x64 and M1 ARM64 are here &gt; https://github.com/Alex313031/Thorium-MacOS \ Android builds for ARM32 and ARM64 are here &gt; https://github.com/Alex313031/Thorium-Android \ Raspberry Pi (ARM64) builds are here &gt; https://github.com/Alex313031/Thorium-Raspi \ Builds for Windows 7/8/8.1 are here &gt; https://github.com/Alex313031/thorium-win7 \ Windows on ARM (arm64) builds are here &gt; https://github.com/Alex313031/Thorium-WOA \ Special repo which serves builds for SSE3, SSE4, and 32 bit builds are here &gt; https://github.com/Alex313031/Thorium-Special \ Another related project I'm trying to share is ThoriumOS : ChromiumOS builds with Thorium, Widevine, Google Sync, Google Drive, Kernel 5.15, firmware, and extra packages &gt; https://github.com/Alex313031/ThoriumOS Chromium fork for Windows named after radioactive element No. 90; Windows builds of https://github.com/Alex313031/Thorium    Always built with the latest stable version of Chromium. Intended to behave like and have the featureset of Google Chrome, with differences/patches/enhancements listed Here. I release Thorium as an .exe installer and a portable .zip. Portable intructions are in the Readme in the zip file.  Building   For building yourself, follow instructions at BUILDING_WIN.md (if building natively on Windows), OR BUILDING_WIN_CROSS.md (if cross building for Windows on Linux). Thanks for using Thorium!    </readme><commitcount>123</commitcount><languages>Batchfile 100.0%</languages><tags>windows	chrome	chromium	avx	thorium	chromedriver	web-browser	content-shell	webbrowser	web-platform	chromium-browser	thorium-browser</tags><about>Chromium fork for Windows named after radioactive element No. 90; Windows builds of https://github.com/Alex313031/Thorium</about><starcount>511</starcount><watchcount>0</watchcount></repository><repository><username>imteekay</username><reponame>programming-language-research</reponame><readme>               Programming Language Research Courses Books Learning Paths Experiments Papers Programming Language Design / PLT Compiler PL Paradigms Interpreter PL Research Type System Parsers Backend Program Synthesis Language Learnings TypeScript Ecmascript Rust OCaml Browser Web UI Careers Jobs Schools People in PL License      README.md      Programming Language Research Research on programming languages, compilers, functional programming, devtools. Courses  🎥 301 - Organization of Programming Languages 🎥 Benjamin Pierce - Software foundations 🎥 CIS352 - Principles of Programming Languages 🎥 CMSC 430: Design and Implementation of Programming Languages 🎥 CS 3520/6520 - Programming Languages 🎥 CS3110 - Functional Data Structures 🎥 CS3MI3 - Principles of Programming Languages 🎥 CS520 - Theories Of Programming Languages 🎥 CSE340 - Principles of Programming Languages 🎥 LINFO1131 - Advanced programming language concepts 💻 Accelerated 1 💻 All the courses Matthias Felleisen teach 💻 Building a Parser from scratch 💻 CMSC330 - Organization of Programming Languages 💻 CMU - CS6120 - Advanced Compilers 💻 Compilers — Stanford 💻 Compilers Design Recipe 💻 Compilers: Theory and Practice — Udacity 💻 Concepts in Programming Languages 💻 COP 4020 - Programming Languages I 💻 COS 326 - Functional Programming 💻 CS 242: Programming Languages, Fall 2019 💻 CS019 - Accelerated Introduction to Computer Science 💻 CS164 Programming Languages and Compilers 💻 CS242 - Programming Languages - Stanford 💻 CS2500 - Fundamentals of Computer Science 💻 CS516 - Compiler Design and Implementation 💻 CSCI1730 - Programming Languages - Application and Interpretation 💻 CSE 131 - Compiler Construction 💻 CSE230 - Principles/Programming Languages 💻 Essentials of Programming Languages 💻 Fundamentals 1 💻 Fundamentals 2 💻 Hack Your Own Language 💻 History of Programming Languages 💻 Indiana University Compiler Course 💻 Introduction to Compiler Construction 💻 Jay McCarthy - 406 Compiler Construction 💻 Matthias Felleisen - Intensive Principles of Programming Languages 💻 Matthias Felleisen - Principles of Programming Languages 💻 Principles of Compiler Design 💻 Programming Language Foundations in Agda 💻 Programming Languages and Logic 💻 Programming Languages Course given by DCC-UFMG 💻 Programming Languages Ⅰ 💻 Programming Languages Ⅱ 💻 Ray Total Courses 💻 Theory of Programming Languages  Books  📚 Compiler Construction 📚 Compiler Design 📚 Compiling with Continuations 📚 Concepts of Programming Languages 📚 Discrete Mathematics for Computing 📚 Engineering a Compiler 📚 Homotopy Type Theory 📚 How to Design Programs: An Introduction to Programming and Computing 📚 HTDP 📚 Human-Centered Programming Languages 📚 Introduction to Compilers and Language Design 📚 Modern Compiler Design 📚 Modern Compiler Implementation in C 📚 Parsing Techniques: A Practical Guide 📚 Parsing Techniques. A Practical Guide 📚 Practical Foundations for Programming Languages 📚 Programming Languages: Application and Interpretation 📚 SICP — Structure and Interpretation of Computer 📚 The Elements Of Computing Systems 📚 Thinking with Types: Type-Level Programming in Haskell 📚 Type Theory and Formal Proof: An Introduction 📚 Type Theory and Functional Programming  Learning Paths  Advanced Programming Languages Compiler Engineer Path Learn Type Theory PLR &amp; Compiler Learning Path Programming Language Theory Path So You Want to Be a (Compiler) Wizard  Experiments  A miniature model of the Typescript compiler An interpreter for the Lox programming language An interpreter for the Monkey programming language How the TypeScript compiler works  Papers  10 papers that all PhD students in programming languages ought to know 📜 A syntactic approach to type soundness 📜 A Theory of Type Polymorphism in Programming 📜 Abstracting Gradual Typing 📜 An Axiomatic Basis for Computer Programming 📜 Call-by-name, call-by-value, and the λ-calculus 📜 Classic Papers in Programming Languages and Logic 📜 Efficient Gradual Typing 📜 Fundamental concepts in programming languages 📜 Gradual Type Theory 📜 Gradual Typing for Functional Languages 📜 Gradual Typing: A New Perspective 📜 How Statically-Typed Functional Programmers Write Code 📜 Migrating Gradual Types 📜 On Model Subtyping 📜 On Understanding Types, Data Abstraction, and Polymorphism 📜 Papers on programming languages: ideas from 70's for today 📜 Papers to read 📜 Pluggable Type Systems 📜 Programming in Homotopy Type Theory 📜 Refined Criteria for Gradual Typing 📜 Static Typing Where Possible, Dynamic Typing When Needed: The End of the Cold War Between Programming Languages 📜 The Behavior of Gradual Types: A User Study 📜 The Design Principles of the Elixir Type System 📜 The Next 700 Programming Languages 📜 Towards a theory of type structure 📜 Towards Practical Gradual Typing 📜 Type Inference Algorithms 📜 Type Inference for Records in a Natural Extension of ML 📜 Type Systems for Object-Oriented Programming Languages 📜 Type Systems 📜 What every compiler writer should know about programmers 📜 Why Dependent Types Matter  Programming Language Design / PLT  🎥 A Language Creators' Conversation: Guido van Rossum, James Gosling, Larry Wall &amp; Anders Hejlsberg 🎥 A Tale of Two Asyncs: Open Source Language Design in Rust and Node.js 🎥 Another Go at Language Design 🎥 Concepts of Programming Languages 🎥 CS520 Theories of Programming Languages — KAIST 🎥 Delimited Continuations, Demystified by Alexis King 🎥 Going beyond JavaScript 🎥 Growing a Language, by Guy Steele 🎥 How Rust does OSS 🎥 JavaScript Static Analysis for Evolving Language Specifications 🎥 Linguistics and Compilers 🎥 On understanding types, data abstraction and effects 🎥 Principles of Programming Languages by Robert M. Siegfried 🎥 Principles of Programming Languages 🎥 So many programming languages so little time 🎥 Stephanie Weirich speaks about Programming Language Design 🎥 The Economics of Programming Languages 🎥 The Mess We're In 🎥 The Next Programming Language - Douglas Crockford 🎥 What's Next for Our Programming Languages 🎥 Why Programming Languages Matter 📜 Programming Language Semantics 📝 A Frontend Programmer's Guide to Languages 📝 Building a Debugger: Code Analysis 📝 Concepts of Programming Languages 📝 Grammars for programming languages 📝 How To Create Your Own Programming Language 📝 Language-oriented software engineering 📝 Let’s Build a Programming Language 📝 Minimalism in Programming Language Design 📝 Panel: The Future of Programming Languages 📝 Principles of Programming Languages 📝 Programming and Programming Languages 📝 Programming Language and Compilers Reading List 📝 Programming Language Foundations in Agda 📝 Programming language theory and practice in ReScript 📝 Shriram Krishnamurthi and Joe Gibbs Politz - Programming Languages: Application and Interpretation 📝 The Programming Language Enthusiast 📝 The Study of Programming Languages 📝 Why Study Programming Languages  Compiler  🎥 Anders Hejlsberg on Modern Compiler Construction 🎥 Building a parser in C#, from first principles 🎥 Compiler Design course 🎥 How To Build A Programming Language From Scratch 🎥 How would compilers change in the next 10 years? 🎥 Implementing a bignum calculator 🎥 JavaScript ∩ WebAssembly 🎥 JavaScript implementation in SpiderMonkey 🎥 KAIST CS420: Compiler Design 🎥 Lexical Scanning in Go 🎥 Mozilla Hacks Compiler Compiler 🎥 Quick look at the TypeScript Compiler API 🎥 Typescript Compiler explained by the Author Anders Hejlsberg 🎥 Understanding compiler optimization 📝 A miniature model of the Typescript compiler 📝 Alias analysis 📝 An Incremental Approach to Compiler Construction 📝 An Introduction to Interpreters and JIT Compilation 📝 An Introduction to Interpreters and JIT Compilation 📝 AST (Abstract Syntax Tree) 📝 Bob Nystrom - Expression Parsing Made Easy 📝 Building a Parser from scratch 📝 Compilers and Interpreters 📝 Compilers Resources 📝 Compilers: Nuts and bolts of Programming Languages 📝 Compiling Scheme to C with closure conversion 📝 Compiling to Java 📝 Compiling to lambda-calculus 📝 Douglas Crockford - Top Down Operator Precedence 📝 Dr. Dobbs - Bob: A Tiny Object-Oriented Language 📝 Flattening ASTs (and Other Compiler Data Structures) 📝 Fredrik Lundh - Simple Town-Down Parsing In Python 📝 Garbage Collection Algorithms 📝 How to implement a programming language in JavaScript 📝 IU Compiler Course 📝 Jack W. Crenshaw - Let’s Build a Compiler! 📝 Language grammar 📝 Lessons from Writing a Compiler 📝 Let's build a compiler 📝 Let's make a Teeny Tiny compiler, part 1 📝 Let's make a Teeny Tiny compiler, part 2 📝 Let's make a Teeny Tiny compiler, part 3 📝 Lexical Analysis 📝 Lexing in JS style 📝 Make a Lisp 📝 Nick Desaulniers - Interpreter, Compiler, JIT 📝 Parsing Algorithms 📝 Pursuit of Performance on Building a JavaScript Compiler 📝 Resources for Amateur Compiler Writers 📝 Scheme from Scratch - Introduction 📝 TypeScript Compiler Notes 📝 Understanding GC in JSC From Scratch 📝 Which Interpreters are Faster, AST or Bytecode? 📝 Write you a Haskell  PL Paradigms  🎥 Compiling with Continuations or without? Whatever 📝 Continuation-passing style in JavaScript 📝 Continuation-passing style  Interpreter  🎥 Cheaply writing a fast interpreter 💻 Building an Interpreter from scratch 💻 Crafting an Interpreter 📝 (How to Write a (Lisp) Interpreter (in Python)) 📝 A Python Interpreter Written in Python 📝 Building an Interpreter 📝 Crafting Interpreters 📝 How languages work #1: String interpolation 📝 Implementing a Simple Compiler on 25 Lines of JavaScript 📝 Let’s Build A Simple Interpreter 📝 Little lisp interpreter 📝 Little Lisp interpreter 📝 Pratt Parsers: Expression Parsing Made Easy 📝 What do people mean when they say “transpiler”?  PL Research  📝 Increasing the Impact of PL Research 📝 What is PL research and how is it useful?  Type System  🎥 "Hindley-Milner Type Inference — Part 1 🎥 "Hindley-Milner Type Inference — Part 2 🎥 "Propositions as Types" by Philip Wadler 🎥 A Taste of Type Theory 🎥 A Type System From Scratch 🎥 Bringing Types to Elixir by Giuseppe Castagna and Guillaume Duboc 🎥 Discrete Math — Dr. Trefor Bazett 🎥 Gradual Type Theory 🎥 Gradual Typing: A New Perspective 🎥 How to Evaluate the Performance of Gradual Type Systems 🎥 Let's build a typesystem in Haskell! 🎥 The Hindley-Milner Type System 🎥 Thinking with Types 🎥 Type Systems - The Good, Bad and Ugly 🎥 Type-safe embedded domain-specific languages 1/4 🎥 Type-safe embedded domain-specific languages 2/4 🎥 Type-safe embedded domain-specific languages 3/4 🎥 Type-safe embedded domain-specific languages 4/4 🎥 Types and Programming Languages Book club 🎥 Types are like the Weather, Type Systems are like Weathermen 🎥 Typing the Untyped: Soundness in Gradual Type Systems 🎥 What is Decidable about Gradual Types? 💻 Building a Typechecker from scratch 📜 How to evaluate the performance of gradual type systems 📜 Optimizing and Evaluating Transient Gradual Typing 📜 Putting gradual types to work 📜 Safe &amp; Efficient Gradual Typing for TypeScript 📜 What Is Decidable about Gradual Types? 📝 A brief introduction to type theory and the univalence axiom 📝 A reckless introduction to Hindley-Milner type inference 📝 An accessible introduction to type theory and implementing a type-checker 📝 Bootstrapping a Type System 📝 Gradual Typing from Theory to Practice 📝 Hindley-Milner Type Checking AST 📝 How should I read type system notation? 📝 Introduction to Type Theory 📝 MyPy TypeChecker 📝 Programming and Programming Languages: Type Inference 📝 Propositions as types: explained 📝 Python internals: Symbol tables, part 1 📝 Python internals: Symbol tables, part 2 📝 Strong arrows: a new approach to gradual typing 📝 Type Checking If Expressions 📝 Type inference under the hood 📝 Type Systems by Luca Cardelli, Microsoft Research 📝 Type Theory Workshop 📝 Understanding types as sets 📝 What is Gradual Typing  Parsers  📝 Building Extensible Parsers with Camlp4 📝 Demystifying Pratt Parsers 📝 Rewriting the Ruby parser  Backend  🎥 How LLVM &amp; Clang work 💻 Building a Virtual Machine  Program Synthesis  🎥 Generating Programs from Types  Language Learnings  JavaScript Python Ruby Haskell Elixir Clojure Rust  TypeScript  🎥 A horrifically deep dive into TypeScript module resolution 🎥 A Trip into the Compiler 🎥 Advanced Types in TypeScript 🎥 AreTheTypesWrong with Andrew Branch 🎥 Hidden Gems of TypeScript compiler 🎥 How safe is "safe enough" for TypeScript 🎥 How the TypeScript Compiler Compiles 🎥 The Road to Private Methods 🎥 Type Level Programming in TypeScript 🎥 Typescript Performance: Going Beyond The Surface 🎥 TypeScript Performance 🎥 Typescript Type System 📝 @typescript/analyze-trace 📝 A preview of Ezno's checker 📝 An Introduction To Type Programming In TypeScript 📝 Debugging the TypeScript Codebase 📝 Investigating TypeScript compiler APIs 📝 JavaScript scope, Closures, and the TypeScript compiler 📝 Making sense of TypeScript using set theory 📝 Modules in TypeScript 📝 Optimizing TypeScript Memory Usage 📝 Reconstructing TypeScript, part 0: intro and background 📝 Reconstructing TypeScript, part 1: bidirectional type checking 📝 Reconstructing TypeScript, part 2: functions and function calls 📝 Reconstructing TypeScript, part 3: operators and singleton types 📝 Reconstructing TypeScript, part 4: union types 📝 Reconstructing TypeScript, part 5: intersection types 📝 Reconstructing TypeScript, part 6: narrowing 📝 Static TypeScript 📝 Type level programming in TypeScript 📝 TypeScript / How the compiler compiles 📝 TypeScript and the dawn of gradual types 📝 TypeScript AST Resources 📝 TypeScript Bytecode Interpreter / Runtime Types 📝 TypeScript Compiler API: Improve API Integrations Using Code Generation 📝 TypeScript Compiler Internals 📝 TypeScript Compiler Manual 📝 TypeScript Modules - Theory 📝 TypeScript Performance Tracing 📝 TypeScript Performance 📝 TypeScript's type-independent output  Ecmascript  🎥 Create Your Own JavaScript Runtime 🎥 Ectype - bringing type safety (and more!) to vanilla JavaScript 🎥 Static Hermes: the Next Generation of Hermes 🎥 TC39 – From the proposal to ECMAScript - Step by Step 📝 SingleEscapeCharacter 📝 Automatic semicolon insertions in JavaScript 📝 Deep JavaScript 📝 Empty Statement 📝 From Research Prototypes to Continuous Integration: Guiding the Design and Implementation of JavaScript 📝 JavaScript Closures 📝 JavaScript Garbage Collector 📝 JavaScript Memory management 📝 JavaScript Static Analysis for Evolving Language Specifications 📝 Relation of Garbage Collector and Closure in JavaScript 📝 V8: full compiler 📝 V8: Garbage Collection  Rust  🎥 Things I Learned (TIL) - Nicholas Matsakis 📝 Rust fact vs. fiction: 5 Insights from Google's Rust journey in 2022 📝 Rust's language ergonomics initiative 📝 Why Static Languages Suffer From Complexity  OCaml  📝 How Jane Street is making OCaml the new Rust  Browser  🎥 Browser From Scratch Live Streams 🎥 Chrome University 2018: Life of a Script 🎥 Critical rendering path - Crash course on web performance 🎥 In The Loop - setTimeout, micro tasks, requestAnimationFrame, requestIdleCallback 🎥 Intro to Rust (Building a Browser Engine: Commands and Rendering in OpenGL) 🎥 Intro to Rust-lang (Adding a CSS engine and CSS parsing to our Browser) 🎥 Intro to Rust-lang (Adding a Style Tree to our Browser) 🎥 Intro to Rust-lang (Building the Dom and an HTML Parser) 🎥 Servo: Designing and Implementing a Parallel Browser 🎥 The internet computer 📚 Web Browser Engineering 📝 Browser from scratch 📝 Browser from Scratch 📝 Browser Performance 📝 Building a Rust Web Browser 📝 Concurrent JavaScript: It can work! 📝 David Baron's blog: Blink engineer 📝 How Browsers Work: Behind the scenes of modern web browsers 📝 How browsers work 📝 Key data structures and their roles in RenderingNG 📝 Let's build a browser engine! 📝 Notes on how browsers work 📝 Notes on JavaScript Interpreters 📝 Notes on Javascript Memory Profiling in Google Chrome 📝 Overview of the RenderingNG architecture 📝 reflows &amp; repaints: css performance making your javascript slow? 📝 Rendering: repaint, reflow/relayout, restyle 📝 RenderingNG deep-dive: LayoutNG 📝 RenderingNG: an architecture that makes and keeps Chrome fast for the long term 📝 Round-up of web browser internals resources 📝 Understanding Reflow and Repaint in the browser 📝 Web Browser Engineering 📝 What forces layout / reflow  Web UI  🎥 Algebraic effects, Fibers, Coroutines 📝 Algebraic Effects for React Developers 📝 Algebraic Effects for the Rest of Us 📝 PL web frameworks 📝 React - Basic Theoretical Concepts 📝 React Concurrent mode 📝 TypeScript + fp-ts: ReaderTaskEither and React  Careers Jobs  Chromium Engineer at Browser Company Senior Software Engineer at Mozilla Corporation JavaScript Virtual Machine Compiler Engineer at Apple Compiler Jobs Swift Type System Engineer Compiler Engineer Groq Compiler Engineer Modular AI Compiler Engineer  Schools  Programming Language and Compiler Research Groups Indiana University Bloomington KAIST Programming Language Research Group Tokyo Institute of Technology  TIT: How to Join the Group Scholarships: Mext Examples of Dissertations    People in PL  Anders Hejlsberg Dmitry Soshnikov Gilad Bracha Researchers in Programming Languages and Compilers Roberto Giacobazzi Bob Nystrom StackExchange: Programming Language Design and Implementation Matthias Felleisen Paulette Koronkevich Programming Linnguistics Stefan Marr Jihyeok Park Gavin Bierman Mads Torgersen  License MIT © TK    </readme><commitcount>569</commitcount><languages>Clojure 20.4%	JavaScript 19.8%	Python 14.0%	TypeScript 10.1%	Standard ML 7.7%	Ruby 6.6%</languages><tags>compiler	compiler-design	programming-language-theory	plt	programming-languages-design</tags><about>Programming Language Research, Applied PLT &amp; Compilers</about><starcount>515</starcount><watchcount>0</watchcount></repository><repository><username>devfullcycle</username><reponame>imersao15</reponame><readme>            Imersão Fullcycle 15 - Codepix Sobre o repositório Alterações no Dockerfile - Go: Alteração de comando - Aula 02 - Imersão - gRPC e o abismo entre devs e empresas      README.md     Imersão Fullcycle 15 - Codepix  Participe gratuitamente: https://imersao.fullcycle.com.br/ Sobre o repositório Esse repositório contém todo código utilizado durante as aulas para referência. Faça seu fork e também nos dê uma estrelinha para nos ajudar a divulgar o projeto.  Alterações no Dockerfile - Go: Vamos atualizar a versão do Go para a golang:1.19, desta forma os pacotes instalados serão compatíveis entre eles. Durante a aula 01 o Wesley configura o Dockerfile com a instalação do cobra que vamos utilizar mais a frente no curso, mas há uma alteração a ser feita na instalação desta ferramenta como abaixo: Precisamos substituir e incluir o comando de instalação do cobra-cli: go get github.com/spf13/cobra-cli@v1.3.0 &amp;&amp; \ Após realizar as alterações acima o Dockerfile ficará da seguinte forma: FROM golang:1.19  WORKDIR /go/src ENV PATH="/go/bin:${PATH}" ENV GO111MODULE=on ENV CGO_ENABLED=1  RUN apt-get update &amp;&amp; \     apt-get install build-essential protobuf-compiler librdkafka-dev -y &amp;&amp; \     go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.3.0 &amp;&amp; \     go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.31.0 &amp;&amp; \     go install github.com/spf13/cobra-cli@v1.3.0 &amp;&amp; \     wget https://github.com/ktr0731/evans/releases/download/0.9.1/evans_linux_amd64.tar.gz &amp;&amp; \     tar -xzvf evans_linux_amd64.tar.gz &amp;&amp; \     mv evans ../bin &amp;&amp; rm -f evans_linux_amd64.tar.gz  CMD ["tail", "-f", "/dev/null"]  Dentro do arquivo go.mod apague todo o conteúdo de require e no terminal rode o comando abaixo para instalar todas as depêndencias: go mod init Com isso a aplicação deve funcionar corretamente.  Alteração de comando - Aula 02 - Imersão - gRPC e o abismo entre devs e empresas Como explicado acima, precisamos incluir o cobra-cli nas instalações internas do container, por isso o comando para iniciar o cobra será: cobra-cli init   </readme><commitcount>31</commitcount><languages>Go 50.9%	TypeScript 47.0%	Dockerfile 1.2%	JavaScript 0.9%</languages><tags /><about>Programming Language Research, Applied PLT &amp; Compilers</about><starcount>94</starcount><watchcount>0</watchcount></repository><repository><username>langchain-ai</username><reponame>langchain</reponame><readme>            🦜️🔗 LangChain 🚨Breaking Changes for select chains (SQLDatabase) on 7/28/23 Quick Install 🤔 What is this? 📖 Documentation 🚀 What can this help with? 💁 Contributing      README.md     🦜️🔗 LangChain ⚡ Building applications with LLMs through composability ⚡             Looking for the JS/TS version? Check out LangChain.js. To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building, testing, and monitoring LLM applications. Fill out this form to get off the waitlist or speak with our sales team 🚨Breaking Changes for select chains (SQLDatabase) on 7/28/23 In an effort to make langchain leaner and safer, we are moving select chains to langchain_experimental. This migration has already started, but we are remaining backwards compatible until 7/28. On that date, we will remove functionality from langchain. Read more about the motivation and the progress here. Read how to migrate your code here. Quick Install pip install langchain or pip install langsmith &amp;&amp; conda install langchain -c conda-forge 🤔 What is this? Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge. This library aims to assist in the development of those types of applications. Common examples of these applications include: ❓ Question Answering over specific documents  Documentation End-to-end Example: Question Answering over Notion Database  💬 Chatbots  Documentation End-to-end Example: Chat-LangChain  🤖 Agents  Documentation End-to-end Example: GPT+WolframAlpha  📖 Documentation Please see here for full documentation on:  Getting started (installation, setting up the environment, simple examples) How-To examples (demos, integrations, helper functions) Reference (full API docs) Resources (high-level explanation of core concepts)  🚀 What can this help with? There are six main areas that LangChain is designed to help with. These are, in increasing order of complexity: 📃 LLMs and Prompts: This includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs. 🔗 Chains: Chains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications. 📚 Data Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources. 🤖 Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents. 🧠 Memory: Memory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory. 🧐 Evaluation: [BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this. For more information on these concepts, please see our full documentation. 💁 Contributing As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation. For detailed information on how to contribute, see here.   </readme><commitcount>5,347</commitcount><languages>Python 78.7%	Jupyter Notebook 20.6%	MDX 0.5%	Makefile 0.1%	XSLT 0.1%	HTML 0.0%</languages><tags /><about>⚡ Building applications with LLMs through composability ⚡</about><starcount>65.3k</starcount><watchcount>0</watchcount></repository><repository><username>Azure-Samples</username><reponame>azure-search-openai-demo</reponame><readme>               ChatGPT + Enterprise data with Azure OpenAI and Cognitive Search Table of Contents Features Azure account requirements Azure deployment Cost estimation Project setup GitHub Codespaces VS Code Dev Containers Local environment Deploying from scratch Deploying with existing Azure resources Existing resource group Existing OpenAI resource Azure OpenAI: Openai.com OpenAI: Existing Azure Cognitive Search resource Other existing Azure resources Provision remaining resources Deploying again Sharing environments Enabling optional features Enabling Application Insights Enabling authentication Enabling login and document level access control Enabling CORS for an alternate frontend Running locally Using the app Productionizing Resources Clean up Note FAQ Troubleshooting      README.md        name description languages products page_type urlFragment     ChatGPT + Enterprise data Chat with your data using OpenAI and Cognitive Search.    azdeveloper typescript python bicep        azure azure-cognitive-search azure-openai azure-app-service     sample azure-search-openai-demo    ChatGPT + Enterprise data with Azure OpenAI and Cognitive Search Table of Contents  Features Azure account requirements Azure deployment  Cost estimation Project setup  GitHub Codespaces VS Code Dev Containers Local environment   Deploying from scratch Deploying with existing Azure resources Deploying again   Sharing environments Enabling optional features  Enabling Application Insights Enabling authentication Enabling login and document level access control Enabling CORS for an alternate frontend   Using the app Running locally Productionizing Resources  Note FAQ Troubleshooting      This sample demonstrates a few approaches for creating ChatGPT-like experiences over your own data using the Retrieval Augmented Generation pattern. It uses Azure OpenAI Service to access the ChatGPT model (gpt-35-turbo), and Azure Cognitive Search for data indexing and retrieval. The repo includes sample data so it's ready to try end to end. In this sample application we use a fictitious company called Contoso Electronics, and the experience allows its employees to ask questions about the benefits, internal policies, as well as job descriptions and roles.  Features  Chat and Q&amp;A interfaces Explores various options to help users evaluate the trustworthiness of responses with citations, tracking of source content, etc. Shows possible approaches for data preparation, prompt construction, and orchestration of interaction between model (ChatGPT) and retriever (Cognitive Search) Settings directly in the UX to tweak the behavior and experiment with options Optional performance tracing and monitoring with Application Insights   Azure account requirements IMPORTANT: In order to deploy and run this example, you'll need:  Azure account. If you're new to Azure, get an Azure account for free and you'll get some free Azure credits to get started. Azure subscription with access enabled for the Azure OpenAI service. You can request access with this form. If your access request to Azure OpenAI service doesn't match the acceptance criteria, you can use OpenAI public API instead. Learn how to switch to an OpenAI instance. Azure account permissions:  Your Azure account must have Microsoft.Authorization/roleAssignments/write permissions, such as Role Based Access Control Administrator, User Access Administrator, or Owner. If you don't have subscription-level permissions, you must be granted RBAC for an existing resource group and deploy to that existing group. Your Azure account also needs Microsoft.Resources/deployments/write permissions on the subscription level.    Azure deployment Cost estimation Pricing varies per region and usage, so it isn't possible to predict exact costs for your usage. However, you can try the Azure pricing calculator for the resources below.  Azure App Service: Basic Tier with 1 CPU core, 1.75 GB RAM. Pricing per hour. Pricing Azure OpenAI: Standard tier, ChatGPT and Ada models. Pricing per 1K tokens used, and at least 1K tokens are used per question. Pricing Form Recognizer: SO (Standard) tier using pre-built layout. Pricing per document page, sample documents have 261 pages total. Pricing Azure Cognitive Search: Standard tier, 1 replica, free level of semantic search. Pricing per hour.Pricing Azure Blob Storage: Standard tier with ZRS (Zone-redundant storage). Pricing per storage and read operations. Pricing Azure Monitor: Pay-as-you-go tier. Costs based on data ingested. Pricing  To reduce costs, you can switch to free SKUs for Azure App Service and Form Recognizer by changing the parameters file under the infra folder. There are some limits to consider; for example, the free Form Recognizer resource only analyzes the first 2 pages of each document. You can also reduce costs associated with the Form Recognizer by reducing the number of documents in the data folder, or by removing the postprovision hook in azure.yaml that runs the prepdocs.py script. ⚠️ To avoid unnecessary costs, remember to take down your app if it's no longer in use, either by deleting the resource group in the Portal or running azd down. Project setup You have a few options for setting up this project. The easiest way to get started is GitHub Codespaces, since it will setup all the tools for you, but you can also set it up locally if desired. GitHub Codespaces You can run this repo virtually by using GitHub Codespaces, which will open a web-based VS Code in your browser:  VS Code Dev Containers A related option is VS Code Dev Containers, which will open the project in your local VS Code using the Dev Containers extension:  Start Docker Desktop (install it if not already installed) Open the project:  In the VS Code window that opens, once the project files show up (this may take several minutes), open a terminal window Run azd auth login Now you can follow the instructions in Deploying from scratch below  Local environment First install the required tools:  Azure Developer CLI Python 3.9, 3.10, or 3.11  Important: Python and the pip package manager must be in the path in Windows for the setup scripts to work. Important: Ensure you can run python --version from console. On Ubuntu, you might need to run sudo apt install python-is-python3 to link python to python3.   Node.js 14+ Git Powershell 7+ (pwsh) - For Windows users only.  Important: Ensure you can run pwsh.exe from a PowerShell terminal. If this fails, you likely need to upgrade PowerShell.    Then bring down the project code:  Create a new folder and switch to it in the terminal Run azd auth login Run azd init -t azure-search-openai-demo  note that this command will initialize a git repository and you do not need to clone this repository    Deploying from scratch Execute the following command, if you don't have any pre-existing Azure services and want to start from a fresh deployment.  Run azd up - This will provision Azure resources and deploy this sample to those resources, including building the search index based on the files found in the ./data folder.  Important: Beware that the resources created by this command will incur immediate costs, primarily from the Cognitive Search resource. These resources may accrue costs even if you interrupt the command before it is fully executed. You can run azd down or delete the resources manually to avoid unnecessary spending. You will be prompted to select two locations, one for the majority of resources and one for the OpenAI resource, which is currently a short list. That location list is based on the OpenAI model availability table and may become outdated as availability changes.   After the application has been successfully deployed you will see a URL printed to the console.  Click that URL to interact with the application in your browser. It will look like the following:    NOTE: It may take 5-10 minutes for the application to be fully deployed. If you see a "Python Developer" welcome screen or an error page, then wait a bit and refresh the page.  Deploying with existing Azure resources If you already have existing Azure resources, you can re-use those by setting azd environment values. Existing resource group  Run azd env set AZURE_RESOURCE_GROUP {Name of existing resource group} Run azd env set AZURE_LOCATION {Location of existing resource group}  Existing OpenAI resource Azure OpenAI:  Run azd env set AZURE_OPENAI_SERVICE {Name of existing OpenAI service} Run azd env set AZURE_OPENAI_RESOURCE_GROUP {Name of existing resource group that OpenAI service is provisioned to} Run azd env set AZURE_OPENAI_CHATGPT_DEPLOYMENT {Name of existing ChatGPT deployment}. Only needed if your ChatGPT deployment is not the default 'chat'. Run azd env set AZURE_OPENAI_EMB_DEPLOYMENT {Name of existing GPT embedding deployment}. Only needed if your embeddings deployment is not the default 'embedding'.  When you run azd up after and are prompted to select a value for openAiResourceGroupLocation, make sure to select the same location as the existing OpenAI resource group. Openai.com OpenAI:  Run azd env set OPENAI_HOST openai Run azd env set OPENAI_ORGANIZATION {Your OpenAI organization} Run azd env set OPENAI_API_KEY {Your OpenAI API key} Run azd up  You can retrieve your OpenAI key by checking your user page and your organization by navigating to your organization page. Learn more about creating an OpenAI free trial at this link. Do not check your key into source control. When you run azd up after and are prompted to select a value for openAiResourceGroupLocation, you can select any location as it will not be used. Existing Azure Cognitive Search resource  Run azd env set AZURE_SEARCH_SERVICE {Name of existing Azure Cognitive Search service} Run azd env set AZURE_SEARCH_SERVICE_RESOURCE_GROUP {Name of existing resource group with ACS service} If that resource group is in a different location than the one you'll pick for the azd up step, then run azd env set AZURE_SEARCH_SERVICE_LOCATION {Location of existing service} If the search service's SKU is not standard, then run azd env set AZURE_SEARCH_SERVICE_SKU {Name of SKU}. The free tier won't work as it doesn't support managed identity. (See other possible values) If you have an existing index that is set up with all the expected fields, then run azd env set AZURE_SEARCH_INDEX {Name of existing index}. Otherwise, the azd up command will create a new index.  You can also customize the search service (new or existing) for non-English searches:  To configure the language of the search query to a value other than "en-us", run azd env set AZURE_SEARCH_QUERY_LANGUAGE {Name of query language}. (See other possible values) To turn off the spell checker, run azd env set AZURE_SEARCH_QUERY_SPELLER none. (See other possible values) To configure the name of the analyzer to use for a searchable text field to a value other than "en.microsoft", run azd env set AZURE_SEARCH_ANALYZER_NAME {Name of analyzer name}. (See other possible values)  Other existing Azure resources You can also use existing Form Recognizer and Storage Accounts. See ./infra/main.parameters.json for list of environment variables to pass to azd env set to configure those existing resources. Provision remaining resources Now you can run azd up, following the steps in Deploying from scratch above. That will both provision resources and deploy the code. Deploying again If you've only changed the backend/frontend code in the app folder, then you don't need to re-provision the Azure resources. You can just run: azd deploy If you've changed the infrastructure files (infra folder or azure.yaml), then you'll need to re-provision the Azure resources. You can do that by running: azd up Sharing environments To give someone else access to a completely deployed and existing environment, either you or they can follow these steps:  Install the Azure CLI Run azd init -t azure-search-openai-demo or clone this repository. Run azd env refresh -e {environment name} They will need the azd environment name, subscription ID, and location to run this command. You can find those values in your .azure/{env name}/.env file.  This will populate their azd environment's .env file with all the settings needed to run the app locally. Set the environment variable AZURE_PRINCIPAL_ID either in that .env file or in the active shell to their Azure ID, which they can get with az ad signed-in-user show. Run ./scripts/roles.ps1 or .scripts/roles.sh to assign all of the necessary roles to the user.  If they do not have the necessary permission to create roles in the subscription, then you may need to run this script for them. Once the script runs, they should be able to run the app locally.  Enabling optional features Enabling Application Insights To enable Application Insights and the tracing of each request, along with the logging of errors, set the AZURE_USE_APPLICATION_INSIGHTS variable to true before running azd up  Run azd env set AZURE_USE_APPLICATION_INSIGHTS true Run azd up  To see the performance data, go to the Application Insights resource in your resource group, click on the "Investigate -&gt; Performance" blade and navigate to any HTTP request to see the timing data. To inspect the performance of chat requests, use the "Drill into Samples" button to see end-to-end traces of all the API calls made for any chat request:  To see any exceptions and server errors, navigate to the "Investigate -&gt; Failures" blade and use the filtering tools to locate a specific exception. You can see Python stack traces on the right-hand side. Enabling authentication By default, the deployed Azure web app will have no authentication or access restrictions enabled, meaning anyone with routable network access to the web app can chat with your indexed data.  You can require authentication to your Azure Active Directory by following the Add app authentication tutorial and set it up against the deployed web app. To then limit access to a specific set of users or groups, you can follow the steps from Restrict your Azure AD app to a set of users by changing "Assignment Required?" option under the Enterprise Application, and then assigning users/groups access.  Users not granted explicit access will receive the error message -AADSTS50105: Your administrator has configured the application &lt;app_name&gt; to block users unless they are specifically granted ('assigned') access to the application.- Enabling login and document level access control By default, the deployed Azure web app allows users to chat with all your indexed data. You can enable an optional login system using Azure Active Directory to restrict access to indexed data based on the logged in user. Enable the optional login and document level access control system by following this guide. Enabling CORS for an alternate frontend By default, the deployed Azure web app will only allow requests from the same origin.  To enable CORS for a frontend hosted on a different origin, run:  Run azd env set ALLOWED_ORIGIN https://&lt;your-domain.com&gt; Run azd up  For the frontend code, change BACKEND_URI in api.ts to point at the deployed backend URL, so that all fetch requests will be sent to the deployed backend. Running locally You can only run locally after having successfully run the azd up command. If you haven't yet, follow the steps in Azure deployment above.  Run azd auth login Change dir to app Run ./start.ps1 or ./start.sh or run the "VS Code Task: Start App" to start the project locally.  Using the app  In Azure: navigate to the Azure WebApp deployed by azd. The URL is printed out when azd completes (as "Endpoint"), or you can find it in the Azure portal. Running locally: navigate to 127.0.0.1:50505  Once in the web app:  Try different topics in chat or Q&amp;A context. For chat, try follow up questions, clarifications, ask to simplify or elaborate on answer, etc. Explore citations and sources Click on "settings" to try different options, tweak prompts, etc.  Productionizing This sample is designed to be a starting point for your own production application, but you should do a thorough review of the security and performance before deploying to production. Here are some things to consider:  OpenAI Capacity: The default TPM (tokens per minute) is set to 30K. That is equivalent to approximately 30 conversations per minute (assuming 1K per user message/response). You can increase the capacity by changing the chatGptDeploymentCapacity and embeddingDeploymentCapacity parameters in infra/main.bicep to your account's maximum capacity. You can also view the Quotas tab in Azure OpenAI studio to understand how much capacity you have. Azure Storage: The default storage account uses the Standard_LRS SKU. To improve your resiliency, we recommend using Standard_ZRS for production deployments, which you can specify using the sku property under the storage module in infra/main.bicep. Azure Cognitive Search: The default search service uses the Standard SKU with the free semantic search option, which gives you 1000 free queries a month. Assuming your app will experience more than 1000 questions, you should either change semanticSearch to "standard" or disable semantic search entirely in the /app/backend/approaches files. If you see errors about search service capacity being exceeded, you may find it helpful to increase the number of replicas by changing replicaCount in infra/core/search/search-services.bicep or manually scaling it from the Azure Portal. Azure App Service: The default app service plan uses the Basic SKU with 1 CPU core and 1.75 GB RAM. We recommend using a Premium level SKU, starting with 1 CPU core. You can use auto-scaling rules or scheduled scaling rules, and scale up the maximum/minimum based on load. Authentication: By default, the deployed app is publicly accessible. We recommend restricting access to authenticated users. See Enabling authentication above for how to enable authentication. Networking: We recommend deploying inside a Virtual Network. If the app is only for internal enterprise use, use a private DNS zone. Also consider using Azure API Management (APIM) for firewalls and other forms of protection. For more details, read Azure OpenAI Landing Zone reference architecture. Loadtesting: We recommend running a loadtest for your expected number of users. You can use the locust tool with the locustfile.py in this sample or set up a loadtest with Azure Load Testing.  Resources  Revolutionize your Enterprise Data with ChatGPT: Next-gen Apps w/ Azure OpenAI and Cognitive Search Azure Cognitive Search Azure OpenAI Service Comparing Azure OpenAI and OpenAI  Clean up To clean up all the resources created by this sample:  Run azd down When asked if you are sure you want to continue, enter y When asked if you want to permanently delete the resources, enter y  The resource group and all the resources will be deleted. Note  Note: The PDF documents used in this demo contain information generated using a language model (Azure OpenAI Service). The information contained in these documents is only for demonstration purposes and does not reflect the opinions or beliefs of Microsoft. Microsoft makes no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the information contained in this document. All rights reserved to Microsoft.  FAQ  Why do we need to break up the PDFs into chunks when Azure Cognitive Search supports searching large documents? Chunking allows us to limit the amount of information we send to OpenAI due to token limits. By breaking up the content, it allows us to easily find potential chunks of text that we can inject into OpenAI. The method of chunking we use leverages a sliding window of text such that sentences that end one chunk will start the next. This allows us to reduce the chance of losing the context of the text.   How can we upload additional PDFs without redeploying everything? To upload more PDFs, put them in the data/ folder and run ./scripts/prepdocs.sh or ./scripts/prepdocs.ps1. To avoid reuploading existing docs, move them out of the data folder. You could also implement checks to see whats been uploaded before; our code doesn't yet have such checks.   How does this sample compare to other Chat with Your Data samples? Another popular repository for this use case is here: https://github.com/Microsoft/sample-app-aoai-chatGPT/ That repository is designed for use by customers using Azure OpenAI studio and Azure Portal for setup. It also includes azd support for folks who want to deploy it completely from scratch. The primary differences:  This repository includes multiple RAG (retrieval-augmented generation) approaches that chain the results of multiple API calls (to Azure OpenAI and ACS) together in different ways. The other repository uses only the built-in data sources option for the ChatCompletions API, which uses a RAG approach on the specified ACS index. That should work for most uses, but if you needed more flexibility, this sample may be a better option. This repository is also a bit more experimental in other ways, since it's not tied to the Azure OpenAI Studio like the other repository.  Feature comparison:    Feature azure-search-openai-demo sample-app-aoai-chatGPT     RAG approach Multiple approaches Only via ChatCompletion API data_sources   Vector support ✅ Yes ✅ Yes   Data ingestion ✅ Yes (PDF) ✅ Yes (PDF, TXT, MD, HTML)   Persistent chat history ❌ No (browser tab only) ✅ Yes, in CosmosDB    Technology comparison:    Tech azure-search-openai-demo sample-app-aoai-chatGPT     Frontend React React   Backend Python (Quart) Python (Flask)   Vector DB Azure Cognitive Search Azure Cognitive Search   Deployment Azure Developer CLI (azd) Azure Portal, az, azd      How do you use GPT-4 with this sample? In infra/main.bicep, change chatGptModelName to 'gpt-4' instead of 'gpt-35-turbo'. You may also need to adjust the capacity above that line depending on how much TPM your account is allowed.   What is the difference between the Chat and Ask tabs? The chat tab uses the approach programmed in chatreadretrieveread.py.  It uses the ChatGPT API to turn the user question into a good search query. It queries Azure Cognitive Search for search results for that query (optionally using the vector embeddings for that query). It then combines the search results and original user question, and asks ChatGPT API to answer the question based on the sources. It includes the last 4K of message history as well (or however many tokens are allowed by the deployed model).  The ask tab uses the approach programmed in retrievethenread.py.  It queries Azure Cognitive Search for search results for the user question (optionally using the vector embeddings for that question). It then combines the search results and user question, and asks ChatGPT API to answer the question based on the sources.    What does the `azd up` command do? The azd up command comes from the Azure Developer CLI, and takes care of both provisioning the Azure resources and deploying code to the selected Azure hosts. The azd up command uses the azure.yaml file combined with the infrastructure-as-code .bicep files in the infra/ folder. The azure.yaml file for this project declares several "hooks" for the prepackage step and postprovision steps. The up command first runs the prepackage hook which installs Node dependencies and builds the React.JS-based JavaScript files. It then packages all the code (both frontend and backend) into a zip file which it will deploy later. Next, it provisions the resources based on main.bicep and main.parameters.json. At that point, since there is no default value for the OpenAI resource location, it asks you to pick a location from a short list of available regions. Then it will send requests to Azure to provision all the required resources. With everything provisioned, it runs the postprovision hook to process the local data and add it to an Azure Cognitive Search index. Finally, it looks at azure.yaml to determine the Azure host (appservice, in this case) and uploads the zip to Azure App Service. The azd up command is now complete, but it may take another 5-10 minutes for the App Service app to be fully available and working, especially for the initial deploy. Related commands are azd provision for just provisioning (if infra files change) and azd deploy for just deploying updated app code.   How can we view logs from the App Service app? You can view production logs in the Portal using either the Log stream or by downloading the default_docker.log file from Advanced tools. The following line of code in app/backend/app.py configures the logging level: logging.basicConfig(level=os.getenv("APP_LOG_LEVEL", default_level)) To change the default level, either change default_level or set the APP_LOG_LEVEL environment variable to one of the allowed log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL. If you need to log in a route handler, use the the global variable current_app's logger: async def chat():     current_app.logger.info("Received /chat request") Otherwise, use the logging module's root logger: logging.info("System message: %s", system_message) If you're having troubles finding the logs in App Service, see this blog post on tips for debugging App Service app deployments or watch this video about viewing App Service logs.  Troubleshooting Here are the most common failure scenarios and solutions:   The subscription (AZURE_SUBSCRIPTION_ID) doesn't have access to the Azure OpenAI service. Please ensure AZURE_SUBSCRIPTION_ID matches the ID specified in the OpenAI access request process.   You're attempting to create resources in regions not enabled for Azure OpenAI (e.g. East US 2 instead of East US), or where the model you're trying to use isn't enabled. See this matrix of model availability.   You've exceeded a quota, most often number of resources per region. See this article on quotas and limits.   You're getting "same resource name not allowed" conflicts. That's likely because you've run the sample multiple times and deleted the resources you've been creating each time, but are forgetting to purge them. Azure keeps resources for 48 hours unless you purge from soft delete. See this article on purging resources.   You see CERTIFICATE_VERIFY_FAILED when the prepdocs.py script runs. That's typically due to incorrect SSL certificates setup on your machine. Try the suggestions in this StackOverflow answer.   After running azd up and visiting the website, you see a '404 Not Found' in the browser. Wait 10 minutes and try again, as it might be still starting up. Then try running azd deploy and wait again. If you still encounter errors with the deployed app, consult these tips for debugging App Service app deployments or watch this video about downloading App Service logs. Please file an issue if the logs don't help you resolve the error.     </readme><commitcount>289</commitcount><languages>Python 62.3%	TypeScript 22.6%	Bicep 7.8%	CSS 3.2%	PowerShell 2.5%	Shell 1.5%	HTML 0.1%</languages><tags>azure	openai	azurecognitivesearch	azd-templates	chatgpt	azureopenai</tags><about>A sample app for the Retrieval-Augmented Generation pattern running in Azure, using Azure Cognitive Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&amp;A experiences.</about><starcount>3.9k</starcount><watchcount>0</watchcount></repository><repository><username>felipemotarocha</username><reponame>fullstackweek-store</reponame><readme>            Projeto Full Stack Week 2.0 💻 Tecnologias Utilizadas 🚀 Funcionalidades 📦 Protótipo no Figma 🎨 Contribuições e Colaborações 🤝 Como Contribuir      README.md     Projeto Full Stack Week 2.0 💻 Este é o repositório oficial do e-commerce desenvolvido durante a Full Stack Week, um evento diferente de tudo que você já viu, com 4 lives de muito conteúdo. Nosso objetivo principal é criar um projeto altamente relevante, utilizando as tecnologias mais modernas e demandadas pelo mercado, a fim de adicionar autoridade ao currículo de desenvolvedor. Tecnologias Utilizadas 🚀   React: Uma biblioteca JavaScript popular para construir interfaces de usuário interativas.   Next.js 13: Um framework React que oferece renderização do lado do servidor (SSR), geração estática (SSG), entre muitos outros recursos.   Next Auth: Biblioteca para autenticação de usuários com OAuth.   Postgres: Um sistema de gerenciamento de banco de dados relacional.   Prisma: Um ORM (Object-Relational Mapping) para Node.js e TypeScript.   shadcn/ui: Uma biblioteca de componentes de IU reutilizáveis e estilizáveis.   Tailwind CSS: Um framework CSS que oferece várias classes para utilização já pré-estilizadas.   API do Stripe: Uma API de pagamento popular para processar pagamentos online de forma segura.   Funcionalidades 📦   Login com o Google: Permitimos que os usuários façam login usando suas contas do Google para uma experiência de autenticação simplificada.   Navegação por Categorias: Os usuários podem explorar produtos por categorias, facilitando a busca e a compra.   Descontos em Produtos: Alguns produtos podem ter descontos especiais, permitindo aos usuários economizar em suas compras.   Gerenciamento do Carrinho de Compras: Os usuários podem adicionar produtos ao seu carrinho de compras, remover produtos e também modificar a quantidade de um produto no carrinho de compras conforme necessário.   Pagamento do Pedido com a API do Stripe: Oferecemos uma experiência segura de pagamento online com a integração da API do Stripe, incluindo o uso de webhooks para processar eventos relacionados ao pagamento. Os usuários podem concluir seus pedidos com facilidade e segurança.   Protótipo no Figma 🎨 Você pode visualizar o protótipo do nosso projeto no Figma. Ele oferece uma prévia visual de como a interface do usuário é projetada e como as diferentes funcionalidades são organizadas. Confira o protótipo aqui. Fique à vontade para explorar e compartilhar suas opiniões sobre o design do projeto! Contribuições e Colaborações 🤝 Este projeto está totalmente aberto a contribuições. Se você deseja colaborar, fique à vontade para criar pull requests, corrigir bugs, adicionar novos recursos ou aprimorar a documentação. Sua contribuição é valiosa e ajuda a melhorar ainda mais este projeto! Como Contribuir   Faça um fork deste repositório.   Crie uma branch para sua contribuição:       git checkout -b minha-contribuicao   Faça suas alterações e adicione commits descritivos (seguindo o Conventional Commits, preferencialmente).   Crie um pull request para a branch main deste repositório.     </readme><commitcount>57</commitcount><languages>TypeScript 98.8%</languages><tags /><about>A sample app for the Retrieval-Augmented Generation pattern running in Azure, using Azure Cognitive Search for retrieval and Azure OpenAI large language models to power ChatGPT-style and Q&amp;A experiences.</about><starcount>113</starcount><watchcount>0</watchcount></repository><repository><username>Azure</username><reponame>azure-sdk-for-java</reponame><readme>               Azure SDK for Java Getting started Prerequisites Available packages Client Libraries Management Libraries Historical Releases Need help? Navigating the repository Main branch Release branches (Release tagging) Contributing Additional Helpful Links for Contributors Reporting security issues and security bugs License      README.md     Azure SDK for Java   This repository is for active development of the Azure SDK for Java. For consumers of the SDK we recommend visiting our public developer docs or our versioned developer docs. Getting started To get started with a specific service library, see the README.md file located in the library's project folder. You can find service libraries in the /sdk directory. For a list of all the services we support access our list of all existing libraries. For tutorials, samples, quick starts and other documentation, visit Azure for Java Developers. Prerequisites All libraries are baselined on Java 8, with testing and forward support up until the latest Java long-term support release (currently Java 17). Available packages Each service can have both 'client' and 'management' libraries. 'Client' libraries are used to consume the service, whereas 'management' libraries are used to configure and manage the service. Client Libraries Our client libraries follow the Azure SDK Design Guidelines for Java, and share a number of core features such as HTTP retries, logging, transport protocols, authentication protocols, etc., so that once you learn how to use these features in one client library, you will know how to use them in other client libraries. You can learn about these shared features here. These libraries can be easily identified by folder, package, and namespaces names starting with azure-, e.g. azure-keyvault. You can find the most up to date list of all of the new packages on our page. This list includes the most recent releases: both stable and beta.  NOTE: If you need to ensure your code is ready for production use one of the stable, non-beta libraries.  Management Libraries Similar to our client libraries, the management libraries follow the Azure SDK Design Guidelines for Java. These libraries provide a high-level, object-oriented API for managing Azure resources, that are optimized for ease of use, succinctness, and consistency. You can find the list of management libraries on this page. For general documentation on how to use the new libraries for Azure Resource Management, please visit here. We have also prepared plenty of code samples as well as migration guide in case you are upgrading from previous versions. The management libraries can be identified by namespaces that start with azure-resourcemanager, e.g. azure-resourcemanager-compute. Historical Releases Note that the latest libraries from Microsoft are in the com.azure Maven group ID, and have the package naming pattern of beginning with com.azure. If you're using libraries that are in com.microsoft.azure Maven group ID, or have this as the package structure, please consider migrating to the latest libraries. You can find a mapping table from these historical releases to their equivalent here. Need help?  For reference documentation visit the Azure SDK for Java documentation. For tutorials, samples, quick starts and other documentation, visit Azure for Java Developers. For build reports on code quality, test coverage, etc, visit Azure Java SDK. File an issue via Github Issues. Check previous questions or ask new ones on StackOverflow using azure-java-sdk tag.  Navigating the repository Main branch The main branch has the most recent code with new features and bug fixes. It does not represent latest released stable SDK. Release branches (Release tagging) For each package we release there will be a unique git tag created that contains the name and the version of the package to mark the commit of the code that produced the package. This tag will be used for servicing via hotfix branches as well as debugging the code for a particular beta or stable release version. Format of the release tags are &lt;package-name&gt;_&lt;package-version&gt;. For more information please see our branching strategy. Contributing For details on contributing to this repository, see the contributing guide. This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, view Microsoft's CLA. When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA. This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments. Additional Helpful Links for Contributors Many people all over the world have helped make this project better.  You'll want to check out:  What are some good first issues for new contributors to the repo? How to build and test your change How you can make a change happen! Frequently Asked Questions (FAQ) and Conceptual Topics in the detailed Azure SDK for Java wiki.  Reporting security issues and security bugs Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) secure@microsoft.com. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the Security TechCenter. License Azure SDK for Java is licensed under the MIT license.    </readme><commitcount>28,176</commitcount><languages>Java 99.5%	Scala 0.3%	PowerShell 0.1%	Groovy 0.1%	Python 0.0%	Jupyter Notebook 0.0%</languages><tags>java	azure	media	azure-sdk	azure-resources	hacktoberfest	azure-services</tags><about>This repository is for active development of the Azure SDK for Java. For consumers of the SDK we recommend visiting our public developer docs at https://docs.microsoft.com/java/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-java.</about><starcount>2.1k</starcount><watchcount>0</watchcount></repository><repository><username>trufflesecurity</username><reponame>trufflehog</reponame><readme>               TruffleHog 🔎 Now Scanning 📢 Join Our Community 📺 Demo 💾 Installation 🚀 Quick Start 1: Scan a repo for only verified secrets 2: Scan a GitHub Org for only verified secrets 3: Scan a GitHub Repo for only verified keys and get JSON output 4: Scan a GitHub Repo + its Issues and Pull Requests. 5: Scan an S3 bucket for verified keys 6: Scan S3 buckets using IAM Roles 7: Scan a Github Repo using SSH authentication in docker 8: Scan individual files or directories 9: Scan GCS buckets for verified secrets. 10: Scan a Docker image for verified secrets. ❓ FAQ 📰 What's new in v3? What is credential verification? 📝 Usage S3  TruffleHog Github Action Pre-commit Hook Regex Detector (alpha) Regex Detector Example Verification Server Example (Python) ❤️ Contributors 💻 Contributing Adding new secret detectors Use as a library License Change 💸 Enterprise product      README.md       TruffleHog Find leaked credentials.         🔎 Now Scanning   ...and more  📢 Join Our Community Have questions? Feedback? Jump in slack or discord and hang out with us Join our Slack Community Join the Secret Scanning Discord 📺 Demo  docker run --rm -it -v "$PWD:/pwd" trufflesecurity/trufflehog:latest github --org=trufflesecurity 💾 Installation Several options available for you: # MacOS users brew install trufflesecurity/trufflehog/trufflehog  # Docker docker run --rm -it -v "$PWD:/pwd" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys  # Docker for M1 and M2 Mac docker run --platform linux/arm64 --rm -it -v "$PWD:/pwd" trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys  # Binary releases Download and unpack from https://github.com/trufflesecurity/trufflehog/releases  # Compile from source git clone https://github.com/trufflesecurity/trufflehog.git cd trufflehog; go install  # Using installation script curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin # Using installation script to install a specific version curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin &lt;ReleaseTag like v3.56.0&gt; 🚀 Quick Start 1: Scan a repo for only verified secrets Command: trufflehog git https://github.com/trufflesecurity/test_keys --only-verified Expected output: 🐷🔑🐷  TruffleHog. Unearth your secrets. 🐷🔑🐷  Found verified result 🐷🔑 Detector Type: AWS Decoder Type: PLAIN Raw result: AKIAYVP4CIPPERUVIFXG Line: 4 Commit: fbc14303ffbf8fb1c2c1914e8dda7d0121633aca File: keys Email: counter &lt;counter@counters-MacBook-Air.local&gt; Repository: https://github.com/trufflesecurity/test_keys Timestamp: 2022-06-16 10:17:40 -0700 PDT ...  2: Scan a GitHub Org for only verified secrets trufflehog github --org=trufflesecurity --only-verified 3: Scan a GitHub Repo for only verified keys and get JSON output Command: trufflehog git https://github.com/trufflesecurity/test_keys --only-verified --json Expected output: {"SourceMetadata":{"Data":{"Git":{"commit":"fbc14303ffbf8fb1c2c1914e8dda7d0121633aca","file":"keys","email":"counter \u003ccounter@counters-MacBook-Air.local\u003e","repository":"https://github.com/trufflesecurity/test_keys","timestamp":"2022-06-16 10:17:40 -0700 PDT","line":4}}},"SourceID":0,"SourceType":16,"SourceName":"trufflehog - git","DetectorType":2,"DetectorName":"AWS","DecoderName":"PLAIN","Verified":true,"Raw":"AKIAYVP4CIPPERUVIFXG","Redacted":"AKIAYVP4CIPPERUVIFXG","ExtraData":{"account":"595918472158","arn":"arn:aws:iam::595918472158:user/canarytokens.com@@mirux23ppyky6hx3l6vclmhnj","user_id":"AIDAYVP4CIPPJ5M54LRCY"},"StructuredData":null} ...  4: Scan a GitHub Repo + its Issues and Pull Requests. trufflehog github --repo=https://github.com/trufflesecurity/test_keys --issue-comments --pr-comments 5: Scan an S3 bucket for verified keys trufflehog s3 --bucket=&lt;bucket name&gt; --only-verified 6: Scan S3 buckets using IAM Roles trufflehog s3 --role-arn=&lt;iam role arn&gt; 7: Scan a Github Repo using SSH authentication in docker docker run --rm -v "$HOME/.ssh:/root/.ssh:ro" trufflesecurity/trufflehog:latest git ssh://github.com/trufflesecurity/test_keys 8: Scan individual files or directories trufflehog filesystem path/to/file1.txt path/to/file2.txt path/to/dir 9: Scan GCS buckets for verified secrets. trufflehog gcs --project-id=&lt;project-ID&gt; --cloud-environment --only-verified 10: Scan a Docker image for verified secrets. Use the --image flag multiple times to scan multiple images. trufflehog docker --image trufflesecurity/secrets --only-verified ❓ FAQ  All I see is 🐷🔑🐷  TruffleHog. Unearth your secrets. 🐷🔑🐷 and the program exits, what gives?  That means no secrets were detected   Why is the scan is taking a long time when I scan a GitHub org  Unauthenticated GitHub scans have rate limits. To improve your rate limits, include the --token flag with a personal access token   It says a private key was verified, what does that mean?  Check out our Driftwood blog post to learn how to do this, in short we've confirmed the key can be used live for SSH or SSL Blog post   Is there an easy way to ignore specific secrets?  If the scanned source supports line numbers, then you can add a trufflehog:ignore comment on the line containing the secret to ignore that secrets.    📰 What's new in v3? TruffleHog v3 is a complete rewrite in Go with many new powerful features.  We've added over 700 credential detectors that support active verification against their respective APIs. We've also added native support for scanning GitHub, GitLab, filesystems, S3, GCS and Circle CI. Instantly verify private keys against millions of github users and billions of TLS certificates using our Driftwood technology. Scan binaries and other file formats Available as a GitHub Action and a pre-commit hook  What is credential verification? For every potential credential that is detected, we've painstakingly implemented programmatic verification against the API that we think it belongs to. Verification eliminates false positives. For example, the AWS credential detector performs a GetCallerIdentity API call against the AWS API to verify if an AWS credential is active. 📝 Usage TruffleHog has a sub-command for each source of data that you may want to scan:  git github gitlab docker S3 filesystem (files and directories) syslog circleci GCS (Google Cloud Storage) stdin (coming soon)  Each subcommand can have options that you can see with the --help flag provided to the sub command: $ trufflehog git --help usage: TruffleHog git [&lt;flags&gt;] &lt;uri&gt;  Find credentials in git repositories.  Flags:       --help                     Show context-sensitive help (also try --help-long and --help-man).       --debug                    Run in debug mode.       --trace                    Run in trace mode.       --profile                  Enables profiling and sets a pprof and fgprof server on :18066.   -j, --json                     Output in JSON format.       --json-legacy              Use the pre-v3.0 JSON format. Only works with git, gitlab, and github sources.       --concurrency=10           Number of concurrent workers.       --no-verification          Don't verify the results.       --only-verified            Only output verified results.       --filter-unverified        Only output first unverified result per chunk per detector if there are more than one results.       --config=CONFIG            Path to configuration file.       --print-avg-detector-time  Print the average time spent on each detector.       --no-update                Don't check for updates.       --fail                     Exit with code 183 if results are found.       --version                  Show application version.  Args:   &lt;uri&gt;  Git repository URL. https://, file://, or ssh:// schema expected.  For example, to scan a  git repository, start with $ trufflehog git https://github.com/trufflesecurity/trufflehog.git  S3 The S3 source supports assuming IAM roles for scanning in addition to IAM users. This makes it easier for users to scan multiple AWS accounts without needing to rely on hardcoded credentials for each account. The IAM identity that TruffleHog uses initially will need to have AssumeRole privileges as a principal in the trust policy of each IAM role to assume. To scan a specific bucket using locally set credentials or instance metadata if on an EC2 instance: trufflehog s3 --bucket=&lt;bucket-name&gt; To scan a specific bucket using an assumed role: trufflehog s3 --bucket=&lt;bucket-name&gt; --role-arn=&lt;iam-role-arn&gt; Multiple roles can be passed as separate arguments. The following command will attempt to scan every bucket each role has permissions to list in the S3 API: trufflehog s3 --role-arn=&lt;iam-role-arn-1&gt; --role-arn=&lt;iam-role-arn-2&gt; Exit Codes:  0: No errors and no results were found. 1: An error was encountered. Sources may not have completed scans. 183: No errors were encountered, but results were found. Will only be returned if --fail flag is used.   TruffleHog Github Action - name: TruffleHog   uses: trufflesecurity/trufflehog@main   with:     # Repository path     path:     # Start scanning from here (usually main branch).     base:     # Scan commits until here (usually dev branch).     head: # optional     # Extra args to be passed to the trufflehog cli.     extra_args: --debug --only-verified The TruffleHog OSS Github Action can be used to scan a range of commits for leaked credentials. The action will fail if any results are found. For example, to scan the contents of pull requests you could use the following workflow: name: TruffleHog Secrets Scan on: [pull_request] jobs:   TruffleHog:     runs-on: ubuntu-latest     steps:       - name: Checkout code         uses: actions/checkout@v3         with:           fetch-depth: 0       - name: TruffleHog OSS         uses: trufflesecurity/trufflehog@main         with:           path: ./           base: ${{ github.event.repository.default_branch }}           head: HEAD           extra_args: --debug --only-verified Pre-commit Hook Trufflehog can be used in a pre-commit hook to prevent credentials from leaking before they ever leave your computer. An example .pre-commit-config.yaml is provided (see pre-commit.com for installation). repos: - repo: local   hooks:     - id: trufflehog       name: TruffleHog       description: Detect secrets in your data.       entry: bash -c 'trufflehog git file://. --since-commit HEAD --only-verified --fail'       # For running trufflehog in docker, use the following entry instead:       # entry: bash -c 'docker run --rm -v "$(pwd):/workdir" -i --rm trufflesecurity/trufflehog:latest git file:///workdir --since-commit HEAD --only-verified --fail'       language: system       stages: ["commit", "push"] Regex Detector (alpha) Trufflehog supports detection and verification of custom regular expressions. For detection, at least one regular expression and keyword is required. A keyword is a fixed literal string identifier that appears in or around the regex to be detected. To allow maximum flexibility for verification, a webhook is used containing the regular expression matches. Trufflehog will send a JSON POST request containing the regex matches to a configured webhook endpoint. If the endpoint responds with a 200 OK response status code, the secret is considered verified. NB: This feature is alpha and subject to change. Regex Detector Example # config.yaml detectors: - name: hog detector   keywords:   - hog   regex:     adjective: hogs are (\S+)   verify:   - endpoint: http://localhost:8000/     # unsafe must be set if the endpoint is HTTP     unsafe: true     headers:     - 'Authorization: super secret authorization header' $ trufflehog filesystem /tmp --config config.yaml --only-verified 🐷🔑🐷  TruffleHog. Unearth your secrets. 🐷🔑🐷  Found verified result 🐷🔑 Detector Type: CustomRegex Decoder Type: PLAIN Raw result: hogs are cool File: /tmp/hog-facts.txt  Verification Server Example (Python) Unless you run a verification server, secrets found by the custom regex detector will be unverified. Here is an example Python implementation of a verification server for the above config.yaml file. import json from http.server import BaseHTTPRequestHandler, HTTPServer  AUTH_HEADER = 'super secret authorization header'   class Verifier(BaseHTTPRequestHandler):     def do_GET(self):         self.send_response(405)         self.end_headers()      def do_POST(self):         try:             if self.headers['Authorization'] != AUTH_HEADER:                 self.send_response(401)                 self.end_headers()                 return              # read the body             length = int(self.headers['Content-Length'])             request = json.loads(self.rfile.read(length))             self.log_message("%s", request)              # check the match             if request['hog detector']['adjective'][-1] == 'cool':                 self.send_response(200)                 self.end_headers()             else:                 # any other response besides 200                 self.send_response(406)                 self.end_headers()         except Exception:             self.send_response(400)             self.end_headers()   with HTTPServer(('', 8000), Verifier) as server:     try:         server.serve_forever()     except KeyboardInterrupt:         pass ❤️ Contributors This project exists thanks to all the people who contribute. [Contribute].    💻 Contributing Contributions are very welcome! Please see our contribution guidelines first. We no longer accept contributions to TruffleHog v2, but that code is available in the v2 branch. Adding new secret detectors We have published some documentation and tooling to get started on adding new secret detectors. Let's improve detection together! Use as a library Currently, trufflehog is in heavy development and no guarantees can be made on the stability of the public APIs at this time. License Change Since v3.0, TruffleHog is released under a AGPL 3 license, included in LICENSE. TruffleHog v3.0 uses none of the previous codebase, but care was taken to preserve backwards compatibility on the command line interface. The work previous to this release is still available licensed under GPL 2.0 in the history of this repository and the previous package releases and tags. A completed CLA is required for us to accept contributions going forward. 💸 Enterprise product Are you interested in continuously monitoring your Git, Jira, Slack, Confluence, etc.. for credentials? We have an enterprise product that can help. Reach out here to learn more https://trufflesecurity.com/contact/ We take the revenue from the enterprise product to fund more awesome open source projects that the whole community can benefit from.   </readme><commitcount>2,389</commitcount><languages>Go 99.7%</languages><tags>security	credentials	secret	secret-management	verification	secrets	dynamic-analysis	scanning	hacktoberfest	precommit	security-tools	devsecops	trufflehog</tags><about>Find and verify credentials</about><starcount>12.4k</starcount><watchcount>0</watchcount></repository><repository><username>Kong</username><reponame>kong</reponame><readme>               Getting Started Features Plugin Hub Contributing Releases Join the Community Konnect Cloud License      README.md             Kong or Kong API Gateway is a cloud-native, platform-agnostic, scalable API Gateway distinguished for its high performance and extensibility via plugins. By providing functionality for proxying, routing, load balancing, health checking, authentication (and more), Kong serves as the central layer for orchestrating microservices or conventional API traffic with ease. Kong runs natively on Kubernetes thanks to its official Kubernetes Ingress Controller.  Installation | Documentation | Discussions | Forum | Blog | Builds  Getting Started Let’s test drive Kong by adding authentication to an API in under 5 minutes. We suggest using the docker-compose distribution via the instructions below, but there is also a docker installation procedure if you’d prefer to run the Kong API Gateway in DB-less mode. Whether you’re running in the cloud, on bare metal, or using containers, you can find every supported distribution on our official installation page.  To start, clone the Docker repository and navigate to the compose folder.    $ git clone https://github.com/Kong/docker-kong   $ cd docker-kong/compose/  Start the Gateway stack using:    $ KONG_DATABASE=postgres docker-compose --profile database up The Gateway is now available on the following ports on localhost:  :8000 - send traffic to your service via Kong :8001 - configure Kong using Admin API or via decK :8002 - access Kong's management Web UI (Kong Manager) on localhost:8002  Next, follow the quick start guide to tour the Gateway features. Features By centralizing common API functionality across all your organization's services, the Kong API Gateway creates more freedom for engineering teams to focus on the challenges that matter most. The top Kong features include:  Advanced routing, load balancing, health checking - all configurable via a RESTful admin API or declarative configuration. Authentication and authorization for APIs using methods like JWT, basic auth, OAuth, ACLs and more. Proxy, SSL/TLS termination, and connectivity support for L4 or L7 traffic. Plugins for enforcing traffic controls, rate limiting, req/res transformations, logging, monitoring and including a plugin developer hub. Sophisticated deployment models like Declarative Databaseless Deployment and Hybrid Deployment (control plane/data plane separation) without any vendor lock-in. Native ingress controller support for serving Kubernetes.   Plugin Hub Plugins provide advanced functionality that extends the use of the Gateway. Many of the Kong Inc. and community-developed plugins like AWS Lambda, Correlation ID, and Response Transformer are showcased at the Plugin Hub. Contribute to the Plugin Hub and ensure your next innovative idea is published and available to the broader community! Contributing We ❤️ pull requests, and we’re continually working hard to make it as easy as possible for developers to contribute. Before beginning development with the Kong API Gateway, please familiarize yourself with the following developer resources:  Community Pledge (COMMUNITY_PLEDGE.md) for our pledge to interact with you, the open source community. Contributor Guide (CONTRIBUTING.md) to learn about how to contribute to Kong. Development Guide (DEVELOPER.md): Setting up your development environment. CODE_OF_CONDUCT and COPYRIGHT  Use the Plugin Development Guide for building new and creative plugins, or browse the online version of Kong's source code documentation in the Plugin Development Kit (PDK) Reference. Developers can build plugins in Lua, Go or JavaScript. Releases Please see the Changelog for more details about a given release. The SemVer Specification is followed when versioning Gateway releases. Join the Community  Check out the docs Join the Kong discussions forum Join the Kong discussions at the Kong Nation forum: https://discuss.konghq.com/ Join our Community Slack Read up on the latest happenings at our blog Follow us on X Subscribe to our YouTube channel Visit our homepage to learn more  Konnect Cloud Kong Inc. offers commercial subscriptions that enhance the Kong API Gateway in a variety of ways. Customers of Kong's Konnect Cloud subscription take advantage of additional gateway functionality, commercial support, and access to Kong's managed (SaaS) control plane platform. The Konnect Cloud platform features include real-time analytics, a service catalog, developer portals, and so much more! Get started with Konnect Cloud. License Copyright 2016-2023 Kong Inc.  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at     https://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.    </readme><commitcount>9,851</commitcount><languages>Lua 87.6%	Perl 9.8%	Shell 1.0%	Starlark 0.9%	Python 0.5%	Makefile 0.1%</languages><tags>docker	nginx	kubernetes	iot	devops	microservices	microservice	serverless	consul	api-management	api-gateway	luajit	reverse-proxy	kong	apis	cloud-native	kubernetes-ingress	kubernetes-ingress-controller	servicecontrol</tags><about>🦍 The Cloud-Native API Gateway</about><starcount>36.1k</starcount><watchcount>0</watchcount></repository><repository><username>nextauthjs</username><reponame>next-auth</reponame><readme>               Auth.js Features Flexible and easy to use Own your data Secure by default TypeScript Security Acknowledgments Sponsors Contributing License      README.md        Auth.js Authentication for the Web. Open Source. Full Stack. Own Your Data.                Auth.js is a set of open-source packages that are built on Web Standard APIs for authentication in modern applications with any framework on any platform in any JS runtime. See authjs.dev for our framework-specific libraries, or check out next-auth.js.org for next-auth (Next.js). Features Flexible and easy to use  Designed to work with any OAuth service, it supports 2.0+, OIDC Built-in support for many popular sign-in services Email/Passwordless authentication Bring Your Database - or none! - stateless authentication with any backend (Active Directory, LDAP, etc.) Runtime-agnostic, runs anywhere! (Vercel Edge Functions, Node.js, Serverless, etc.)  Own your data Auth.js can be used with or without a database.  An open-source solution that allows you to keep control of your data Built-in support for MySQL, MariaDB, Postgres, Microsoft SQL Server, MongoDB, SQLite, etc. Works great with databases from popular hosting providers  Secure by default  Promotes the use of passwordless sign-in mechanisms Designed to be secure by default and encourage best practices for safeguarding user data Uses Cross-Site Request Forgery (CSRF) Tokens on POST routes (sign in, sign out) Default cookie policy aims for the most restrictive policy appropriate for each cookie When JSON Web Tokens are used, they are encrypted by default (JWE) with A256GCM Features tab/window syncing and session polling to support short-lived sessions Attempts to implement the latest guidance published by Open Web Application Security Project  Advanced configuration allows you to define your routines to handle controlling what accounts are allowed to sign in, for encoding and decoding JSON Web Tokens and to set custom cookie security policies and session properties, so you can control who can sign in and how often sessions have to be re-validated. TypeScript Auth.js libraries are written with type safety in mind. Check out the docs for more information. Security If you think you have found a vulnerability (or are not sure) in Auth.js or any of the related packages (i.e. Adapters), we ask you to read our Security Policy to reach out responsibly. Please do not open Pull Requests/Issues/Discussions before consulting with us. Acknowledgments Auth.js is made possible thanks to all of its contributors.       Sponsors         We have an OpenCollective for companies and individuals looking to contribute financially to the project!        Vercel 🥉 Bronze Financial Sponsor  ☁️ Infrastructure Support      Prisma 🥉 Bronze Financial Sponsor      Clerk 🥉 Bronze Financial Sponsor      Lowdefy 🥉 Bronze Financial Sponsor      WorkOS 🥉 Bronze Financial Sponsor      Descope 🥉 Bronze Financial Sponsor      Checkly ☁️ Infrastructure Support      superblog ☁️ Infrastructure Support      Contributing We're open to all community contributions! If you'd like to contribute in any way, please first read our Contributing Guide. License ISC   </readme><commitcount>2,533</commitcount><languages>TypeScript 90.8%	JavaScript 5.8%	CSS 1.2%	Shell 1.0%	Svelte 0.6%	PLpgSQL 0.3%</languages><tags>react	nodejs	oauth	jwt	oauth2	web	authentication	nextjs	nuxt	auth	csrf	oidc	solidjs	nuxt-auth	nextauth	sveltekit	remix-auth	solid-auth</tags><about>Authentication for the Web.</about><starcount>19.4k</starcount><watchcount>0</watchcount></repository><repository><username>t3-oss</username><reponame>create-t3-app</reponame><readme>                  create-t3-app  Table of contents The T3 Stack So... what is create-t3-app? A template? T3 Axioms 1. Solve Problems 2. Bleed Responsibly 3. Typesafety Isn't Optional Getting Started npm yarn pnpm bun Community Contributors      README.md              create-t3-app     Interactive CLI to start a full-stack, typesafe Next.js app.     Get started with the T3 Stack by running npm create t3-app@latest              Watch Theo's overview on Youtube here  Table of contents  The T3 Stack T3 Axioms Getting Started Community Contributors  The T3 Stack The "T3 Stack" is a web development stack made by Theo focused on simplicity, modularity, and full-stack typesafety. It consists of:  Next.js tRPC Tailwind CSS TypeScript Prisma NextAuth.js  So... what is create-t3-app? A template? Kind of? create-t3-app is a CLI built by seasoned T3 Stack devs to streamline the setup of a modular T3 Stack app. This means each piece is optional, and the "template" is generated based on your specific needs. After countless projects and many years on this tech, we have lots of opinions and insights. We’ve done our best to encode them into this CLI. This is NOT an all-inclusive template. We expect you to bring your own libraries that solve the needs of YOUR application. While we don’t want to prescribe solutions to more specific problems like state management and deployment, we do have some recommendations listed here. T3 Axioms We'll be frank - this is an opinionated project. We share a handful of core beliefs around building and we treat them as the basis for our decisions. 1. Solve Problems It's easy to fall into the trap of "adding everything" - we explicitly don't want to do that. Everything added to create-t3-app should solve a specific problem that exists within the core technologies included. This means we won't add things like state libraries (zustand, redux) but we will add things like NextAuth.js and integrate Prisma and tRPC for you. 2. Bleed Responsibly We love our bleeding edge tech. The amount of speed and, honestly, fun that comes out of new shit is really cool. We think it's important to bleed responsibly, using riskier tech in the less risky parts. This means we wouldn't ⛔️ bet on risky new database tech (SQL is great!). But we happily ✅ bet on tRPC since it's just functions that are trivial to move off. 3. Typesafety Isn't Optional The stated goal of create-t3-app is to provide the quickest way to start a new full-stack, typesafe web application. We take typesafety seriously in these parts as it improves our productivity and helps us ship fewer bugs. Any decision that compromises the typesafe nature of create-t3-app is a decision that should be made in a different project. Getting Started To scaffold an app using create-t3-app, run any of the following four commands and answer the command prompt questions: npm npm create t3-app@latest yarn yarn create t3-app pnpm pnpm create t3-app@latest bun bunx create-t3-app@latest For more advanced usage, check out the CLI docs. Community For help, discussion about best practices, or any other conversation that would benefit create-t3-app: Join the T3 Discord Server Contributors We 💖 contributors! Feel free to contribute to this project but please read the Contributing Guidelines before opening an issue or PR so you understand the branching strategy and local development environment. We also welcome you to join our Discord community for either support or contributing guidance.         Made with contrib.rocks         </readme><commitcount>1,142</commitcount><languages>TypeScript 61.1%	Astro 28.5%	CSS 6.9%	JavaScript 3.5%</languages><tags>cli	typescript	nextjs	hacktoberfest	prisma	npx	tailwindcss	t3	trpc	next-auth	t3-stack	create-t3-app</tags><about>The best way to start a full-stack, typesafe Next.js app</about><starcount>20.6k</starcount><watchcount>0</watchcount></repository><repository><username>mkkellogg</username><reponame>GaussianSplats3D</reponame><readme>            3D Gaussian splatting for Three.js Building Usage Controls      README.md     3D Gaussian splatting for Three.js This repository contains a Three.js-based implementation of 3D Gaussian Splatting for Real-Time Radiance Field Rendering, a technique for the real-time visualization of real-world 3D scenes. Their project was CUDA-based and I wanted to build a viewer that was accessible via the web. When I started, web-based viewers were already available -- A WebGL-based viewer from antimatter15 and a WebGPU viewer from cvlab-epfl -- However no Three.js version existed. I used those versions as a starting point for my initial implementation, but as of now this project contains all my own code. Highlights:  Organized into ES modules Rendering is done entirely through Three.js The sorting algorithm is a C++ counting sort contained in a WASM module. Rasterization code is documented to describe 2D covariance computations as well as computations of corresponding eigen-values and eigen-vectors Scene is partitioned via octree that is used to cull non-visible splats prior to sorting Splat data (position, covariance, color) is stored via textures so that only splat indexes are transferred between host and GPU Allows a Three.js scene or object group to be rendered along with the splats  Online demo: https://projects.markkellogg.org/threejs/demo_gaussian_splats_3d.php This is still very much a work in progress! There are several things that still need to be done:  Improve the method by which splat data is stored in textures (currently much texture space is wasted or packed inefficiently) Properly incorporate spherical harmonics data to achieve view dependent lighting effects Improve the layout of the SplatBuffer object for better efficiency and reduced file size Improve splat sorting -- maybe an incremental sort of some kind? Implement double buffering so that the next splat index array in the main thread can be filled while the current one is sorted in the worker thread  Building Navigate to the code directory and run npm install  Followed by npm run build  To view the demo scenes locally run npm run demo  The demo will be accessible locally at http://127.0.0.1:8080/index.html. You will need to download the data for the demo scenes and extract them into &lt;code directory&gt;/build/demo/assets/data  The demo scene data is available here: https://projects.markkellogg.org/downloads/gaussian_splat_data.zip Usage To run the built-in viewer: const viewer = new GaussianSplat3D.Viewer({   'cameraUp': [0, -1, -0.6],   'initialCameraPosition': [-1, -4, 6],   'initialCameraLookAt': [0, 4, -0] }); viewer.init(); viewer.loadFile('&lt;path to .ply or .splat file&gt;') .then(() =&gt; {     viewer.start(); }); The loadFile() method will accept the original .ply files as well as my custom .splat files. To convert a .ply file into the stripped-down .splat format (currently only compatible with this viewer): const plyLoader = new GaussianSplat3D.PlyLoader(); plyLoader.loadFromFile('&lt;path to .ply file&gt;') .then((splatBuffer) =&gt; {     new GaussianSplat3D.SplatLoader(splatBuffer).saveToFile('converted_file.splat'); }); This code will prompt your browser to automatically start downloading the converted .splat file. It is now possible to integrate your own Three.js scene into the viewer (still somewhat experimental). The Viewer class now accepts a scene parameter by which you can pass in any 'normal' Three.js objects you want to be rendered along with the splats. Rendering the splats correctly with external objects requires a special sequence of steps so the viewer needs to be aware of them: const scene = new THREE.Scene();  const boxColor = 0xBBBBBB; const boxGeometry = new THREE.BoxGeometry(2, 2, 2); const boxMesh = new THREE.Mesh(boxGeometry, new THREE.MeshBasicMaterial({'color': boxColor})); scene.add(boxMesh); boxMesh.position.set(3, 2, 2);  const viewer = new GaussianSplat3D.Viewer({   'scene': scene,   'cameraUp': [0, -1, -0.6],   'initialCameraPosition': [-1, -4, 6],   'initialCameraLookAt': [0, 4, -0] }); viewer.init(); viewer.loadFile('&lt;path to .ply or .splat file&gt;') .then(() =&gt; {     viewer.start(); }); The viewer allows for various levels of customization via constructor parameters. You can control when its update() and render() methods are called by passing false for the selfDrivenMode parameter and then calling those methods whenever/wherever you decide is appropriate. You can tell the viewer to not use its built-in camera controls by passing false for the useBuiltInControls parameter. You can also use your own Three.js renderer and camera by passing those values to the viewer's constructor. The sample below shows all of these options: const renderWidth = 800; const renderHeight = 600;  const rootElement = document.createElement('div'); rootElement.style.width = renderWidth + 'px'; rootElement.style.height = renderHeight + 'px'; document.body.appendChild(rootElement);  const renderer = new THREE.WebGLRenderer({     antialias: false }); renderer.setSize(renderWidth, renderHeight); rootElement.appendChild(renderer.domElement);  const camera = new THREE.PerspectiveCamera(65, renderWidth / renderHeight, 0.1, 500); camera.position.copy(new THREE.Vector3().fromArray([-1, -4, 6])); camera.lookAt(new THREE.Vector3().fromArray([0, 4, -0])); camera.up = new THREE.Vector3().fromArray([0, -1, -0.6]).normalize();  const viewer = new GaussianSplat3D.Viewer({     'selfDrivenMode': false,     'renderer': renderer,     'camera': camera,     'useBuiltInControls': false }); viewer.init(); viewer.loadFile('&lt;path to .ply or .splat file&gt;') .then(() =&gt; {     requestAnimationFrame(update); }); Since selfDrivenMode is false, it is up to the developer to call the update() and render() methods on the Viewer class: function update() {     requestAnimationFrame(update);     viewer.update();     viewer.render(); } Controls Mouse  Left click and drag to orbit around the focal point Right click and drag to pan the camera and focal point  Keyboard   C Toggles the mesh cursor, which shows where a ray projected from the mouse cursor intersects the splat mesh   I Toggles an info panel that displays the mesh cursor position, current FPS, and current window size     </readme><commitcount>142</commitcount><languages>JavaScript 98.3%	C++ 1.6%	Shell 0.1%</languages><tags>webgl	threejs	three-js	gaussian-splatting</tags><about>Three.js-based implementation of the 3D Gaussian splat viewer</about><starcount>236</starcount><watchcount>0</watchcount></repository><repository><username>Orange-Cyberdefense</username><reponame>GOAD</reponame><readme>               Description Licenses Available labs Requirements tldr; quick install Installation Check before install Install Provisioning WriteUp Troubleshoot Road Map Lab organization Special Thanks to Socials Links Note      README.md         Description GOAD is a pentest active directory LAB project. The purpose of this lab is to give pentesters a vulnerable Active directory environment ready to use to practice usual attack techniques. Warning This lab is extremely vulnerable, do not reuse recipe to build your environment and do not deploy this environment on internet without isolation (this is a recommendation, use it as your own risk). This repository was build for pentest practice. Licenses This lab use free windows VM only (180 days). After that delay enter a license on each server or rebuild all the lab (may be it's time for an update ;)) Available labs  GOAD : 5 vms, 2 forests, 3 domains (full goad lab)      GOAD-Light : 3 vms, 1 forest, 2 domains (smaller goad lab for those with a smaller pc)     Requirements   Used space  The lab takes about 77GB (but you have to get the space for the vms vagrant images windows server 2016 (22GB) / windows server 2019 (14GB) / ubuntu 18.04 (502M)) The total space needed for the lab is ~115 GB (and more if you take snapshots)    Linux operating system  The lab intend to be installed from a Linux host and was tested only on this. Some people have successfully installed the lab from a windows OS, to do that they create the VMs with vagrant and have done the ansible provisioning part from a linux machine. In this case the linux machine used to do the provisioning must be setup with one adapter on NAT and one adapter on the same virtual private network as the lab.    tldr; quick install  You are on linux, you already got virtualbox, vagrant and docker installed on your host and you know what you are doing, just run :  ./goad.sh -t check -l GOAD -p virtualbox -m docker ./goad.sh -t install -l GOAD -p virtualbox  -m docker  Now you can grab a coffee ☕ it will take time :)  Installation   Installation depend of the provider you use, please follow the appropriate guide :  Install with Virtualbox Install with VmWare Install with Proxmox Install with Azure    Installation is in three parts :  Templating : this will create the template to use (needed only for proxmox) Providing : this will instantiate the virtual machines depending on your provider Provisioning : it is always made with ansible, it will install all the stuff to create the lab    Check before install  For linux users check dependencies installation before install :  ./goad -t check -l &lt;LAB&gt; -p &lt;PROVIDER&gt; -m &lt;ANSIBLE_RUN_METHOD&gt;    LAB: lab must be one of the following (folder in ad/)  GOAD GOAD-Light    PROVIDER : provider must be one of the following:  virtualbox vmware azure proxmox    ANSIBLE_RUN_METHOD : ansible method to use :  local : to use local ansible install docker : to use docker ansible install    Please install all the needed tools before run the install process   There is no automatic installer for the dependencies tools (virtualbox, vagrant, python, ansible,... ) you will have to install them by yourself depending on your package manager an linux system.   Install  Launch all the install (vagrant or terraform) vms creation followed by ansible provisioning :  ./goad -t install -l &lt;LAB&gt; -p &lt;PROVIDER&gt; -m &lt;ANSIBLE_RUN_METHOD&gt;   The goad install will run all the ansible playbook one by one with a failover to restart the ansible playbook if something goes wrong (sometimes vms or playbook hit timeout so this will restart the playbook automatically)  Provisioning  The provisioning is always done with ansible, more detail on the ansible provisioning here : Ansible provisioning  WriteUp  All the writeups of the Game Of Active Directory lab are available on this blog : mayfly blog  Troubleshoot  see troubleshoot  Road Map   Password reuse between computer (PTH)  Spray User = Password  Password in description  SMB share anonymous  SMB not signed  Responder  Zerologon  Windows defender  ASREPRoast  Kerberoasting  AD Acl abuse  Unconstraint delegation  Ntlm relay  Constrained delegation  Install MSSQL  MSSQL trusted link  MSSQL impersonate  Install IIS  Upload asp app  Multiples forest  Anonymous RPC user listing  Child parent domain  Generate certificate and enable ldaps  ADCS - ESC 1/2/3/8  Certifry  Samaccountname/nopac  Petitpotam unauthent  Printerbug  Drop the mic  Shadow credentials  Mitm6  Add LAPS  GPO abuse  Add Webdav  Add RDP bot  Add full proxmox integration  Add Gmsa (receipe created)  Add azure support  Refactoring lab and providers  Add PPL  Add Credential Guard  Add Applocker  Zone transfert  Wsus  Sccm  Exchange  Lab organization  The lab configuration is located on the ad/ folder Each Ad folder correspond to a lab and contains the following files :  ad/   labname/            # The lab name must be the same as the variable : domain_name from the data/inventory     data/       config.json     # The json file containing all the variables and configuration of the lab       inventory       # The global lab inventory (provider independent) (this should no contains variables)     files/            # This folder contains files you want to copy on your vms     scripts/          # This folder contains ps1 scripts you want to play on your vm (Must be added in the "scripts" entries of your vms)     providers/        # Your lab available provider       vmware/         inventory     # specific vmware inventory         Vagrantfile   # specific vmware vagrantfile       virtualbox/         inventory     # specific virtualbox inventory         Vagrantfile   # specific virtualbox vagrantfile       proxmox/         terraform/    # specific proxmox terraform recipe         inventory     # specific proxmox inventory       azure/         terraform/    # specific azure terraform recipe         inventory     # specific azure inventory  Special Thanks to  Julien Arrault (Azure recipes) Thomas Rollain (tests &amp; some vulns writing) Quentin Galliou (tests)  Socials   Links  https://unicornsec.com/home/siem-home-lab-series-part-1 https://github.com/jckhmr/adlab https://www.jonathanmedd.net/2019/09/ansible-windows-and-powershell-the-basics-introduction.html https://www.secframe.com/badblood/ https://josehelps.com/blog/2019-08-06_building-a-windows-2016-dc/ https://medium.com/@vartaisecurity/lab-building-guide-virtual-active-directory-5f0d0c8eb907 https://www.ansible.com/blog/an-introduction-to-windows-security-with-ansible https://github.com/rgl/windows-domain-controller-vagrant https://www.sconstantinou.com/powershell-active-directory-delegation-part-1/ https://www.shellandco.net/playing-acl-active-directory-objects/ https://github.com/clong/DetectionLab https://www.ired.team/offensive-security-experiments/active-directory-kerberos-abuse/abusing-active-directory-acls-aces ...  Note  This repo is based on the work of jckhmr and kkolk    </readme><commitcount>153</commitcount><languages>PowerShell 73.8%	Shell 13.7%	HCL 7.9%	Jinja 3.8%	ASP.NET 0.5%	Dockerfile 0.1%</languages><tags /><about>game of active directory</about><starcount>3.1k</starcount><watchcount>0</watchcount></repository><repository><username>cloudflare</username><reponame>workers-sdk</reponame><readme>               ⛅️ Home to wrangler, the CLI for Cloudflare Workers®, as well as other tools for interacting with Workers Quick Start Create a Project Installation: Commands wrangler init [name] wrangler dev wrangler deploy Pages wrangler pages dev [directory] [-- command] Documentation Contributing      README.md     ⛅️ Home to wrangler, the CLI for Cloudflare Workers®, as well as other tools for interacting with Workers This monorepo contains:  wrangler-devtools Cloudflare's fork of Chrome DevTools for inspecting your local or remote Workers templates Templates &amp; examples for writing Cloudflare Workers wrangler A command line tool for building Cloudflare Workers. pages-shared Used internally to power Wrangler and Cloudflare Pages. It contains all the code that is shared between these clients. C3 A CLI for creating and deploying new applications to Cloudflare.  Wrangler and the workers-sdk is developed in the open on GitHub, and you can see what we're working on in GitHub Issues, as well as in our workers-sdk GitHub Project board. If you've found a bug or would like to request a feature, please file an issue! Quick Start # Make a javascript file echo "export default { fetch() { return new Response('hello world') } }" &gt; index.js # try it out npx wrangler dev index.js # and then deploy it npx wrangler deploy index.js --name my-worker --latest # visit https://my-worker.&lt;your workers subdomain&gt;.workers.dev Create a Project # Generate a new project npx wrangler init my-worker # try it out cd my-worker &amp;&amp; npm run start # and then deploy it npm run deploy Installation: $ npm install wrangler --save-dev Commands wrangler init [name] Creates a Worker project. For details on configuration keys and values, refer to the documentation. wrangler dev Start a local development server, with live reloading and devtools. wrangler deploy Deploys the given script to the worldwide Cloudflare network. For more commands and options, refer to the documentation. Pages wrangler pages dev [directory] [-- command] Either serves a static build asset directory, or proxies itself in front of a command. Builds and runs functions from a ./functions directory or uses a _worker.js file inside the static build asset directory. For more commands and options, refer to the documentation or run wrangler pages dev --help. Documentation For the latest Wrangler documentation, click here. Contributing Refer to the CONTRIBUTING.md guide for details.   </readme><commitcount>2,004</commitcount><languages>TypeScript 83.0%	JavaScript 11.7%	HTML 1.6%	Mustache 1.5%	CSS 1.0%	F# 0.5%</languages><tags>javascript	cli	serverless	wasm	cloudflare	cloudflare-workers</tags><about>⛅️ Home to Wrangler, the CLI for Cloudflare Workers®</about><starcount>1.8k</starcount><watchcount>0</watchcount></repository><repository><username>nodejs</username><reponame>node</reponame><readme>               Node.js Table of contents Support Release types Download Current and LTS releases Nightly releases API documentation Verifying binaries Building Node.js Security Contributing to Node.js Current project team members TSC (Technical Steering Committee) TSC voting members TSC regular members TSC emeriti members Collaborators Collaborator emeriti Triagers Release keys Security release stewards License      README.md     Node.js Node.js is an open-source, cross-platform JavaScript runtime environment. For information on using Node.js, see the Node.js website. The Node.js project uses an open governance model. The OpenJS Foundation provides support for the project. Contributors are expected to act in a collaborative manner to move the project forward. We encourage the constructive exchange of contrary opinions and compromise. The TSC reserves the right to limit or block contributors who repeatedly act in ways that discourage, exhaust, or otherwise negatively affect other participants. This project has a Code of Conduct. Table of contents  Support Release types  Download  Current and LTS releases Nightly releases API documentation   Verifying binaries   Building Node.js Security Contributing to Node.js Current project team members  TSC (Technical Steering Committee) Collaborators Triagers Release keys   License  Support Looking for help? Check out the instructions for getting support. Release types  Current: Under active development. Code for the Current release is in the branch for its major version number (for example, v19.x). Node.js releases a new major version every 6 months, allowing for breaking changes. This happens in April and October every year. Releases appearing each October have a support life of 8 months. Releases appearing each April convert to LTS (see below) each October. LTS: Releases that receive Long Term Support, with a focus on stability and security. Every even-numbered major version will become an LTS release. LTS releases receive 12 months of Active LTS support and a further 18 months of Maintenance. LTS release lines have alphabetically-ordered code names, beginning with v4 Argon. There are no breaking changes or feature additions, except in some special circumstances. Nightly: Code from the Current branch built every 24-hours when there are changes. Use with caution.  Current and LTS releases follow semantic versioning. A member of the Release Team signs each Current and LTS release. For more information, see the Release README. Download Binaries, installers, and source tarballs are available at https://nodejs.org/en/download/. Current and LTS releases https://nodejs.org/download/release/ The latest directory is an alias for the latest Current release. The latest-codename directory is an alias for the latest release from an LTS line. For example, the latest-hydrogen directory contains the latest Hydrogen (Node.js 18) release. Nightly releases https://nodejs.org/download/nightly/ Each directory name and filename contains a date (in UTC) and the commit SHA at the HEAD of the release. API documentation Documentation for the latest Current release is at https://nodejs.org/api/. Version-specific documentation is available in each release directory in the docs subdirectory. Version-specific documentation is also at https://nodejs.org/download/docs/. Verifying binaries Download directories contain a SHASUMS256.txt file with SHA checksums for the files. To download SHASUMS256.txt using curl: curl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt To check that a downloaded file matches the checksum, run it through sha256sum with a command such as: grep node-vx.y.z.tar.gz SHASUMS256.txt | sha256sum -c - For Current and LTS, the GPG detached signature of SHASUMS256.txt is in SHASUMS256.txt.sig. You can use it with gpg to verify the integrity of SHASUMS256.txt. You will first need to import the GPG keys of individuals authorized to create releases. To import the keys: gpg --keyserver hkps://keys.openpgp.org --recv-keys 4ED778F539E3634C779C87C6D7062848A1AB005C See Release keys for a script to import active release keys. Next, download the SHASUMS256.txt.sig for the release: curl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt.sig Then use gpg --verify SHASUMS256.txt.sig SHASUMS256.txt to verify the file's signature. Building Node.js See BUILDING.md for instructions on how to build Node.js from source and a list of supported platforms. Security For information on reporting security vulnerabilities in Node.js, see SECURITY.md. Contributing to Node.js  Contributing to the project Working Groups Strategic initiatives Technical values and prioritization  Current project team members For information about the governance of the Node.js project, see GOVERNANCE.md. TSC (Technical Steering Committee) TSC voting members  aduh95 - Antoine du Hamel &lt;duhamelantoine1995@gmail.com&gt; (he/him) anonrig - Yagiz Nizipli &lt;yagiz@nizipli.com&gt; (he/him) apapirovski - Anatoli Papirovski &lt;apapirovski@mac.com&gt; (he/him) benjamingr - Benjamin Gruenbaum &lt;benjamingr@gmail.com&gt; BridgeAR - Ruben Bridgewater &lt;ruben@bridgewater.de&gt; (he/him) cjihrig - Colin Ihrig &lt;cjihrig@gmail.com&gt; (he/him) GeoffreyBooth - Geoffrey Booth &lt;webadmin@geoffreybooth.com&gt; (he/him) gireeshpunathil - Gireesh Punathil &lt;gpunathi@in.ibm.com&gt; (he/him) jasnell - James M Snell &lt;jasnell@gmail.com&gt; (he/him) joyeecheung - Joyee Cheung &lt;joyeec9h3@gmail.com&gt; (she/her) legendecas - Chengzhong Wu &lt;legendecas@gmail.com&gt; (he/him) mcollina - Matteo Collina &lt;matteo.collina@gmail.com&gt; (he/him) mhdawson - Michael Dawson &lt;midawson@redhat.com&gt; (he/him) MoLow - Moshe Atlow &lt;moshe@atlow.co.il&gt; (he/him) RafaelGSS - Rafael Gonzaga &lt;rafael.nunu@hotmail.com&gt; (he/him) RaisinTen - Darshan Sen &lt;raisinten@gmail.com&gt; (he/him) richardlau - Richard Lau &lt;rlau@redhat.com&gt; ronag - Robert Nagy &lt;ronagy@icloud.com&gt; ruyadorno - Ruy Adorno &lt;ruyadorno@google.com&gt; (he/him) targos - Michaël Zasso &lt;targos@protonmail.com&gt; (he/him) tniessen - Tobias Nießen &lt;tniessen@tnie.de&gt; (he/him)  TSC regular members  BethGriggs - Beth Griggs &lt;bethanyngriggs@gmail.com&gt; (she/her) bnoordhuis - Ben Noordhuis &lt;info@bnoordhuis.nl&gt; ChALkeR - Сковорода Никита Андреевич &lt;chalkerx@gmail.com&gt; (he/him) codebytere - Shelley Vohr &lt;shelley.vohr@gmail.com&gt; (she/her) danbev - Daniel Bevenius &lt;daniel.bevenius@gmail.com&gt; (he/him) danielleadams - Danielle Adams &lt;adamzdanielle@gmail.com&gt; (she/her) fhinkel - Franziska Hinkelmann &lt;franziska.hinkelmann@gmail.com&gt; (she/her) gabrielschulhof - Gabriel Schulhof &lt;gabrielschulhof@gmail.com&gt; mscdex - Brian White &lt;mscdex@mscdex.net&gt; MylesBorins - Myles Borins &lt;myles.borins@gmail.com&gt; (he/him) rvagg - Rod Vagg &lt;r@va.gg&gt; TimothyGu - Tiancheng "Timothy" Gu &lt;timothygu99@gmail.com&gt; (he/him) Trott - Rich Trott &lt;rtrott@gmail.com&gt; (he/him)   TSC emeriti members TSC emeriti members  addaleax - Anna Henningsen &lt;anna@addaleax.net&gt; (she/her) chrisdickinson - Chris Dickinson &lt;christopher.s.dickinson@gmail.com&gt; evanlucas - Evan Lucas &lt;evanlucas@me.com&gt; (he/him) Fishrock123 - Jeremiah Senkpiel &lt;fishrock123@rocketmail.com&gt; (he/they) gibfahn - Gibson Fahnestock &lt;gibfahn@gmail.com&gt; (he/him) indutny - Fedor Indutny &lt;fedor@indutny.com&gt; isaacs - Isaac Z. Schlueter &lt;i@izs.me&gt; joshgav - Josh Gavant &lt;josh.gavant@outlook.com&gt; mmarchini - Mary Marchini &lt;oss@mmarchini.me&gt; (she/her) nebrius - Bryan Hughes &lt;bryan@nebri.us&gt; ofrobots - Ali Ijaz Sheikh &lt;ofrobots@google.com&gt; (he/him) orangemocha - Alexis Campailla &lt;orangemocha@nodejs.org&gt; piscisaureus - Bert Belder &lt;bertbelder@gmail.com&gt; sam-github - Sam Roberts &lt;vieuxtech@gmail.com&gt; shigeki - Shigeki Ohtsu &lt;ohtsu@ohtsu.org&gt; (he/him) thefourtheye - Sakthipriyan Vairamani &lt;thechargingvolcano@gmail.com&gt; (he/him) trevnorris - Trevor Norris &lt;trev.norris@gmail.com&gt;   Collaborators  addaleax - Anna Henningsen &lt;anna@addaleax.net&gt; (she/her) aduh95 - Antoine du Hamel &lt;duhamelantoine1995@gmail.com&gt; (he/him) anonrig - Yagiz Nizipli &lt;yagiz@nizipli.com&gt; (he/him) antsmartian - Anto Aravinth &lt;anto.aravinth.cse@gmail.com&gt; (he/him) apapirovski - Anatoli Papirovski &lt;apapirovski@mac.com&gt; (he/him) AshCripps - Ash Cripps &lt;email@ashleycripps.co.uk&gt; atlowChemi - Chemi Atlow &lt;chemi@atlow.co.il&gt; (he/him) Ayase-252 - Qingyu Deng &lt;i@ayase-lab.com&gt; bengl - Bryan English &lt;bryan@bryanenglish.com&gt; (he/him) benjamingr - Benjamin Gruenbaum &lt;benjamingr@gmail.com&gt; BethGriggs - Beth Griggs &lt;bethanyngriggs@gmail.com&gt; (she/her) bmeck - Bradley Farias &lt;bradley.meck@gmail.com&gt; bnb - Tierney Cyren &lt;hello@bnb.im&gt; (they/them) bnoordhuis - Ben Noordhuis &lt;info@bnoordhuis.nl&gt; BridgeAR - Ruben Bridgewater &lt;ruben@bridgewater.de&gt; (he/him) cclauss - Christian Clauss &lt;cclauss@me.com&gt; (he/him) ChALkeR - Сковорода Никита Андреевич &lt;chalkerx@gmail.com&gt; (he/him) cjihrig - Colin Ihrig &lt;cjihrig@gmail.com&gt; (he/him) codebytere - Shelley Vohr &lt;shelley.vohr@gmail.com&gt; (she/her) cola119 - Kohei Ueno &lt;kohei.ueno119@gmail.com&gt; (he/him) daeyeon - Daeyeon Jeong &lt;daeyeon.dev@gmail.com&gt; (he/him) danbev - Daniel Bevenius &lt;daniel.bevenius@gmail.com&gt; (he/him) danielleadams - Danielle Adams &lt;adamzdanielle@gmail.com&gt; (she/her) debadree25 - Debadree Chatterjee &lt;debadree333@gmail.com&gt; (he/him) deokjinkim - Deokjin Kim &lt;deokjin81.kim@gmail.com&gt; (he/him) devnexen - David Carlier &lt;devnexen@gmail.com&gt; devsnek - Gus Caplan &lt;me@gus.host&gt; (they/them) edsadr - Adrian Estrada &lt;edsadr@gmail.com&gt; (he/him) erickwendel - Erick Wendel &lt;erick.workspace@gmail.com&gt; (he/him) fhinkel - Franziska Hinkelmann &lt;franziska.hinkelmann@gmail.com&gt; (she/her) F3n67u - Feng Yu &lt;F3n67u@outlook.com&gt; (he/him) Flarna - Gerhard Stöbich &lt;deb2001-github@yahoo.de&gt;  (he/they) gabrielschulhof - Gabriel Schulhof &lt;gabrielschulhof@gmail.com&gt; gengjiawen - Jiawen Geng &lt;technicalcute@gmail.com&gt; GeoffreyBooth - Geoffrey Booth &lt;webadmin@geoffreybooth.com&gt; (he/him) gireeshpunathil - Gireesh Punathil &lt;gpunathi@in.ibm.com&gt; (he/him) guybedford - Guy Bedford &lt;guybedford@gmail.com&gt; (he/him) H4ad - Vinícius Lourenço Claro Cardoso &lt;contact@viniciusl.com.br&gt; (he/him) HarshithaKP - Harshitha K P &lt;harshitha014@gmail.com&gt; (she/her) himself65 - Zeyu "Alex" Yang &lt;himself65@outlook.com&gt; (he/him) iansu - Ian Sutherland &lt;ian@iansutherland.ca&gt; JacksonTian - Jackson Tian &lt;shyvo1987@gmail.com&gt; JakobJingleheimer - Jacob Smith &lt;jacob@frende.me&gt; (he/him) jasnell - James M Snell &lt;jasnell@gmail.com&gt; (he/him) jkrems - Jan Krems &lt;jan.krems@gmail.com&gt; (he/him) joesepi - Joe Sepi &lt;sepi@joesepi.com&gt; (he/him) joyeecheung - Joyee Cheung &lt;joyeec9h3@gmail.com&gt; (she/her) juanarbol - Juan José Arboleda &lt;soyjuanarbol@gmail.com&gt; (he/him) JungMinu - Minwoo Jung &lt;nodecorelab@gmail.com&gt; (he/him) KhafraDev - Matthew Aitken &lt;maitken033380023@gmail.com&gt; (he/him) kuriyosh - Yoshiki Kurihara &lt;yosyos0306@gmail.com&gt; (he/him) kvakil - Keyhan Vakil &lt;kvakil@sylph.kvakil.me&gt; legendecas - Chengzhong Wu &lt;legendecas@gmail.com&gt; (he/him) linkgoron - Nitzan Uziely &lt;linkgoron@gmail.com&gt; LiviaMedeiros - LiviaMedeiros &lt;livia@cirno.name&gt; lpinca - Luigi Pinca &lt;luigipinca@gmail.com&gt; (he/him) lukekarrys - Luke Karrys &lt;luke@lukekarrys.com&gt; (he/him) Lxxyx - Zijian Liu &lt;lxxyxzj@gmail.com&gt; (he/him) marco-ippolito - Marco Ippolito &lt;marcoippolito54@gmail.com&gt; (he/him) marsonya - Akhil Marsonya &lt;akhil.marsonya27@gmail.com&gt; (he/him) mcollina - Matteo Collina &lt;matteo.collina@gmail.com&gt; (he/him) meixg - Xuguang Mei &lt;meixuguang@gmail.com&gt; (he/him) Mesteery - Mestery &lt;mestery@protonmail.com&gt; (he/him) mhdawson - Michael Dawson &lt;midawson@redhat.com&gt; (he/him) miladfarca - Milad Fa &lt;mfarazma@redhat.com&gt; (he/him) mildsunrise - Alba Mendez &lt;me@alba.sh&gt; (she/her) MoLow - Moshe Atlow &lt;moshe@atlow.co.il&gt; (he/him) mscdex - Brian White &lt;mscdex@mscdex.net&gt; MylesBorins - Myles Borins &lt;myles.borins@gmail.com&gt; (he/him) ovflowd - Claudio Wunder &lt;cwunder@gnome.org&gt; (he/they) oyyd - Ouyang Yadong &lt;oyydoibh@gmail.com&gt; (he/him) panva - Filip Skokan &lt;panva.ip@gmail.com&gt; (he/him) Qard - Stephen Belanger &lt;admin@stephenbelanger.com&gt; (he/him) RafaelGSS - Rafael Gonzaga &lt;rafael.nunu@hotmail.com&gt; (he/him) RaisinTen - Darshan Sen &lt;raisinten@gmail.com&gt; (he/him) rluvaton - Raz Luvaton &lt;rluvaton@gmail.com&gt; (he/him) richardlau - Richard Lau &lt;rlau@redhat.com&gt; rickyes - Ricky Zhou &lt;0x19951125@gmail.com&gt; (he/him) ronag - Robert Nagy &lt;ronagy@icloud.com&gt; ruyadorno - Ruy Adorno &lt;ruyadorno@google.com&gt; (he/him) rvagg - Rod Vagg &lt;rod@vagg.org&gt; ryzokuken - Ujjwal Sharma &lt;ryzokuken@disroot.org&gt; (he/him) santigimeno - Santiago Gimeno &lt;santiago.gimeno@gmail.com&gt; shisama - Masashi Hirano &lt;shisama07@gmail.com&gt; (he/him) ShogunPanda - Paolo Insogna &lt;paolo@cowtech.it&gt; (he/him) srl295 - Steven R Loomis &lt;srl295@gmail.com&gt; sxa - Stewart X Addison &lt;sxa@redhat.com&gt; (he/him) targos - Michaël Zasso &lt;targos@protonmail.com&gt; (he/him) theanarkh - theanarkh &lt;theratliter@gmail.com&gt; (he/him) TimothyGu - Tiancheng "Timothy" Gu &lt;timothygu99@gmail.com&gt; (he/him) tniessen - Tobias Nießen &lt;tniessen@tnie.de&gt; (he/him) trivikr - Trivikram Kamat &lt;trivikr.dev@gmail.com&gt; Trott - Rich Trott &lt;rtrott@gmail.com&gt; (he/him) vdeturckheim - Vladimir de Turckheim &lt;vlad2t@hotmail.com&gt; (he/him) vmoroz - Vladimir Morozov &lt;vmorozov@microsoft.com&gt; (he/him) VoltrexKeyva - Mohammed Keyvanzadeh &lt;mohammadkeyvanzade94@gmail.com&gt; (he/him) watilde - Daijiro Wachi &lt;daijiro.wachi@gmail.com&gt; (he/him) XadillaX - Khaidi Chu &lt;i@2333.moe&gt; (he/him) yashLadha - Yash Ladha &lt;yash@yashladha.in&gt; (he/him) ZYSzys - Yongsheng Zhang &lt;zyszys98@gmail.com&gt; (he/him)   Emeriti Collaborator emeriti  ak239 - Aleksei Koziatinskii &lt;ak239spb@gmail.com&gt; andrasq - Andras &lt;andras@kinvey.com&gt; AnnaMag - Anna M. Kedzierska &lt;anna.m.kedzierska@gmail.com&gt; AndreasMadsen - Andreas Madsen &lt;amwebdk@gmail.com&gt; (he/him) aqrln - Alexey Orlenko &lt;eaglexrlnk@gmail.com&gt; (he/him) bcoe - Ben Coe &lt;bencoe@gmail.com&gt; (he/him) bmeurer - Benedikt Meurer &lt;benedikt.meurer@gmail.com&gt; boneskull - Christopher Hiller &lt;boneskull@boneskull.com&gt; (he/him) brendanashworth - Brendan Ashworth &lt;brendan.ashworth@me.com&gt; bzoz - Bartosz Sosnowski &lt;bartosz@janeasystems.com&gt; calvinmetcalf - Calvin Metcalf &lt;calvin.metcalf@gmail.com&gt; chrisdickinson - Chris Dickinson &lt;christopher.s.dickinson@gmail.com&gt; claudiorodriguez - Claudio Rodriguez &lt;cjrodr@yahoo.com&gt; DavidCai1993 - David Cai &lt;davidcai1993@yahoo.com&gt; (he/him) davisjam - Jamie Davis &lt;davisjam@vt.edu&gt; (he/him) digitalinfinity - Hitesh Kanwathirtha &lt;digitalinfinity@gmail.com&gt; (he/him) dmabupt - Xu Meng &lt;dmabupt@gmail.com&gt; (he/him) dnlup dnlup &lt;dnlup.dev@gmail.com&gt; eljefedelrodeodeljefe - Robert Jefe Lindstaedt &lt;robert.lindstaedt@gmail.com&gt; estliberitas - Alexander Makarenko &lt;estliberitas@gmail.com&gt; eugeneo - Eugene Ostroukhov &lt;eostroukhov@google.com&gt; evanlucas - Evan Lucas &lt;evanlucas@me.com&gt; (he/him) firedfox - Daniel Wang &lt;wangyang0123@gmail.com&gt; Fishrock123 - Jeremiah Senkpiel &lt;fishrock123@rocketmail.com&gt; (he/they) gdams - George Adams &lt;gadams@microsoft.com&gt; (he/him) geek - Wyatt Preul &lt;wpreul@gmail.com&gt; gibfahn - Gibson Fahnestock &lt;gibfahn@gmail.com&gt; (he/him) glentiki - Glen Keane &lt;glenkeane.94@gmail.com&gt; (he/him) hashseed - Yang Guo &lt;yangguo@chromium.org&gt; (he/him) hiroppy - Yuta Hiroto &lt;hello@hiroppy.me&gt; (he/him) iarna - Rebecca Turner &lt;me@re-becca.org&gt; imran-iq - Imran Iqbal &lt;imran@imraniqbal.org&gt; imyller - Ilkka Myller &lt;ilkka.myller@nodefield.com&gt; indutny - Fedor Indutny &lt;fedor@indutny.com&gt; isaacs - Isaac Z. Schlueter &lt;i@izs.me&gt; italoacasas - Italo A. Casas &lt;me@italoacasas.com&gt; (he/him) jasongin - Jason Ginchereau &lt;jasongin@microsoft.com&gt; jbergstroem - Johan Bergström &lt;bugs@bergstroem.nu&gt; jdalton - John-David Dalton &lt;john.david.dalton@gmail.com&gt; jhamhader - Yuval Brik &lt;yuval@brik.org.il&gt; joaocgreis - João Reis &lt;reis@janeasystems.com&gt; joshgav - Josh Gavant &lt;josh.gavant@outlook.com&gt; julianduque - Julian Duque &lt;julianduquej@gmail.com&gt; (he/him) kfarnung - Kyle Farnung &lt;kfarnung@microsoft.com&gt; (he/him) kunalspathak - Kunal Pathak &lt;kunal.pathak@microsoft.com&gt; lance - Lance Ball &lt;lball@redhat.com&gt; (he/him) Leko - Shingo Inoue &lt;leko.noor@gmail.com&gt; (he/him) lucamaraschi - Luca Maraschi &lt;luca.maraschi@gmail.com&gt; (he/him) lundibundi - Denys Otrishko &lt;shishugi@gmail.com&gt; (he/him) lxe - Aleksey Smolenchuk &lt;lxe@lxe.co&gt; maclover7 - Jon Moss &lt;me@jonathanmoss.me&gt; (he/him) mafintosh - Mathias Buus &lt;mathiasbuus@gmail.com&gt; (he/him) matthewloring - Matthew Loring &lt;mattloring@google.com&gt; micnic - Nicu Micleușanu &lt;micnic90@gmail.com&gt; (he/him) mikeal - Mikeal Rogers &lt;mikeal.rogers@gmail.com&gt; misterdjules - Julien Gilli &lt;jgilli@netflix.com&gt; mmarchini - Mary Marchini &lt;oss@mmarchini.me&gt; (she/her) monsanto - Christopher Monsanto &lt;chris@monsan.to&gt; MoonBall - Chen Gang &lt;gangc.cxy@foxmail.com&gt; not-an-aardvark - Teddy Katz &lt;teddy.katz@gmail.com&gt; (he/him) ofrobots - Ali Ijaz Sheikh &lt;ofrobots@google.com&gt; (he/him) Olegas - Oleg Elifantiev &lt;oleg@elifantiev.ru&gt; orangemocha - Alexis Campailla &lt;orangemocha@nodejs.org&gt; othiym23 - Forrest L Norvell &lt;ogd@aoaioxxysz.net&gt; (they/them/themself) petkaantonov - Petka Antonov &lt;petka_antonov@hotmail.com&gt; phillipj - Phillip Johnsen &lt;johphi@gmail.com&gt; piscisaureus - Bert Belder &lt;bertbelder@gmail.com&gt; pmq20 - Minqi Pan &lt;pmq2001@gmail.com&gt; PoojaDurgad - Pooja D P &lt;Pooja.D.P@ibm.com&gt; (she/her) princejwesley - Prince John Wesley &lt;princejohnwesley@gmail.com&gt; psmarshall - Peter Marshall &lt;petermarshall@chromium.org&gt; (he/him) puzpuzpuz - Andrey Pechkurov &lt;apechkurov@gmail.com&gt; (he/him) refack - Refael Ackermann (רפאל פלחי) &lt;refack@gmail.com&gt; (he/him/הוא/אתה) rexagod - Pranshu Srivastava &lt;rexagod@gmail.com&gt; (he/him) rlidwka - Alex Kocharin &lt;alex@kocharin.ru&gt; rmg - Ryan Graham &lt;r.m.graham@gmail.com&gt; robertkowalski - Robert Kowalski &lt;rok@kowalski.gd&gt; romankl - Roman Klauke &lt;romaaan.git@gmail.com&gt; ronkorving - Ron Korving &lt;ron@ronkorving.nl&gt; RReverser - Ingvar Stepanyan &lt;me@rreverser.com&gt; rubys - Sam Ruby &lt;rubys@intertwingly.net&gt; saghul - Saúl Ibarra Corretgé &lt;s@saghul.net&gt; sam-github - Sam Roberts &lt;vieuxtech@gmail.com&gt; sebdeckers - Sebastiaan Deckers &lt;sebdeckers83@gmail.com&gt; seishun - Nikolai Vavilov &lt;vvnicholas@gmail.com&gt; shigeki - Shigeki Ohtsu &lt;ohtsu@ohtsu.org&gt; (he/him) silverwind - Roman Reiss &lt;me@silverwind.io&gt; starkwang - Weijia Wang &lt;starkwang@126.com&gt; stefanmb - Stefan Budeanu &lt;stefan@budeanu.com&gt; tellnes - Christian Tellnes &lt;christian@tellnes.no&gt; thefourtheye - Sakthipriyan Vairamani &lt;thechargingvolcano@gmail.com&gt; (he/him) thlorenz - Thorsten Lorenz &lt;thlorenz@gmx.de&gt; trevnorris - Trevor Norris &lt;trev.norris@gmail.com&gt; tunniclm - Mike Tunnicliffe &lt;m.j.tunnicliffe@gmail.com&gt; vkurchatkin - Vladimir Kurchatkin &lt;vladimir.kurchatkin@gmail.com&gt; vsemozhetbyt - Vse Mozhet Byt &lt;vsemozhetbyt@gmail.com&gt; (he/him) watson - Thomas Watson &lt;w@tson.dk&gt; whitlockjc - Jeremy Whitlock &lt;jwhitlock@apache.org&gt; yhwang - Yihong Wang &lt;yh.wang@ibm.com&gt; yorkie - Yorkie Liu &lt;yorkiefixer@gmail.com&gt; yosuke-furukawa - Yosuke Furukawa &lt;yosuke.furukawa@gmail.com&gt;   Collaborators follow the Collaborator Guide in maintaining the Node.js project. Triagers  atlowChemi - Chemi Atlow &lt;chemi@atlow.co.il&gt; (he/him) Ayase-252 - Qingyu Deng &lt;i@ayase-lab.com&gt; bmuenzenmeyer - Brian Muenzenmeyer &lt;brian.muenzenmeyer@gmail.com&gt; (he/him) daeyeon - Daeyeon Jeong &lt;daeyeon.dev@gmail.com&gt; (he/him) F3n67u - Feng Yu &lt;F3n67u@outlook.com&gt; (he/him) himadriganguly - Himadri Ganguly &lt;himadri.tech@gmail.com&gt; (he/him) iam-frankqiu - Frank Qiu &lt;iam.frankqiu@gmail.com&gt; (he/him) marsonya - Akhil Marsonya &lt;akhil.marsonya27@gmail.com&gt; (he/him) meixg - Xuguang Mei &lt;meixuguang@gmail.com&gt; (he/him) mertcanaltin - Mert Can Altin &lt;mertgold60@gmail.com&gt; Mesteery - Mestery &lt;mestery@protonmail.com&gt; (he/him) preveen-stack - Preveen Padmanabhan &lt;wide4head@gmail.com&gt; (he/him) PoojaDurgad - Pooja Durgad &lt;Pooja.D.P@ibm.com&gt; RaisinTen - Darshan Sen &lt;raisinten@gmail.com&gt; VoltrexKeyva - Mohammed Keyvanzadeh &lt;mohammadkeyvanzade94@gmail.com&gt; (he/him)  Triagers follow the Triage Guide when responding to new issues. Release keys Primary GPG keys for Node.js Releasers (some Releasers sign with subkeys):  Beth Griggs &lt;bethanyngriggs@gmail.com&gt; 4ED778F539E3634C779C87C6D7062848A1AB005C Bryan English &lt;bryan@bryanenglish.com&gt; 141F07595B7B3FFE74309A937405533BE57C7D57 Danielle Adams &lt;adamzdanielle@gmail.com&gt; 74F12602B6F1C4E913FAA37AD3A89613643B6201 Juan José Arboleda &lt;soyjuanarbol@gmail.com&gt; DD792F5973C6DE52C432CBDAC77ABFA00DDBF2B7 Michaël Zasso &lt;targos@protonmail.com&gt; 8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600 Myles Borins &lt;myles.borins@gmail.com&gt; C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8 RafaelGSS &lt;rafael.nunu@hotmail.com&gt; 890C08DB8579162FEE0DF9DB8BEAB4DFCF555EF4 Richard Lau &lt;rlau@redhat.com&gt; C82FA3AE1CBEDC6BE46B9360C43CEC45C17AB93C Ruy Adorno &lt;ruyadorno@hotmail.com&gt; 108F52B48DB57BB0CC439B2997B01419BD92F80A Ulises Gascón &lt;ulisesgascongonzalez@gmail.com&gt; A363A499291CBBC940DD62E41F10027AF002F8B0  To import the full set of trusted release keys (including subkeys possibly used to sign releases): gpg --keyserver hkps://keys.openpgp.org --recv-keys 4ED778F539E3634C779C87C6D7062848A1AB005C gpg --keyserver hkps://keys.openpgp.org --recv-keys 141F07595B7B3FFE74309A937405533BE57C7D57 gpg --keyserver hkps://keys.openpgp.org --recv-keys 74F12602B6F1C4E913FAA37AD3A89613643B6201 gpg --keyserver hkps://keys.openpgp.org --recv-keys DD792F5973C6DE52C432CBDAC77ABFA00DDBF2B7 gpg --keyserver hkps://keys.openpgp.org --recv-keys 8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600 gpg --keyserver hkps://keys.openpgp.org --recv-keys C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8 gpg --keyserver hkps://keys.openpgp.org --recv-keys 890C08DB8579162FEE0DF9DB8BEAB4DFCF555EF4 gpg --keyserver hkps://keys.openpgp.org --recv-keys C82FA3AE1CBEDC6BE46B9360C43CEC45C17AB93C gpg --keyserver hkps://keys.openpgp.org --recv-keys 108F52B48DB57BB0CC439B2997B01419BD92F80A gpg --keyserver hkps://keys.openpgp.org --recv-keys A363A499291CBBC940DD62E41F10027AF002F8B0 See Verifying binaries for how to use these keys to verify a downloaded file.  Other keys used to sign some previous releases  Chris Dickinson &lt;christopher.s.dickinson@gmail.com&gt; 9554F04D7259F04124DE6B476D5A82AC7E37093B Colin Ihrig &lt;cjihrig@gmail.com&gt; 94AE36675C464D64BAFA68DD7434390BDBE9B9C5 Danielle Adams &lt;adamzdanielle@gmail.com&gt; 1C050899334244A8AF75E53792EF661D867B9DFA Evan Lucas &lt;evanlucas@me.com&gt; B9AE9905FFD7803F25714661B63B535A4C206CA9 Gibson Fahnestock &lt;gibfahn@gmail.com&gt; 77984A986EBC2AA786BC0F66B01FBB92821C587A Isaac Z. Schlueter &lt;i@izs.me&gt; 93C7E9E91B49E432C2F75674B0A78B0A6C481CF6 Italo A. Casas &lt;me@italoacasas.com&gt; 56730D5401028683275BD23C23EFEFE93C4CFFFE James M Snell &lt;jasnell@keybase.io&gt; 71DCFD284A79C3B38668286BC97EC7A07EDE3FC1 Jeremiah Senkpiel &lt;fishrock@keybase.io&gt; FD3A5288F042B6850C66B31F09FE44734EB7990E Juan José Arboleda &lt;soyjuanarbol@gmail.com&gt; 61FC681DFB92A079F1685E77973F295594EC4689 Julien Gilli &lt;jgilli@fastmail.fm&gt; 114F43EE0176B71C7BC219DD50A3051F888C628D Rod Vagg &lt;rod@vagg.org&gt; DD8F2338BAE7501E3DD5AC78C273792F7D83545D Ruben Bridgewater &lt;ruben@bridgewater.de&gt; A48C2BEE680E841632CD4E44F07496B3EB3C1762 Shelley Vohr &lt;shelley.vohr@gmail.com&gt; B9E2F5981AA6E0CD28160D9FF13993A75599653C Timothy J Fontaine &lt;tjfontaine@gmail.com&gt; 7937DFD2AB06298B2293C3187D33FF9D0246406D   Security release stewards When possible, the commitment to take slots in the security release steward rotation is made by companies in order to ensure individuals who act as security stewards have the support and recognition from their employer to be able to prioritize security releases. Security release stewards manage security releases on a rotation basis as outlined in the security release process.  Datadog  bengl - Bryan English &lt;bryan@bryanenglish.com&gt; (he/him)   NearForm  RafaelGSS - Rafael Gonzaga &lt;rafael.nunu@hotmail.com&gt; (he/him)   NodeSource  juanarbol - Juan José Arboleda &lt;soyjuanarbol@gmail.com&gt; (he/him)   Platformatic  mcollina - Matteo Collina &lt;matteo.collina@gmail.com&gt; (he/him)   Red Hat and IBM  joesepi - Joe Sepi &lt;joesepi@ibm.com&gt; (he/him) mhdawson - Michael Dawson &lt;midawson@redhat.com&gt; (he/him)    License Node.js is available under the MIT license. Node.js also includes external libraries that are available under a variety of licenses.  See LICENSE for the full license text.   </readme><commitcount>39,986</commitcount><languages>JavaScript 62.5%	C++ 22.0%	Python 10.8%	C 2.9%	HTML 0.7%	Shell 0.5%</languages><tags>nodejs	javascript	windows	macos	linux	node	mit	js	runtime</tags><about>Node.js JavaScript runtime ✨🐢🚀✨</about><starcount>98.8k</starcount><watchcount>0</watchcount></repository><repository><username>Vectorized</username><reponame>solady</reponame><readme>               Installation Contracts Directories Contributing Safety Upgradability EVM Compatibility Acknowledgements      README.md         Gas optimized Solidity snippets. I'm sooooooOooooooooOoooOoooooooooooooooo... Installation To install with Foundry: forge install vectorized/solady To install with Hardhat or Truffle: npm install solady Contracts The Solidity smart contracts are located in the src directory. accounts ├─ Receiver — "Receiver mixin for ETH and safe-transferred ERC721 and ERC1155 tokens" ├─ ERC4337 — "Simple ERC4337 account implementation" ├─ ERC4337Factory — "Simple ERC4337 account factory implementation" ├─ ERC6551 — "Simple ERC6551 account implementation" ├─ ERC6551Proxy — "Simple ERC6551 account proxy implementation" auth ├─ Ownable — "Simple single owner authorization mixin" ├─ OwnableRoles — "Simple single owner and multiroles authorization mixin" tokens ├─ WETH — "Simple Wrapped Ether implementation" ├─ ERC20 — "Simple ERC20 + EIP-2612 implementation" ├─ ERC4626 — "Simple ERC4626 tokenized Vault implementation" ├─ ERC721 — "Simple ERC721 implementation with storage hitchhiking" ├─ ERC2981 — "Simple ERC2981 NFT Royalty Standard implementation" ├─ ERC1155 — "Simple ERC1155 implementation" ├─ ERC6909 — "Simple EIP-6909 minimal multi-token implementation" utils ├─ MerkleProofLib — "Library for verification of Merkle proofs" ├─ SignatureCheckerLib — "Library for verification of ECDSA and ERC1271 signatures" ├─ ECDSA — "Library for verification of ECDSA signatures" ├─ EIP712 — "Contract for EIP-712 typed structured data hashing and signing" ├─ ERC1967Factory — "Factory for deploying and managing ERC1967 proxy contracts" ├─ ERC1967FactoryConstants — "The address and bytecode of the canonical ERC1967Factory" ├─ JSONParserLib — "Library for parsing JSONs" ├─ LibSort — "Library for efficient sorting of memory arrays" ├─ LibPRNG — "Library for generating pseudorandom numbers" ├─ Base64 — "Library for Base64 encoding and decoding" ├─ SSTORE2 — "Library for cheaper reads and writes to persistent storage" ├─ CREATE3 — "Deploy to deterministic addresses without an initcode factor" ├─ LibRLP — "Library for computing contract addresses from their deployer and nonce" ├─ LibBit — "Library for bit twiddling and boolean operations" ├─ LibZip — "Library for compressing and decompressing bytes" ├─ Clone — "Class with helper read functions for clone with immutable args" ├─ LibClone — "Minimal proxy library" ├─ UUPSUpgradeable — "UUPS proxy mixin" ├─ LibString — "Library for converting numbers into strings and other string operations" ├─ LibBitmap — "Library for storage of packed booleans" ├─ LibMap — "Library for storage of packed unsigned integers" ├─ MinHeapLib — "Library for managing a min-heap in storage" ├─ RedBlackTreeLib — "Library for managing a red-black-tree in storage" ├─ Multicallable — "Contract that enables a single call to call multiple methods on itself" ├─ SafeTransferLib — "Safe ERC20/ETH transfer lib that handles missing return values" ├─ DynamicBufferLib — "Library for buffers with automatic capacity resizing" ├─ MetadataReaderLib — "Library for reading contract metadata robustly" ├─ FixedPointMathLib — "Arithmetic library with operations for fixed-point numbers" ├─ SafeCastLib — "Library for integer casting that reverts on overflow" └─ DateTimeLib — "Library for date time operations" Directories src — "Solidity smart contracts" test — "Foundry Forge tests" js — "Accompanying JavaScript helper library" ext — "Extra tests" audits — "Audit reports" Contributing This repository serves as a laboratory for cutting edge snippets that may be merged into Solmate. Feel free to make a pull request. Do refer to the contribution guidelines for more details. Safety This is experimental software and is provided on an "as is" and "as available" basis. We do not give any warranties and will not be liable for any loss incurred through any use of this codebase. While Solady has been heavily tested, there may be parts that may exhibit unexpected emergent behavior when used with other code, or may break in future Solidity versions. Please always include your own thorough tests when using Solady to make sure it works correctly with your code. Upgradability Most contracts in Solady are compatible with both upgradeable and non-upgradeable (i.e. regular) contracts. Please call any required internal initialization methods accordingly. EVM Compatibility Some parts of Solady may not be compatible with chains with partial EVM equivalence. Please always check and test for compatibility accordingly. Acknowledgements This repository is inspired by or directly modified from many sources, primarily:  Solmate OpenZeppelin ERC721A Zolidity 🐍 Snekmate Femplate    </readme><commitcount>761</commitcount><languages>Solidity 94.0%	Python 5.1%	JavaScript 0.9%</languages><tags /><about>Optimized Solidity snippets.</about><starcount>1.7k</starcount><watchcount>0</watchcount></repository><repository><username>aras-p</username><reponame>UnityGaussianSplatting</reponame><readme>            Gaussian Splatting playground in Unity Usage Write-ups Performance numbers: License and External Code Used      readme.md     Gaussian Splatting playground in Unity SIGGRAPH 2023 had a paper "3D Gaussian Splatting for Real-Time Radiance Field Rendering" by Kerbl, Kopanas, Leimkühler, Drettakis that looks pretty cool! Check out their website, source code repository, data sets and so on. I've decided to try to implement the realtime visualization part (i.e. the one that takes already-produced gaussian splat "model" file) in Unity.  The original paper code has a purely CUDA-based realtime renderer; other people have done their own implementations (e.g. WebGPU at cvlab-epfl, Taichi at wanmeihuali, etc.). Code in here so far is randomly cribbled together from reading the paper (as well as earlier literature on EWA splatting), looking at the official CUDA implementation, and so on. Current state:  The code does not use the "tile-based splat rasterizer" bit from the paper; it just draws each gaussian splat as an oriented quad that covers the extents of it. Splat color accumulation is done by rendering front-to-back, with a blending mode that results in the same accumulated color as their tile-based renderer. Splat sorting is done with a AMD FidelityFX derived radix sort.  Usage ⚠️ Note: this is all a toy that I'm just playing around with. If you file bugs or feature requests, I will most likely just ignore them and do whatever I please. I told you so! ⚠️ Download or clone this repository and open projects/GaussianExample as a Unity project (I use Unity 2022.3, other versions might also work). Note that the project requires DX12 or Vulkan on Windows, i.e. DX11 will not work. This is not tested at all on mobile/web, and probably does not work there.  Next up, create some GaussianSplat assets: open Tools -&gt; Gaussian Splats -&gt; Create GaussianSplatAsset menu within Unity. In the dialog, point Input PLY File to your Gaussian Splat file (note that it has to be a gaussian splat PLY file, not some other PLY file. E.g. in the official paper models, the correct files are under point_cloud/iteration_*/point_cloud.ply). Optionally there can be cameras.json next to it or somewhere in parent folders. Pick desired compression options and output folder, and press "Create Asset" button. The compression even at "very low" quality setting is decently usable, e.g. this capture at Very Low preset is under 8MB of total size (click to see the video):   If everything was fine, there should be a GaussianSplat asset that has several data files next to it. Since the gaussian splat models are quite large, I have not included any in this Github repo. The original paper github page has a a link to 14GB zip of their models. In the game object that has a GaussianSplatRenderer script, point the Asset field to one of your created assets. There are various controls on the script to debug/visualize the data, as well as a slider to move game camera into one of asset's camera locations. The rendering takes game object transformation matrix into account; the official gaussian splat models seem to be all rotated by about -160 degrees around X axis, and mirrored around Z axis, so in the sample scene the object has such a transform set up. Additional documentation:  Render Pipeline Integration Editing Splats  That's it! Future plans, roadmap and a todo list does not really exist, I'll just do whatever I randomly decide. I keep some of the open ideas as github issues. Write-ups My own blog posts about all this:  Gaussian Splatting is pretty cool! (2023 Sep 5) Making Gaussian Splats smaller (2023 Sep 13) Making Gaussian Splats more smaller (2023 Sep 27)  Performance numbers: "bicycle" scene from the paper, with 6.1M splats and first camera in there, rendering at 1200x797 resolution, at "Medium" asset quality level (282MB asset file):  Windows (NVIDIA RTX 3080 Ti):  Official SBIR viewer: 7.4ms (135FPS). 4.8GB VRAM usage. Unity, DX12 or Vulkan: 8.1ms (123FPS) - 4.55ms rendering, 2.37ms sorting, 0.78ms splat view calc. 1.3GB VRAM usage.   Mac (Apple M1 Max):  Unity, Metal: 23.6ms (42FPS).    Besides the gaussian splat asset that is loaded into GPU memory, currently this also needs about 48 bytes of GPU memory per splat (for sorting, caching view dependent data etc.). License and External Code Used The code I wrote for this is under MIT license. The project also uses several 3rd party libraries:  zanders3/json, MIT license, (c) 2018 Alex Parker. "Ffx" GPU sorting code is based on AMD FidelityFX ParallelSort, MIT license, (c) 2020-2021 Advanced Micro Devices, Inc. Ported to Unity by me.  However, keep in mind that the license of the original paper implementation says that the official training software for the Gaussian Splats is for educational / academic / non-commercial purpose; commercial usage requires getting license from INRIA. That is: even if this viewer / integration into Unity is just "MIT license", you need to separately consider how did you get your Gaussian Splat PLY files.   </readme><commitcount>233</commitcount><languages>C# 80.4%	HLSL 16.6%	ShaderLab 3.0%</languages><tags>unity	gaussian-splatting</tags><about>Toy Gaussian Splatting visualization in Unity</about><starcount>1k</starcount><watchcount>0</watchcount></repository><repository><username>protolambda</username><reponame>eth2-testnet-genesis</reponame><readme>               Eth2 Testnet Genesis State Creator Process Overview Outcome Usage Subcommands: Common Inputs: Outputs: Example Usage: Extra Details: Validators List License      README.md     Eth2 Testnet Genesis State Creator This repository contains a utility for generating an Eth2 testnet genesis state, eliminating the need to make deposits for all validators. Process Overview  Deploy a deposit contract. Select an Eth1 block hash and corresponding timestamp as the starting point. The deposit contract contents must be empty at this block. Set the minimum genesis time. This usually determines the first eligible Eth1 block starting point. Ensure the selected Eth1 block is compatible (i.e., has an equal or later timestamp) with the minimum genesis time. If a future compared to the min. genesis time is chosen, the generated state will be invalid. Adjust the actual genesis time by configuring the genesis delay (actual genesis time = chosen Eth1 time + delay).  Outcome  The deposit data tree at genesis will be empty, rather than filled with the genesis validators. The expected empty deposit tree root is 0xd70a234731285c6804c2a4f56711ddb8c82c99740f207854891028af34e27e5e (32th order zero hash, with zero length-mixin). The validators will be pre-filled in the genesis state. The state will have an existing Eth1 block, an empty deposit tree, and a zero deposit count, from where deposits can continue. The state will have a zero deposit index, meaning block proposers won't have to search for non-existing deposits of the pre-filled validators. The deposit contract will be considered empty by Eth2 nodes at genesis. Any subsequent deposits will be appended to the validator set, as expected.  Usage Subcommands:  phase0: Create genesis state for phase0 beacon chain. altair: Create genesis state for Altair beacon chain. bellatrix: Create genesis state for Bellatrix beacon chain, from execution-layer (only required if post-transition) and consensus-layer configs. capella: Create genesis state for Capella beacon chain, from execution-layer (only required if post-transition) and consensus-layer configs. version: Print version and exit.  Common Inputs:  Eth1 config: A genesis.json, as specified in Geth. Eth2 config: A standard YAML file, as specified in Eth2.0 specs. Concatenation of configs of all relevant phases. Mnemonics: The mnemonics.yaml is formatted as shown below. It specifies the amount of validators for each mnemonic. Validators List: Alternatively, a file with a list of validators can be specified with the --additional-validators flag.  Outputs:  genesis.ssz: A state to start the network with. tranches: A directory with text files for each mnemonic, listing all pubkeys (1 per line). Useful for checking if keystores are generated correctly before genesis, and for tracking the validators.  Example Usage:  For bellatrix genesis state:  eth2-testnet-genesis bellatrix --config=config.yaml --mnemonics=mnemonics.yaml --eth1-config=genesis.json  For capella genesis state:  eth2-testnet-genesis capella --config=config.yaml --mnemonics=mnemonics.yaml --eth1-config=genesis.json  For capella genesis state with additional validators specified :  eth2-testnet-genesis capella --config=config.yaml --mnemonics=mnemonics.yaml --eth1-config=genesis.json --validators ./validators.yaml  For capella genesis state for a shadowfork:  eth2-testnet-genesis capella --config=config.yaml --eth1-config="genesis.json" --mnemonics=mnemonics.yaml --shadow-fork-eth1-rpc=http://localhost:8545 Extra Details:  To get additional information such as fork digest, genesis validators root, etc., run the compute_genesis_details.py script. Install dependencies with pip install milagro-bls-binding==1.6.3 eth2spec==1.1.0a7 (use of a venv recommended). An alternate approach to get this information is to use zcli. E.g: zcli pretty &lt;bellatrix/capella&gt;  BeaconState genesis.ssz &gt; parsedState.json If you want to fetch the EL block to embed in the genesis state from a live node, you can run the tool with the flag --shadow-fork-eth1-rpc=http://&lt;EL-JSON-RPC-URL&gt;  Validators List In addition, or as alternative to the mnemonic-based validator generation a file with a list of validators can be specified with the --additional-validators flag. The list of validators is formatted as: # &lt;validator pubkey&gt;:&lt;withdrawal credentials&gt;[:&lt;balance&gt;] 0x9824e447621e4b3bca7794b91c664cc0b43322a70b1881b2f804e3a990a3965a64bfe7f098cb4c0396cd0c89218de0b4:001547805ff0547da9e51a7463a6a0c603eeda01dd930f7016185f0642b9ecaf:32000000000 0xace5689384f87725790499fb5261b586d7dfb7d86058f0a909856272ba02df9929dcdb4b1ea529b02b948b3a1dca4d57:008aa7b9c37bf27e7c49a3185a3e721c7a02c94da7a0b6ad5f88f1b0477d3b88:32000000000 # ... more  # balance is optional and defaults to 32000000000: 0xa33dfc09b4031e8c520469024c0ef419cc148f71d7b9501f58f2e54fc644462f208119791e57c5c9b33bf5e47f705060:00b84654c946dc68b353384426a29a3c5d736d9f751c192d5038206e93f79d73  # individual 0x01 credentials can be set: 0x82fc9f31d6e768c57d09483e788b24444235f64d2cae5f2f8a9dd28b6e8ed6636a5f378febc762cfcd9f8ab808286608:010000000000000000000000CcCCccccCCCCcCCCCCCcCcCccCcCCCcCcccccccC 0xb744b5466a214762ee17621dc4c75d1bba16417e20755f7c9c2485ea518580be50d2c87d70cc4ac393158eb34311c9a2:010000000000000000000000000000000000000000000000000000000000dEaD  A validators list can be generated separately via: export MNEMONIC="your mnemonic" eth2-val-tools deposit-data --fork-version 0x00000000 --source-max 200 --source-min 0 --validators-mnemonic="$MNEMONIC" --withdrawals-mnemonic="$MNEMONIC" --as-json-list | jq ".[] | \"0x\" + .pubkey + \":\" + .withdrawal_credentials + \":32000000000\"" | tr -d '"' &gt; validators.txt  License This project is licensed under the MIT License. See the LICENSE file for details.   </readme><commitcount>68</commitcount><languages>Go 93.8%	Python 5.5%	Makefile 0.7%</languages><tags /><about>Create a genesis state for an Eth2 testnet</about><starcount>34</starcount><watchcount>0</watchcount></repository><repository><username>slam</username><reponame>yabaictl</reponame><readme>    README.md     yabaictl A wrapper around yabai for better dual-monitor support. This is a rewrite of the python version in rust. It is an excuse to learn rust. The wrapper idea originally came from aiguofer/dotfiles. This requires yabai 4.0.2+. In 4.0.2, the yabai client/server message format has changed. yabaictl only supports the new format.   </readme><commitcount>36</commitcount><languages>Rust 100.0%</languages><tags /><about>A wrapper around yabai for better dual-monitor support</about><starcount>8</starcount><watchcount>0</watchcount></repository><repository><username>slam</username><reponame>HongKongKeyboard</reponame><readme>            Hong Kong Keyboard Why? How does it work? How do I install it?      README.md     Hong Kong Keyboard This is a native iOS keyboard for Hong Kong Cantonese. It is an iOS frontend to the excellent Google Input Tools service. The project is built on KeyboardKit. I would not have taken this project on without it. Why? Google Input Tools is extremely good at Cantonese input. It picks the Cantonese phrases intelligently based on context. It is a joy to use. Unfortunately, the service is only available on Android, Chrome, or Windows. The iOS Gboard does support Cantonese input, but it is not in the same league as Google Input Tools. For example, when entering the phrase "neihoma" in Gboard, it suggests "你可馬", instead of what Google Input Tools would suggest correctly, which is "你好嗎". I really wanted to be able to use Google Input Tools on my iOS device. So I wrote my own iOS frontend for it.  How does it work? Hong Kong Keyboard makes a network call to Google Input Tools on every keystroke. It sends the English characters you have typed so far, and the Cantonese sentence you have typed in up to this point as context. The service returns a list of suggestions, you pick from the suggestions, and rinse and repeat. The response from Google Input Tools is pretty fast. If the network is slow, you will observe delayed response. It is unavoidable. How do I install it? Sorry, this is not available on the Apple AppStore. It is too much hassle to go through the review process. It is also unclear if the app could pass the review process. To install it, build it in XCode and run "HongKongKeyboardApp" on your device. XCode 12 works fine.   </readme><commitcount>13</commitcount><languages>Swift 100.0%</languages><tags /><about>Hong Kong keyboard for iOS using Google Input Tools</about><starcount>2</starcount><watchcount>0</watchcount></repository><repository><username>Fuseit</username><reponame>sunspot</reponame><readme>               Sunspot Looking for maintainers Quickstart with Rails 3 / 4 Setting Up Objects Searching Objects Search In Depth Full Text Phrases Phrase Boosts Scoping (Scalar Fields) Positive Restrictions Negative Restrictions Empty Restrictions Disjunctions and Conjunctions Combined with Full-Text Pagination Cursor-based pagination Faceting Field Facets Query Facets Range Facets Ordering Grouping Grouping by Queries Geospatial Filter By Radius Filter By Radius (inexact with bbox) Filter By Bounding Box Sort By Distance Joins If your models have fields with the same name Highlighting Stats Stats on multiple fields Faceting on stats Multiple stats and selective faceting Functions Atomic updates More Like This Spellcheck Indexes In Depth Index-Time Boosts Stored Fields Hits vs. Results Reindexing Objects Use Without Rails Threading Manually Adjusting Solr Parameters Session Proxies Type Reference Configuration Development Running Tests sunspot sunspot_rails Generating Documentation Tutorials and Articles License      README.md     Sunspot   Sunspot is a Ruby library for expressive, powerful interaction with the Solr search engine. Sunspot is built on top of the RSolr library, which provides a low-level interface for Solr interaction; Sunspot provides a simple, intuitive, expressive DSL backed by powerful features for indexing objects and searching for them. Sunspot is designed to be easily plugged in to any ORM, or even non-database-backed objects such as the filesystem. This README provides a high level overview; class-by-class and method-by-method documentation is available in the API reference. For questions about how to use Sunspot in your app, please use the Sunspot Mailing List or search Stack Overflow. Looking for maintainers This project is looking for maintainers. An ideal candidate would be someone on a team whose app makes heavy use of the Sunspot gem. If you think you're a good fit, send a message to contact@culturecode.ca. Quickstart with Rails 3 / 4 Add to Gemfile: gem 'sunspot_rails' gem 'sunspot_solr' # optional pre-packaged Solr distribution for use in development Bundle it! bundle install Generate a default configuration file: rails generate sunspot_rails:install If sunspot_solr was installed, start the packaged Solr distribution with: bundle exec rake sunspot:solr:start # or sunspot:solr:run to start in foreground Setting Up Objects Add a searchable block to the objects you wish to index. class Post &lt; ActiveRecord::Base   searchable do     text :title, :body     text :comments do       comments.map { |comment| comment.body }     end      boolean :featured     integer :blog_id     integer :author_id     integer :category_ids, :multiple =&gt; true     double  :average_rating     time    :published_at     time    :expired_at      string  :sort_title do       title.downcase.gsub(/^(an?|the)/, '')     end   end end text fields will be full-text searchable. Other fields (e.g., integer and string) can be used to scope queries. Searching Objects Post.search do   fulltext 'best pizza'    with :blog_id, 1   with(:published_at).less_than Time.now   order_by :published_at, :desc   paginate :page =&gt; 2, :per_page =&gt; 15   facet :category_ids, :author_id end Search In Depth Given an object Post setup in earlier steps ... Full Text # All posts with a `text` field (:title, :body, or :comments) containing 'pizza' Post.search { fulltext 'pizza' }  # Posts with pizza, scored higher if pizza appears in the title Post.search do   fulltext 'pizza' do     boost_fields :title =&gt; 2.0   end end  # Posts with pizza, scored higher if featured Post.search do   fulltext 'pizza' do     boost(2.0) { with(:featured, true) }   end end  # Posts with pizza *only* in the title Post.search do   fulltext 'pizza' do     fields(:title)   end end  # Posts with pizza in the title (boosted) or in the body (not boosted) Post.search do   fulltext 'pizza' do     fields(:body, :title =&gt; 2.0)   end end Phrases Solr allows searching for phrases: search terms that are close together. In the default query parser used by Sunspot (edismax), phrase searches are represented as a double quoted group of words. # Posts with the exact phrase "great pizza" Post.search do   fulltext '"great pizza"' end If specified, query_phrase_slop sets the number of words that may appear between the words in a phrase. # One word can appear between the words in the phrase, so "great big pizza" # also matches, in addition to "great pizza" Post.search do   fulltext '"great pizza"' do     query_phrase_slop 1   end end Phrase Boosts Phrase boosts add boost to terms that appear in close proximity; the terms do not have to appear in a phrase, but if they do, the document will score more highly. # Matches documents with great and pizza, and scores documents more # highly if the terms appear in a phrase in the title field Post.search do   fulltext 'great pizza' do     phrase_fields :title =&gt; 2.0   end end  # Matches documents with great and pizza, and scores documents more # highly if the terms appear in a phrase (or with one word between them) # in the title field Post.search do   fulltext 'great pizza' do     phrase_fields :title =&gt; 2.0     phrase_slop   1   end end Scoping (Scalar Fields) Fields not defined as text (e.g., integer, boolean, time, etc...) can be used to scope (restrict) queries before full-text matching is performed. Positive Restrictions # Posts with a blog_id of 1 Post.search do   with(:blog_id, 1) end  # Posts with an average rating between 3.0 and 5.0 Post.search do   with(:average_rating, 3.0..5.0) end  # Posts with a category of 1, 3, or 5 Post.search do   with(:category_ids, [1, 3, 5]) end  # Posts published since a week ago Post.search do   with(:published_at).greater_than(1.week.ago) end Negative Restrictions # Posts not in category 1 or 3 Post.search do   without(:category_ids, [1, 3]) end  # All examples in "positive" also work negated using `without` Empty Restrictions # Passing an empty array is equivalent to a no-op, allowing you to replace this... Post.search do   with(:category_ids, id_list) if id_list.present? end  # ...with this Post.search do   with(:category_ids, id_list) end Disjunctions and Conjunctions # Posts that do not have an expired time or have not yet expired Post.search do   any_of do     with(:expired_at).greater_than(Time.now)     with(:expired_at, nil)   end end # Posts with blog_id 1 and author_id 2 Post.search do   all_of do     with(:blog_id, 1)     with(:author_id, 2)   end end # Posts scoring with any of the two fields. Post.search do   any do     fulltext "keyword1", :fields =&gt; :title     fulltext "keyword2", :fields =&gt; :body   end end Disjunctions and conjunctions may be nested Post.search do   any_of do     with(:blog_id, 1)     all_of do       with(:blog_id, 2)       with(:category_ids, 3)     end   end      any do     all do       fulltext "keyword", :fields =&gt; :title       fulltext "keyword", :fields =&gt; :body     end     all do       fulltext "keyword", :fields =&gt; :first_name       fulltext "keyword", :fields =&gt; :last_name     end     fulltext "keyword", :fields =&gt; :description   end end Combined with Full-Text Scopes/restrictions can be combined with full-text searching. The scope/restriction pares down the objects that are searched for the full-text term. # Posts with blog_id 1 and 'pizza' in the title Post.search do   with(:blog_id, 1)   fulltext("pizza") end Pagination All results from Solr are paginated The results array that is returned has methods mixed in that allow it to operate seamlessly with common pagination libraries like will_paginate and kaminari. By default, Sunspot requests the first 30 results from Solr. search = Post.search do   fulltext "pizza" end  # Imagine there are 60 *total* results (at 30 results/page, that is two pages) results = search.results # =&gt; Array with 30 Post elements  search.total           # =&gt; 60  results.total_pages    # =&gt; 2 results.first_page?    # =&gt; true results.last_page?     # =&gt; false results.previous_page  # =&gt; nil results.next_page      # =&gt; 2 results.out_of_bounds? # =&gt; false results.offset         # =&gt; 0 To retrieve the next page of results, recreate the search and use the paginate method. search = Post.search do   fulltext "pizza"   paginate :page =&gt; 2 end  # Again, imagine there are 60 total results; this is the second page results = search.results # =&gt; Array with 30 Post elements  search.total           # =&gt; 60  results.total_pages    # =&gt; 2 results.first_page?    # =&gt; false results.last_page?     # =&gt; true results.previous_page  # =&gt; 1 results.next_page      # =&gt; nil results.out_of_bounds? # =&gt; false results.offset         # =&gt; 30 A custom number of results per page can be specified with the :per_page option to paginate: search = Post.search do   fulltext "pizza"   paginate :page =&gt; 1, :per_page =&gt; 50 end Cursor-based pagination Solr 4.7 and above With default Solr pagination it may turn that same records appear on different pages (e.g. if many records have the same search score). Cursor-based pagination allows to avoid this. Useful for any kinds of export, infinite scroll, etc. Cursor for the first page is "*". search = Post.search do   fulltext "pizza"   paginate :cursor =&gt; "*" end  results = search.results  # Results will contain cursor for the next page results.next_page_cursor # =&gt; "AoIIP4AAACxQcm9maWxlIDEwMTk="  # Imagine there are 60 *total* results (at 30 results/page, that is two pages) results.current_cursor # =&gt; "*" results.total_pages    # =&gt; 2 results.first_page?    # =&gt; true results.last_page?     # =&gt; false To retrieve the next page of results, recreate the search and use the paginate method with cursor from previous results. search = Post.search do   fulltext "pizza"   paginate :cursor =&gt; "AoIIP4AAACxQcm9maWxlIDEwMTk=" end  results = search.results  # Again, imagine there are 60 total results; this is the second page results.next_page_cursor # =&gt; "AoEsUHJvZmlsZSAxNzY5" results.current_cursor   # =&gt; "AoIIP4AAACxQcm9maWxlIDEwMTk=" results.total_pages      # =&gt; 2 results.first_page?      # =&gt; false # Last page will be detected only when current page contains less then per_page elements or contains nothing results.last_page?       # =&gt; false :per_page option is also supported. Faceting Faceting is a feature of Solr that determines the number of documents that match a given search and an additional criterion. This allows you to build powerful drill-down interfaces for search. Each facet returns zero or more rows, each of which represents a particular criterion conjoined with the actual query being performed. For field facets, each row represents a particular value for a given field. For query facets, each row represents an arbitrary scope; the facet itself is just a means of logically grouping the scopes. By default Sunspot will only return the first 100 facet values.  You can increase this limit, or force it to return all facets by setting limit to -1. Field Facets # Posts that match 'pizza' returning counts for each :author_id search = Post.search do   fulltext "pizza"   facet :author_id end  search.facet(:author_id).rows.each do |facet|   puts "Author #{facet.value} has #{facet.count} pizza posts!" end If you are searching by a specific field and you still want to see all the options available in that field you can exclude it in the faceting. # Posts that match 'pizza' and author with id 42 # Returning counts for each :author_id (even those not in the search result) search = Post.search do   fulltext "pizza"   author_filter = with(:author_id, 42)   facet :author_id, exclude: [author_filter] end  search.facet(:author_id).rows.each do |facet|   puts "Author #{facet.value} has #{facet.count} pizza posts!" end Query Facets # Posts faceted by ranges of average ratings search = Post.search do   facet(:average_rating) do     row(1.0..2.0) do       with(:average_rating, 1.0..2.0)     end     row(2.0..3.0) do       with(:average_rating, 2.0..3.0)     end     row(3.0..4.0) do       with(:average_rating, 3.0..4.0)     end     row(4.0..5.0) do       with(:average_rating, 4.0..5.0)     end   end end  # e.g., # Number of posts with rating within 1.0..2.0: 2 # Number of posts with rating within 2.0..3.0: 1 search.facet(:average_rating).rows.each do |facet|   puts "Number of posts with rating within #{facet.value}: #{facet.count}" end Range Facets # Posts faceted by range of average ratings Sunspot.search(Post) do   facet :average_rating, :range =&gt; 1..5, :range_interval =&gt; 1 end Ordering By default, Sunspot orders results by "score": the Solr-determined relevancy metric. Sorting can be customized with the order_by method: # Order by average rating, descending Post.search do   fulltext("pizza")   order_by(:average_rating, :desc) end  # Order by relevancy score and in the case of a tie, average rating Post.search do   fulltext("pizza")    order_by(:score, :desc)   order_by(:average_rating, :desc) end  # Randomized ordering Post.search do   fulltext("pizza")   order_by(:random) end Solr 3.1 and above Solr supports sorting on multiple fields using custom functions. Supported operators and more details are available on the Solr Wiki To sort results by a custom function use the order_by_function method. Functions are defined with prefix notation: # Order by sum of two example fields: rating1 + rating2 Post.search do   fulltext("pizza")   order_by_function(:sum, :rating1, :rating2, :desc) end  # Order by nested functions: rating1 + (rating2*rating3) Post.search do   fulltext("pizza")   order_by_function(:sum, :rating1, [:product, :rating2, :rating3], :desc) end  # Order by fields and constants: rating1 + (rating2 * 5) Post.search do   fulltext("pizza")   order_by_function(:sum, :rating1, [:product, :rating2, '5'], :desc) end  # Order by average of three fields: (rating1 + rating2 + rating3) / 3 Post.search do   fulltext("pizza")   order_by_function(:div, [:sum, :rating1, :rating2, :rating3], '3', :desc) end Grouping Solr 3.3 and above Solr supports grouping documents, similar to an SQL GROUP BY. More information about result grouping/field collapsing is available on the Solr Wiki. Grouping is only supported on string fields that are not multivalued. To group on a field of a different type (e.g., integer), add a denormalized string type class Post &lt; ActiveRecord::Base   searchable do     # Denormalized `string` field because grouping can only be performed     # on string fields     string(:blog_id_str) { |p| p.blog_id.to_s }   end end  # Returns only the top scoring document per blog_id search = Post.search do   group :blog_id_str end  search.group(:blog_id_str).matches # Total number of matches to the query  search.group(:blog_id_str).groups.each do |group|   puts group.value # blog_id of the each document in the group    # By default, there is only one document per group (the highest   # scoring one); if `limit` is specified (see below), multiple   # documents can be returned per group   group.results.each do |result|     # ...   end end Additional options are supported by the DSL: # Returns the top 3 scoring documents per blog_id Post.search do   group :blog_id_str do     limit 3   end end  # Returns document ordered within each group by published_at (by # default, the ordering is score) Post.search do   group :blog_id_str do     order_by(:average_rating, :desc)   end end  # Facet count is based on the most relevant document of each group # matching the query (&gt;= Solr 3.4) Post.search do   group :blog_id_str do     truncate   end    facet :blog_id_str, :extra =&gt; :any end Grouping by Queries It is also possible to group by arbitrary queries instead of on a specific field, much like using query facets instead of field facets. For example, we can group by average rating. # Returns the top post for each range of average ratings search = Post.search do   group do     query("1.0 to 2.0") do       with(:average_rating, 1.0..2.0)     end     query("2.0 to 3.0") do       with(:average_rating, 2.0..3.0)     end     query("3.0 to 4.0") do       with(:average_rating, 3.0..4.0)     end     query("4.0 to 5.0") do       with(:average_rating, 4.0..5.0)     end   end end  search.group(:queries).matches # Total number of matches to the queries  search.group(:queries).groups.each do |group|   puts group.value # The argument to query - "1.0 to 2.0", for example    group.results.each do |result|     # ...   end end This can also be used to query multivalued fields, allowing a single item to be in multiple groups. # This finds the top 10 posts for each category in category_ids. search = Post.search do   group do     limit 10      category_ids.each do |category_id|       query category_id do         with(:category_id, category_id)       end     end   end end Geospatial Sunspot 2.0 only Sunspot 2.0 supports geospatial features of Solr 3.1 and above. Geospatial features require a field defined with latlon: class Post &lt; ActiveRecord::Base   searchable do     # ...     latlon(:location) { Sunspot::Util::Coordinates.new(lat, lon) }   end end Filter By Radius # Searches posts within 100 kilometers of (32, -68) Post.search do   with(:location).in_radius(32, -68, 100) end Filter By Radius (inexact with bbox) # Searches posts within 100 kilometers of (32, -68) with `bbox`. This is # an approximation so searches run quicker, but it may include other # points that are slightly outside of the required distance Post.search do   with(:location).in_radius(32, -68, 100, :bbox =&gt; true) end Filter By Bounding Box # Searches posts within the bounding box defined by the corners (45, # -94) to (46, -93) Post.search do   with(:location).in_bounding_box([45, -94], [46, -93]) end Sort By Distance # Orders documents by closeness to (32, -68) Post.search do   order_by_geodist(:location, 32, -68) end Joins Solr 4 and above Solr joins allow you to filter objects by joining on additional documents.  More information can be found on the Solr Wiki. class Photo &lt; ActiveRecord::Base   searchable do     text :description     string :caption, :default_boost =&gt; 1.5     time :created_at     integer :photo_container_id   end end  class PhotoContainer &lt; ActiveRecord::Base   searchable do     text :name     join(:description, :target =&gt; Photo, :type =&gt; :text, :join =&gt; { :from =&gt; :photo_container_id, :to =&gt; :id })     join(:caption, :target =&gt; Photo, :type =&gt; :string, :join =&gt; { :from =&gt; :photo_container_id, :to =&gt; :id })     join(:photos_created, :target =&gt; Photo, :type =&gt; :time, :join =&gt; { :from =&gt; :photo_container_id, :to =&gt; :id }, :as =&gt; 'created_at_d')   end end  PhotoContainer.search do   with(:caption, 'blah')   with(:photos_created).between(Date.new(2011,3,1)..Date.new(2011,4,1))      fulltext("keywords", :fields =&gt; [:name, :description]) end  # ...or  PhotoContainer.search do   with(:caption, 'blah')   with(:photos_created).between(Date.new(2011,3,1)..Date.new(2011,4,1))      any do     fulltext("keyword1", :fields =&gt; :name)     fulltext("keyword2", :fields =&gt; :description) # will be joined from the Photo model   end end If your models have fields with the same name class Tweet &lt; ActiveRecord::Base   searchable do     text :keywords     integer :profile_id   end end  class Rss &lt; ActiveRecord::Base   searchable do     text :keywords     integer :profile_id   end end  class Profile &lt; ActiveRecord::Base   searchable do     text :name     join(:keywords, :prefix =&gt; "tweet", :target =&gt; Tweet, :type =&gt; :text, :join =&gt; { :from =&gt; :profile_id, :to =&gt; :id })     join(:keywords, :prefix =&gt; "rss", :target =&gt; Rss, :type =&gt; :text, :join =&gt; { :from =&gt; :profile_id, :to =&gt; :id })   end end  Profile.search do   any do     fulltext("keyword1 keyword2", :fields =&gt; [:tweet_keywords]) do       minimum_match 1     end          fulltext("keyword3", :fields =&gt; [:rss_keywords])   end end  # ...produces: # sort: "score desc", fl: "* score", start: 0, rows: 20, # fq: ["type:Profile"], # q: "(_query_:"{!join from=profile_ids_i to=id_i v=$qTweet91755700 fq=$fqTweet91755700}" OR _query_:"{!join from=profile_ids_i to=id_i v=$qRss91753840 fq=$fqRss91753840}")", # qTweet91755700: "_query_:"{!edismax qf='keywords_text' mm='1'}keyword1 keyword2"", fqTweet91755700: "type:Tweet", # qRss91753840: "_query_:"{!edismax qf='keywords_text'}keyword3"", fqRss91753840: "type:Rss" Highlighting Highlighting allows you to display snippets of the part of the document that matched the query. The fields you wish to highlight must be stored. class Post &lt; ActiveRecord::Base   searchable do     # ...     text :body, :stored =&gt; true   end end Highlighting matches on the body field, for instance, can be achieved like: search = Post.search do   fulltext "pizza" do     highlight :body   end end  # Will output something similar to: # Post #1 #   I really love *pizza* #   *Pizza* is my favorite thing # Post #2 #   Pepperoni *pizza* is delicious search.hits.each do |hit|   puts "Post ##{hit.primary_key}"    hit.highlights(:body).each do |highlight|     puts "  " + highlight.format { |word| "*#{word}*" }   end end Stats Solr can return some statistics on indexed numeric fields. Fetching statistics for average_rating: search = Post.search do   stats :average_rating end  puts "Minimum average rating: #{search.stats(:average_rating).min}" puts "Maximum average rating: #{search.stats(:average_rating).max}" Stats on multiple fields search = Post.search do   stats :average_rating, :blog_id end Faceting on stats It's possible to facet field stats on another field: search = Post.search do   stats :average_rating do     facet :featured   end end  search.stats(:average_rating).facet(:featured).rows do |row|   puts "Minimum average rating for featured=#{row.value}: #{row.min}" end Take care when requesting facets on a stats field, since all facet results are returned by Solr! Multiple stats and selective faceting search = Post.search do   stats :average_rating do     facet :featured   end   stats :blog_id do     facet :average_rating   end end Functions Functions in Solr make it possible to dynamically compute values for each document. This gives you more flexability and you don't have to only deal with static values. For more details, please read Fuction Query documentation. Sunspot supports functions in two ways:  You can use functions to dynamically count boosting for field:  #Posts with pizza, scored higher (square promotion field) if is_promoted Post.search do   fulltext 'pizza' do     boost(function {sqrt(:promotion)}) { with(:is_promoted, true) }   end end  You're able to use functions for ordering (see examples for order_by_function)  Atomic updates Atomic Updates is a feature in Solr 4.0 that allows you to update on a field level rather than on a document level. This means that you can update individual fields without having to send the entire document to Solr with the un-updated fields values. For more details, please read Atomic Update documentation. All fields of the model must be stored, otherwise non-stored values will be lost after an update. class Post &lt; ActiveRecord::Base   searchable do     # all fields stored     text :body, :stored =&gt; true     string :title, :stored =&gt; true   end end  post1 = Post.create #... post2 = Post.create #...  # atomic update on class level Post.atomic_update post1.id =&gt; {title: 'A New Title'}, post2.id =&gt; {body: 'A New Body'}  # atomic update on instance level post1.atomic_update body: 'A New Body', title: 'Another New Title' More Like This Sunspot can extract related items using more_like_this. When searching for similar items, you can pass a block with the following options:  fields :field_1[, :field_2, ...] minimum_term_frequency ## minimum_document_frequency ## minimum_word_length ## maximum_word_length ## maximum_query_terms ## boost_by_relevance true/false  class Post &lt; ActiveRecord::Base   searchable do     # The :more_like_this option must be set to true     text :body, :more_like_this =&gt; true   end end  post = Post.first  results = Sunspot.more_like_this(post) do   fields :body   minimum_term_frequency 5 end To use more_like_this you need to have the MoreLikeThis handler enabled in solrconfig.xml. Example handler will look like this: &lt;requestHandler class="solr.MoreLikeThisHandler" name="/mlt"&gt;   &lt;lst name="defaults"&gt;     &lt;str name="mlt.mintf"&gt;1&lt;/str&gt;     &lt;str name="mlt.mindf"&gt;2&lt;/str&gt;   &lt;/lst&gt; &lt;/requestHandler&gt;  Spellcheck Solr supports spellchecking of search results against a dictionary. Sunspot supports turning on the spellchecker via the query DSL and parsing the response. Read the solr docs for more information on how this all works inside Solr. Solr's default spellchecking engine expects to use a dictionary comprised of values from an indexed field. This tends to work better than a static dictionary file, since it includes proper nouns in your index. The default in sunspot's solrconfig.xml is textSpell (note that buildOnCommit isn't recommended in production): &lt;lst name="spellchecker"&gt;    &lt;str name="name"&gt;default&lt;/str&gt;    &lt;!-- change field to textSpell and use copyField in schema.xml    to spellcheck multiple fields --&gt;    &lt;str name="field"&gt;textSpell&lt;/str&gt;    &lt;str name="buildOnCommit"&gt;true&lt;/str&gt;  &lt;/lst&gt;  Define the textSpell field in your schema.xml. &lt;field name="textSpell" stored="false" type="textSpell" multiValued="true" indexed="true"/&gt;  To get some data into your spellchecking field, you can use copyField in schema.xml: &lt;copyField source="*_text"  dest="textSpell" /&gt; &lt;copyField source="*_s"  dest="textSpell" /&gt;  copyField works before any analyzers you have set up on the source fields. You can add your own analyzer by customizing the textSpell field type in schema.xml: &lt;fieldType name="textSpell" class="solr.TextField" positionIncrementGap="100" omitNorms="true"&gt;   &lt;analyzer&gt;     &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;     &lt;filter class="solr.StandardFilterFactory"/&gt;     &lt;filter class="solr.LowerCaseFilterFactory"/&gt;   &lt;/analyzer&gt; &lt;/fieldType&gt;  It's dangerous to add too much to this analyzer chain. It runs before words are inserted into the spellcheck dictionary, which means the suggestions that come back from solr are post-analyzer. With the default above, that means all spelling suggestions will be lower-case. Once you have solr configured, you can turn it on for a given query using the query DSL (see spellcheck_spec.rb for more examples): search = Sunspot.search(Post) do   keywords 'Cofee'   spellcheck :count =&gt; 3 end  Access the suggestions via the spellcheck_suggestions or spellcheck_suggestion_for (for just the top one) methods: search.spellcheck_suggestion_for('cofee') # =&gt; 'coffee'  search.spellcheck_suggestions # =&gt; [{word: 'coffee', freq: 10}, {word: 'toffee', freq: 1}]  If you've turned on collation, you can also get that result: search = Sunspot.search(Post) do   keywords 'Cofee market'   spellcheck :count =&gt; 3 end  search.spellcheck_collation # =&gt; 'coffee market'  Indexes In Depth TODO Index-Time Boosts To specify that a field should be boosted in relation to other fields for all queries, you can specify the boost at index time: class Post &lt; ActiveRecord::Base   searchable do     text :title, :boost =&gt; 5.0     text :body   end end Stored Fields Stored fields keep an original (untokenized/unanalyzed) version of their contents in Solr. Stored fields allow data to be retrieved without also hitting the underlying database (usually an SQL server). They are also required for highlighting and more like this queries. Stored fields come at some performance cost in the Solr index, so use them wisely. class Post &lt; ActiveRecord::Base   searchable do     text :body, :stored =&gt; true   end end  # Retrieving stored contents without hitting the database Post.search.hits.each do |hit|   puts hit.stored(:body) end Hits vs. Results Sunspot simply stores the type and primary key of objects in Solr. When results are retrieved, those primary keys are used to load the actual object (usually from an SQL database). # Using #results pulls in the records from the object-relational # mapper (e.g., ActiveRecord + a SQL server) Post.search.results.each do |result|   puts result.body end To access information about the results without querying the underlying database, use hits: # Using #hits gives back all information requested from Solr, but does # not load the object from the object-relational mapper Post.search.hits.each do |hit|   puts hit.stored(:body) end If you need both the result (ORM-loaded object) and Hit (e.g., for faceting, highlighting, etc...), you can use the convenience method each_hit_with_result: Post.search.each_hit_with_result do |hit, result|   # ... end Reindexing Objects If you are using Rails, objects are automatically indexed to Solr as a part of the save callbacks. There are a number of ways to index manually within Ruby: # On a class itself Person.reindex Sunspot.commit # or commit(true) for a soft commit (Solr4)  # On mixed objects Sunspot.index [post1, item2] Sunspot.index person3 Sunspot.commit # or commit(true) for a soft commit (Solr4)  # With autocommit Sunspot.index! [post1, item2, person3] If you make a change to the object's "schema" (code in the searchable block), you must reindex all objects so the changes are reflected in Solr: bundle exec rake sunspot:reindex  # or, to be specific to a certain model with a certain batch size: bundle exec rake sunspot:reindex[500,Post] # some shells will require escaping [ with \[ and ] with \]  # to skip the prompt asking you if you want to proceed with the reindexing: bundle exec rake sunspot:reindex[,,true] # some shells will require escaping [ with \[ and ] with \] Use Without Rails TODO Threading The default Sunspot Session is not thread-safe. If used in a multi-threaded environment (such as sidekiq), you should configure Sunspot to use the ThreadLocalSessionProxy: Sunspot.session = Sunspot::SessionProxy::ThreadLocalSessionProxy.new Within a Rails app, to ensure your config/sunspot.yml settings are properly setup in this session you can use  Sunspot::Rails.build_session to mirror the normal Sunspot setup process:   session = Sunspot::Rails.build_session  Sunspot::Rails::Configuration.new   Sunspot.session = session Manually Adjusting Solr Parameters To add or modify parameters sent to Solr, use adjust_solr_params: Post.search do   adjust_solr_params do |params|     params[:q] += " AND something_s:more"   end end Session Proxies TODO Type Reference TODO Configuration Configure Sunspot by creating a config/sunspot.yml file or by setting a SOLR_URL or a WEBSOLR_URL environment variable. The defaults are as follows. development:   solr:     hostname: localhost     port: 8982     log_level: INFO  test:   solr:     hostname: localhost     port: 8981     log_level: WARNING You may want to use SSL for production environments with a username and password. For example, set SOLR_URL to https://username:password@production.solr.example.com/solr. You can examine the value of Sunspot::Rails.configuration at runtime. Development Running Tests sunspot Install the required gem dependencies: cd /path/to/sunspot/sunspot bundle install Start a Solr instance on port 8983: bundle exec sunspot-solr start -p 8983 # or `bundle exec sunspot-solr run -p 8983` to run in foreground Run the tests: bundle exec rake spec If desired, stop the Solr instance: bundle exec sunspot-solr stop sunspot_rails Install the gem dependencies for sunspot: cd /path/to/sunspot/sunspot bundle install Start a Solr instance on port 8983: bundle exec sunspot-solr start -p 8983 # or `bundle exec sunspot-solr run -p 8983` to run in foreground Navigate to the sunspot_rails directory: cd ../sunspot_rails Run the tests: rake spec # all Rails versions rake spec RAILS=3.1.1 # specific Rails version only If desired, stop the Solr instance: cd ../sunspot bundle exec sunspot-solr stop Generating Documentation Install the yard and redcarpet gems: $ gem install yard redcarpet Uninstall the rdiscount gem, if installed: $ gem uninstall rdiscount Generate the documentation from topmost directory: $ yardoc -o docs */lib/**/*.rb - README.md Tutorials and Articles  Using Sunspot, Websolr, and Solr on Heroku (mrdanadams) Full Text Searching with Solr and Sunspot (Collective Idea) Full-text search in Rails with Sunspot (Tropical Software Observations) A Few Sunspot Tips (spiral_code) Sunspot: A Solr-Powered Search Engine for Ruby (Linux Magazine) Sunspot Showed Me the Light (ben koonse) RubyGems.org — A case study in upgrading to full-text search (Websolr) How to Implement Spatial Search with Sunspot and Solr (Code Quest) Sunspot 1.2 with Spatial Solr Plugin 2.0 (joelmats) rails3 + heroku + sunspot : madness (anhaminha) heroku + websolr + sunspot (Onemorecloud) How to get full text search working with Sunspot (Hobo Cookbook) Full text search with Sunspot in Rails (hemju) Using Sunspot for Free-Text Search with Redis (While I Pondered...) Default scope with Sunspot (Cloudspace) Index External Models with Sunspot/Solr (Medihack) Testing with Sunspot and Cucumber (Collective Idea) Testing Sunspot with Cucumber (spiral_code) Solr, and Sunspot (YT!) The Saga of the Switch (mrb -- includes comparison of Sunspot and Ultrasphinx) Conditional Indexing with Sunspot (mikepack) Introduction to Full Text Search for Rails Developers (Valve's)  License Sunspot is distributed under the MIT License, copyright (c) 2008-2013 Mat Brown   </readme><commitcount>1,888</commitcount><languages /><tags /><about>Solr-powered search for Ruby objects</about><starcount>0</starcount><watchcount>0</watchcount></repository><repository><username>alastairp</username><reponame>system-design-101</reponame><readme>               System Design 101 Table of Contents Communication protocols REST API vs. GraphQL How does gRPC work? What is a webhook? How to improve API performance? HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) SOAP vs REST vs GraphQL vs RPC Code First vs. API First HTTP status codes What does API gateway do? How do we design effective and safe APIs? TCP/IP encapsulation Why is Nginx called a “reverse” proxy? What are the common load-balancing algorithms? URL, URI, URN - Do you know the differences? CI/CD CI/CD Pipeline Explained in Simple Terms Netflix Tech Stack (CI/CD Pipeline) Architecture patterns MVC, MVP, MVVM, MVVM-C, and VIPER 18 Key Design Patterns Every Developer Should Know Database A nice cheat sheet of different databases in cloud services 8 Data Structures That Power Your Databases How is an SQL statement executed in the database? CAP theorem Types of Memory and Storage Visualizing a SQL query SQL language Cache Data is cached everywhere Why is Redis so fast? How can Redis be used? Top caching strategies Microservice architecture What does a typical microservice architecture look like? Microservice Best Practices What tech stack is commonly used for microservices? Why is Kafka fast Payment systems How to learn payment systems? Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? How does VISA work when we swipe a credit card at a merchant’s shop? Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India DevOps DevOps vs. SRE vs. Platform Engineering. What is the difference? What is k8s (Kubernetes)? Docker vs. Kubernetes. Which one should we use? How does Docker work? GIT How Git Commands work How does Git Work? Git merge vs. Git rebase Cloud Services A nice cheat sheet of different cloud services (2023 edition) What is cloud native? Developer productivity tools Visualize JSON files Automatically turn code into architecture diagrams Linux Linux file system explained 18 Most-used Linux Commands You Should Know Security How does HTTPS work? Oauth 2.0 Explained With Simple Terms. Top 4 Forms of Authentication Mechanisms Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? How to store passwords safely in the database and how to validate a password? Explaining JSON Web Token (JWT) to a 10 year old Kid How does Google Authenticator (or other types of 2-factor authenticators) work? Real World Case Studies Netflix's Tech Stack Twitter Architecture 2022 Evolution of Airbnb’s microservice architecture over the past 15 years Monorepo vs. Microrepo. How will you design the Stack Overflow website? Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? How does Disney Hotstar capture 5 Billion Emojis during a tournament? How Discord Stores Trillions Of Messages How do video live streamings work on YouTube, TikTok live, or Twitch? License      README.md            【        👨🏻‍💻 YouTube    |         📮 Newsletter    】  System Design 101 Explain complex systems using visuals and simple terms. Whether you're preparing for a System Design Interview or you simply want to understand how systems work beneath the surface, we hope this repository will help you achieve that. Table of Contents  Communication protocols  REST API vs. GraphQL How does gRPC work? What is a webhook? How to improve API performance? HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) SOAP vs REST vs GraphQL vs RPC Code First vs. API First HTTP status codes What does API gateway do? How do we design effective and safe APIs? TCP/IP encapsulation Why is Nginx called a “reverse” proxy? What are the common load-balancing algorithms? URL, URI, URN - Do you know the differences?   CI/CD  CI/CD Pipeline Explained in Simple Terms Netflix Tech Stack (CI/CD Pipeline)   Architecture patterns  MVC, MVP, MVVM, MVVM-C, and VIPER 18 Key Design Patterns Every Developer Should Know   Database  A nice cheat sheet of different databases in cloud services 8 Data Structures That Power Your Databases How is an SQL statement executed in the database? CAP theorem Types of Memory and Storage Visualizing a SQL query SQL language   Cache  Data is cached everywhere Why is Redis so fast? How can Redis be used? Top caching strategies   Microservice architecture  What does a typical microservice architecture look like? Microservice Best Practices What tech stack is commonly used for microservices? Why is Kafka fast   Payment systems  How to learn payment systems? Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? How does VISA work when we swipe a credit card at a merchant’s shop? Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India   DevOps  DevOps vs. SRE vs. Platform Engineering. What is the difference? What is k8s (Kubernetes)? Docker vs. Kubernetes. Which one should we use? How does Docker work?   GIT  How Git Commands work How does Git Work? Git merge vs. Git rebase   Cloud Services  A nice cheat sheet of different cloud services (2023 edition) What is cloud native?   Developer productivity tools  Visualize JSON files Automatically turn code into architecture diagrams   Linux  Linux file system explained 18 Most-used Linux Commands You Should Know   Security  How does HTTPS work? Oauth 2.0 Explained With Simple Terms. Top 4 Forms of Authentication Mechanisms Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? How to store passwords safely in the database and how to validate a password? Explaining JSON Web Token (JWT) to a 10 year old Kid How does Google Authenticator (or other types of 2-factor authenticators) work?   Real World Case Studies  Netflix's Tech Stack Twitter Architecture 2022 Evolution of Airbnb’s microservice architecture over the past 15 years Monorepo vs. Microrepo. How will you design the Stack Overflow website? Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? How does Disney Hotstar capture 5 Billion Emojis during a tournament? How Discord Stores Trillions Of Messages How do video live streamings work on YouTube, TikTok live, or Twitch?    Communication protocols Architecture styles define how different components of an application programming interface (API) interact with one another. As a result, they ensure efficiency, reliability, and ease of integration with other systems by providing a standard approach to designing and building APIs. Here are the most used styles:      SOAP:  ature, comprehensive, XML-based Best for enterprise applications    RESTful:  Popular, easy-to-implement, HTTP methods  Ideal for web services    GraphQL:  Query language, request specific data  Reduces network overhead, faster responses    gRPC:  Modern, high-performance, Protocol Buffers  Suitable for microservices architectures    WebSocket:  Real-time, bidirectional, persistent connections  Perfect for low-latency data exchange    Webhook:  Event-driven, HTTP callbacks, asynchronous  Notifies systems when events occur   REST API vs. GraphQL The diagram below shows a quick comparison between REST and GraphQL.      GraphQL is a query language for APIs developed by Meta. It provides a complete description of the data in the API and gives clients the power to ask for exactly what they need.   GraphQL servers sit in between the client and the backend services. GraphQL can aggregate multiple REST requests into one query. GraphQL server organizes the resources in a graph.   GraphQL supports queries, mutations (applying data modifications to resources), and subscriptions (receiving notifications on schema modifications).   How does gRPC work?    RPC (Remote Procedure Call) is called “remote” because it enables communications between remote services when services are deployed to different servers under microservice architecture. From the user’s point of view, it acts like a local function call. The diagram below illustrates the overall data flow for gRPC. Step 1: A REST call is made from the client. The request body is usually in JSON format. Steps 2 - 4: The order service (gRPC client) receives the REST call, transforms it, and makes an RPC call to the payment service. gPRC encodes the client stub into a binary format and sends it to the low-level transport layer. Step 5: gRPC sends the packets over the network via HTTP2. Because of binary encoding and network optimizations, gRPC is said to be 5X faster than JSON. Steps 6 - 8: The payment service (gRPC server) receives the packets from the network, decodes them, and invokes the server application. Steps 9 - 11: The result is returned from the server application, and gets encoded and sent to the transport layer. Steps 12 - 14: The order service receives the packets, decodes them, and sends the result to the client application. What is a webhook? The diagram below shows a comparison between polling and Webhook.     Assume we run an eCommerce website. The clients send orders to the order service via the API gateway, which goes to the payment service for payment transactions. The payment service then talks to an external payment service provider (PSP) to complete the transactions.  There are two ways to handle communications with the external PSP.  1. Short polling  After sending the payment request to the PSP, the payment service keeps asking the PSP about the payment status. After several rounds, the PSP finally returns with the status.  Short polling has two drawbacks:   Constant polling of the status requires resources from the payment service.  The External service communicates directly with the payment service, creating security vulnerabilities.   2. Webhook  We can register a webhook with the external service. It means: call me back at a certain URL when you have updates on the request. When the PSP has completed the processing, it will invoke the HTTP request to update the payment status. In this way, the programming paradigm is changed, and the payment service doesn’t need to waste resources to poll the payment status anymore. What if the PSP never calls back? We can set up a housekeeping job to check payment status every hour. Webhooks are often referred to as reverse APIs or push APIs because the server sends HTTP requests to the client. We need to pay attention to 3 things when using a webhook:  We need to design a proper API for the external service to call. We need to set up proper rules in the API gateway for security reasons. We need to register the correct URL at the external service.  How to improve API performance? The diagram below shows 5 common tricks to improve API performance.    Pagination This is a common optimization when the size of the result is large. The results are streaming back to the client to improve the service responsiveness. Asynchronous Logging Synchronous logging deals with the disk for every call and can slow down the system. Asynchronous logging sends logs to a lock-free buffer first and immediately returns. The logs will be flushed to the disk periodically. This significantly reduces the I/O overhead. Caching We can cache frequently accessed data into a cache. The client can query the cache first instead of visiting the database directly. If there is a cache miss, the client can query from the database. Caches like Redis store data in memory, so the data access is much faster than the database. Payload Compression The requests and responses can be compressed using gzip etc so that the transmitted data size is much smaller. This speeds up the upload and download. Connection Pool When accessing resources, we often need to load data from the database. Opening the closing db connections adds significant overhead. So we should connect to the db via a pool of open connections. The connection pool is responsible for managing the connection lifecycle. HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) What problem does each generation of HTTP solve? The diagram below illustrates the key features.      HTTP 1.0 was finalized and fully documented in 1996. Every request to the same server requires a separate TCP connection.   HTTP 1.1 was published in 1997. A TCP connection can be left open for reuse (persistent connection), but it doesn’t solve the HOL (head-of-line) blocking issue. HOL blocking - when the number of allowed parallel requests in the browser is used up, subsequent requests need to wait for the former ones to complete.   HTTP 2.0 was published in 2015. It addresses HOL issue through request multiplexing, which eliminates HOL blocking at the application layer, but HOL still exists at the transport (TCP) layer. As you can see in the diagram, HTTP 2.0 introduced the concept of HTTP “streams”: an abstraction that allows multiplexing different HTTP exchanges onto the same TCP connection. Each stream doesn’t need to be sent in order.   HTTP 3.0 first draft was published in 2020. It is the proposed successor to HTTP 2.0. It uses QUIC instead of TCP for the underlying transport protocol, thus removing HOL blocking in the transport layer.   QUIC is based on UDP. It introduces streams as first-class citizens at the transport layer. QUIC streams share the same QUIC connection, so no additional handshakes and slow starts are required to create new ones, but QUIC streams are delivered independently such that in most cases packet loss affecting one stream doesn't affect others. SOAP vs REST vs GraphQL vs RPC The diagram below illustrates the API timeline and API styles comparison. Over time, different API architectural styles are released. Each of them has its own patterns of standardizing data exchange. You can check out the use cases of each style in the diagram.    Code First vs. API First The diagram below shows the differences between code-first development and API-first development. Why do we want to consider API first design?     Microservices increase system complexity We have separate services to serve different functions of the system. While this kind of architecture facilitates decoupling and segregation of duty, we need to handle the various communications among services.  It is better to think through the system's complexity before writing the code and carefully defining the boundaries of the services.  Separate functional teams need to speak the same language The dedicated functional teams are only responsible for their own components and services. It is recommended that the organization speak the same language via API design.  We can mock requests and responses to validate the API design before writing code.  Improve software quality and developer productivity Since we have ironed out most of the uncertainties when the project starts, the overall development process is smoother, and the software quality is greatly improved.  Developers are happy about the process as well because they can focus on functional development instead of negotiating sudden changes. The possibility of having surprises toward the end of the project lifecycle is reduced. Because we have designed the API first, the tests can be designed while the code is being developed. In a way, we also have TDD (Test Driven Design) when using API first development. HTTP status codes    The response codes for HTTP are divided into five categories: Informational (100-199) Success (200-299) Redirection (300-399) Client Error (400-499) Server Error (500-599) What does API gateway do? The diagram below shows the details.    Step 1 - The client sends an HTTP request to the API gateway. Step 2 - The API gateway parses and validates the attributes in the HTTP request. Step 3 - The API gateway performs allow-list/deny-list checks. Step 4 - The API gateway talks to an identity provider for authentication and authorization. Step 5 - The rate limiting rules are applied to the request. If it is over the limit, the request is rejected. Steps 6 and 7 - Now that the request has passed basic checks, the API gateway finds the relevant service to route to by path matching. Step 8 - The API gateway transforms the request into the appropriate protocol and sends it to backend microservices. Steps 9-12: The API gateway can handle errors properly, and deals with faults if the error takes a longer time to recover (circuit break). It can also leverage ELK (Elastic-Logstash-Kibana) stack for logging and monitoring. We sometimes cache data in the API gateway. How do we design effective and safe APIs? The diagram below shows typical API designs with a shopping cart example.    Note that API design is not just URL path design. Most of the time, we need to choose the proper resource names, identifiers, and path patterns. It is equally important to design proper HTTP header fields or to design effective rate-limiting rules within the API gateway. TCP/IP encapsulation How is data sent over the network? Why do we need so many layers in the OSI model?    The diagram below shows how data is encapsulated and de-encapsulated when transmitting over the network. Step 1: When Device A sends data to Device B over the network via the HTTP protocol, it is first added an HTTP header at the application layer. Step 2: Then a TCP or a UDP header is added to the data. It is encapsulated into TCP segments at the transport layer. The header contains the source port, destination port, and sequence number. Step 3: The segments are then encapsulated with an IP header at the network layer. The IP header contains the source/destination IP addresses. Step 4: The IP datagram is added a MAC header at the data link layer, with source/destination MAC addresses. Step 5: The encapsulated frames are sent to the physical layer and sent over the network in binary bits. Steps 6-10: When Device B receives the bits from the network, it performs the de-encapsulation process, which is a reverse processing of the encapsulation process. The headers are removed layer by layer, and eventually, Device B can read the data. We need layers in the network model because each layer focuses on its own responsibilities. Each layer can rely on the headers for processing instructions and does not need to know the meaning of the data from the last layer. Why is Nginx called a “reverse” proxy? The diagram below shows the differences between a 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐱𝐲 and a 𝐫𝐞𝐯𝐞𝐫𝐬𝐞 𝐩𝐫𝐨𝐱𝐲.    A forward proxy is a server that sits between user devices and the internet. A forward proxy is commonly used for:  Protect clients Avoid browsing restrictions Block access to certain content  A reverse proxy is a server that accepts a request from the client, forwards the request to web servers, and returns the results to the client as if the proxy server had processed the request. A reverse proxy is good for:  Protect servers Load balancing Cache static contents Encrypt and decrypt SSL communications  What are the common load-balancing algorithms? The diagram below shows 6 common algorithms.     Static Algorithms    Round robin The client requests are sent to different service instances in sequential order. The services are usually required to be stateless.   Sticky round-robin This is an improvement of the round-robin algorithm. If Alice’s first request goes to service A, the following requests go to service A as well.   Weighted round-robin The admin can specify the weight for each service. The ones with a higher weight handle more requests than others.   Hash This algorithm applies a hash function on the incoming requests’ IP or URL. The requests are routed to relevant instances based on the hash function result.   Dynamic Algorithms Least connections A new request is sent to the service instance with the least concurrent connections. Least response time A new request is sent to the service instance with the fastest response time. URL, URI, URN - Do you know the differences? The diagram below shows a comparison of URL, URI, and URN.     URI  URI stands for Uniform Resource Identifier. It identifies a logical or physical resource on the web. URL and URN are subtypes of URI. URL locates a resource, while URN names a resource. A URI is composed of the following parts: scheme:[//authority]path[?query][#fragment]  URL  URL stands for Uniform Resource Locator, the key concept of HTTP. It is the address of a unique resource on the web. It can be used with other protocols like FTP and JDBC.  URN  URN stands for Uniform Resource Name. It uses the urn scheme. URNs cannot be used to locate a resource. A simple example given in the diagram is composed of a namespace and a namespace-specific string. If you would like to learn more detail on the subject, I would recommend W3C’s clarification. CI/CD CI/CD Pipeline Explained in Simple Terms    Section 1 - SDLC with CI/CD The software development life cycle (SDLC) consists of several key stages: development, testing, deployment, and maintenance. CI/CD automates and integrates these stages to enable faster and more reliable releases. When code is pushed to a git repository, it triggers an automated build and test process. End-to-end (e2e) test cases are run to validate the code. If tests pass, the code can be automatically deployed to staging/production. If issues are found, the code is sent back to development for bug fixing. This automation provides fast feedback to developers and reduces the risk of bugs in production. Section 2 - Difference between CI and CD Continuous Integration (CI) automates the build, test, and merge process. It runs tests whenever code is committed to detect integration issues early. This encourages frequent code commits and rapid feedback. Continuous Delivery (CD) automates release processes like infrastructure changes and deployment. It ensures software can be released reliably at any time through automated workflows. CD may also automate the manual testing and approval steps required before production deployment. Section 3 - CI/CD Pipeline A typical CI/CD pipeline has several connected stages:  The developer commits code changes to the source control CI server detects changes and triggers the build Code is compiled, and tested (unit, integration tests) Test results reported to the developer On success, artifacts are deployed to staging environments Further testing may be done on staging before release CD system deploys approved changes to production  Netflix Tech Stack (CI/CD Pipeline)    Planning: Netflix Engineering uses JIRA for planning and Confluence for documentation. Coding: Java is the primary programming language for the backend service, while other languages are used for different use cases. Build: Gradle is mainly used for building, and Gradle plugins are built to support various use cases. Packaging: Package and dependencies are packed into an Amazon Machine Image (AMI) for release. Testing: Testing emphasizes the production culture's focus on building chaos tools. Deployment: Netflix uses its self-built Spinnaker for canary rollout deployment. Monitoring: The monitoring metrics are centralized in Atlas, and Kayenta is used to detect anomalies. Incident report: Incidents are dispatched according to priority, and PagerDuty is used for incident handling. Architecture patterns MVC, MVP, MVVM, MVVM-C, and VIPER These architecture patterns are among the most commonly used in app development, whether on iOS or Android platforms. Developers have introduced them to overcome the limitations of earlier patterns. So, how do they differ?     MVC, the oldest pattern, dates back almost 50 years Every pattern has a "view" (V) responsible for displaying content and receiving user input Most patterns include a "model" (M) to manage business data "Controller," "presenter," and "view-model" are translators that mediate between the view and the model ("entity" in the VIPER pattern)  18 Key Design Patterns Every Developer Should Know Patterns are reusable solutions to common design problems, resulting in a smoother, more efficient development process. They serve as blueprints for building better software structures. These are some of the most popular patterns:     Abstract Factory: Family Creator - Makes groups of related items. Builder: Lego Master - Builds objects step by step, keeping creation and appearance separate. Prototype: Clone Maker - Creates copies of fully prepared examples. Singleton: One and Only - A special class with just one instance. Adapter: Universal Plug - Connects things with different interfaces. Bridge: Function Connector - Links how an object works to what it does. Composite: Tree Builder - Forms tree-like structures of simple and complex parts. Decorator: Customizer - Adds features to objects without changing their core. Facade: One-Stop-Shop - Represents a whole system with a single, simplified interface. Flyweight: Space Saver - Shares small, reusable items efficiently. Proxy: Stand-In Actor - Represents another object, controlling access or actions. Chain of Responsibility: Request Relay - Passes a request through a chain of objects until handled. Command: Task Wrapper - Turns a request into an object, ready for action. Iterator: Collection Explorer - Accesses elements in a collection one by one. Mediator: Communication Hub - Simplifies interactions between different classes. Memento: Time Capsule - Captures and restores an object's state. Observer: News Broadcaster - Notifies classes about changes in other objects. Visitor: Skillful Guest - Adds new operations to a class without altering it.  Database A nice cheat sheet of different databases in cloud services    Choosing the right database for your project is a complex task. Many database options, each suited to distinct use cases, can quickly lead to decision fatigue. We hope this cheat sheet provides high-level direction to pinpoint the right service that aligns with your project's needs and avoid potential pitfalls. Note: Google has limited documentation for their database use cases. Even though we did our best to look at what was available and arrived at the best option, some of the entries may need to be more accurate. 8 Data Structures That Power Your Databases The answer will vary depending on your use case. Data can be indexed in memory or on disk. Similarly, data formats vary, such as numbers, strings, geographic coordinates, etc. The system might be write-heavy or read-heavy. All of these factors affect your choice of database index format.    The following are some of the most popular data structures used for indexing data:  Skiplist: a common in-memory index type. Used in Redis Hash index: a very common implementation of the “Map” data structure (or “Collection”) SSTable: immutable on-disk “Map” implementation LSM tree: Skiplist + SSTable. High write throughput B-tree: disk-based solution. Consistent read/write performance Inverted index: used for document indexing. Used in Lucene Suffix tree: for string pattern search R-tree: multi-dimension search, such as finding the nearest neighbor  How is an SQL statement executed in the database? The diagram below shows the process. Note that the architectures for different databases are different, the diagram demonstrates some common designs.    Step 1 - A SQL statement is sent to the database via a transport layer protocol (e.g.TCP). Step 2 - The SQL statement is sent to the command parser, where it goes through syntactic and semantic analysis, and a query tree is generated afterward. Step 3 - The query tree is sent to the optimizer. The optimizer creates an execution plan. Step 4 - The execution plan is sent to the executor. The executor retrieves data from the execution. Step 5 - Access methods provide the data fetching logic required for execution, retrieving data from the storage engine. Step 6 - Access methods decide whether the SQL statement is read-only. If the query is read-only (SELECT statement), it is passed to the buffer manager for further processing. The buffer manager looks for the data in the cache or data files. Step 7 - If the statement is an UPDATE or INSERT, it is passed to the transaction manager for further processing. Step 8 - During a transaction, the data is in lock mode. This is guaranteed by the lock manager. It also ensures the transaction’s ACID properties. CAP theorem The CAP theorem is one of the most famous terms in computer science, but I bet different developers have different understandings. Let’s examine what it is and why it can be confusing.    CAP theorem states that a distributed system can't provide more than two of these three guarantees simultaneously. Consistency: consistency means all clients see the same data at the same time no matter which node they connect to. Availability: availability means any client that requests data gets a response even if some of the nodes are down. Partition Tolerance: a partition indicates a communication break between two nodes. Partition tolerance means the system continues to operate despite network partitions. The “2 of 3” formulation can be useful, but this simplification could be misleading.   Picking a database is not easy. Justifying our choice purely based on the CAP theorem is not enough. For example, companies don't choose Cassandra for chat applications simply because it is an AP system. There is a list of good characteristics that make Cassandra a desirable option for storing chat messages. We need to dig deeper.   “CAP prohibits only a tiny part of the design space: perfect availability and consistency in the presence of partitions, which are rare”. Quoted from the paper: CAP Twelve Years Later: How the “Rules” Have Changed.   The theorem is about 100% availability and consistency. A more realistic discussion would be the trade-offs between latency and consistency when there is no network partition. See PACELC theorem for more details.   Is the CAP theorem actually useful? I think it is still useful as it opens our minds to a set of tradeoff discussions, but it is only part of the story. We need to dig deeper when picking the right database. Types of Memory and Storage    Visualizing a SQL query    SQL statements are executed by the database system in several steps, including:  Parsing the SQL statement and checking its validity Transforming the SQL into an internal representation, such as relational algebra Optimizing the internal representation and creating an execution plan that utilizes index information Executing the plan and returning the results  The execution of SQL is highly complex and involves many considerations, such as:  The use of indexes and caches The order of table joins Concurrency control Transaction management  SQL language In 1986, SQL (Structured Query Language) became a standard. Over the next 40 years, it became the dominant language for relational database management systems. Reading the latest standard (ANSI SQL 2016) can be time-consuming. How can I learn it?    There are 5 components of the SQL language:  DDL: data definition language, such as CREATE, ALTER, DROP DQL: data query language, such as SELECT DML: data manipulation language, such as INSERT, UPDATE, DELETE DCL: data control language, such as GRANT, REVOKE TCL: transaction control language, such as COMMIT, ROLLBACK  For a backend engineer, you may need to know most of it. As a data analyst, you may need to have a good understanding of DQL. Select the topics that are most relevant to you. Cache Data is cached everywhere This diagram illustrates where we cache data in a typical architecture.    There are multiple layers along the flow.  Client apps: HTTP responses can be cached by the browser. We request data over HTTP for the first time, and it is returned with an expiry policy in the HTTP header; we request data again, and the client app tries to retrieve the data from the browser cache first. CDN: CDN caches static web resources. The clients can retrieve data from a CDN node nearby. Load Balancer: The load Balancer can cache resources as well. Messaging infra: Message brokers store messages on disk first, and then consumers retrieve them at their own pace. Depending on the retention policy, the data is cached in Kafka clusters for a period of time. Services: There are multiple layers of cache in a service. If the data is not cached in the CPU cache, the service will try to retrieve the data from memory. Sometimes the service has a second-level cache to store data on disk. Distributed Cache: Distributed cache like Redis holds key-value pairs for multiple services in memory. It provides much better read/write performance than the database. Full-text Search: we sometimes need to use full-text searches like Elastic Search for document search or log search. A copy of data is indexed in the search engine as well. Database: Even in the database, we have different levels of caches:   WAL(Write-ahead Log): data is written to WAL first before building the B tree index Bufferpool: A memory area allocated to cache query results Materialized View: Pre-compute query results and store them in the database tables for better query performance Transaction log: record all the transactions and database updates Replication Log: used to record the replication state in a database cluster  Why is Redis so fast? There are 3 main reasons as shown in the diagram below.     Redis is a RAM-based data store. RAM access is at least 1000 times faster than random disk access. Redis leverages IO multiplexing and single-threaded execution loop for execution efficiency. Redis leverages several efficient lower-level data structures.  Question: Another popular in-memory store is Memcached. Do you know the differences between Redis and Memcached? You might have noticed the style of this diagram is different from my previous posts. Please let me know which one you prefer. How can Redis be used?    There is more to Redis than just caching. Redis can be used in a variety of scenarios as shown in the diagram.   Session We can use Redis to share user session data among different services.   Cache We can use Redis to cache objects or pages, especially for hotspot data.   Distributed lock We can use a Redis string to acquire locks among distributed services.   Counter We can count how many likes or how many reads for articles.   Rate limiter We can apply a rate limiter for certain user IPs.   Global ID generator We can use Redis Int for global ID.   Shopping cart We can use Redis Hash to represent key-value pairs in a shopping cart.   Calculate user retention We can use Bitmap to represent the user login daily and calculate user retention.   Message queue We can use List for a message queue.   Ranking We can use ZSet to sort the articles.   Top caching strategies Designing large-scale systems usually requires careful consideration of caching. Below are five caching strategies that are frequently utilized.    Microservice architecture What does a typical microservice architecture look like?    The diagram below shows a typical microservice architecture.  Load Balancer: This distributes incoming traffic across multiple backend services. CDN (Content Delivery Network): CDN is a group of geographically distributed servers that hold static content for faster delivery. The clients look for content in CDN first, then progress  to backend services. API Gateway: This handles incoming requests and routes them to the relevant services. It talks to the identity provider and service discovery. Identity Provider: This handles authentication and authorization for users. Service Registry &amp; Discovery: Microservice registration and discovery happen in this component, and the API gateway looks for relevant services in this component to talk to. Management: This component is responsible for monitoring the services. Microservices: Microservices are designed and deployed in different domains. Each domain has its own database. The API gateway talks to the microservices via REST API or other protocols, and the microservices within the same domain talk to each other using RPC (Remote Procedure Call).  Benefits of microservices:  They can be quickly designed, deployed, and horizontally scaled. Each domain can be independently maintained by a dedicated team. Business requirements can be customized in each domain and better supported, as a result.  Microservice Best Practices A picture is worth a thousand words: 9 best practices for developing microservices.    When we develop microservices, we need to follow the following best practices:  Use separate data storage for each microservice Keep code at a similar level of maturity Separate build for each microservice Assign each microservice with a single responsibility Deploy into containers Design stateless services Adopt domain-driven design Design micro frontend Orchestrating microservices  What tech stack is commonly used for microservices? Below you will find a diagram showing the microservice tech stack, both for the development phase and for production.    ▶️ 𝐏𝐫𝐞-𝐏𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧  Define API - This establishes a contract between frontend and backend. We can use Postman or OpenAPI for this. Development - Node.js or react is popular for frontend development, and java/python/go for backend development. Also, we need to change the configurations in the API gateway according to API definitions. Continuous Integration - JUnit and Jenkins for automated testing. The code is packaged into a Docker image and deployed as microservices.  ▶️ 𝐏𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧  NGinx is a common choice for load balancers. Cloudflare provides CDN (Content Delivery Network). API Gateway - We can use spring boot for the gateway, and use Eureka/Zookeeper for service discovery. The microservices are deployed on clouds. We have options among AWS, Microsoft Azure, or Google GCP. Cache and Full-text Search - Redis is a common choice for caching key-value pairs. ElasticSearch is used for full-text search. Communications - For services to talk to each other, we can use messaging infra Kafka or RPC. Persistence - We can use MySQL or PostgreSQL for a relational database, and Amazon S3 for object store. We can also use Cassandra for the wide-column store if necessary. Management &amp; Monitoring - To manage so many microservices, the common Ops tools include Prometheus, Elastic Stack, and Kubernetes.  Why is Kafka fast There are many design decisions that contributed to Kafka’s performance. In this post, we’ll focus on two. We think these two carried the most weight.     The first one is Kafka’s reliance on Sequential I/O. The second design choice that gives Kafka its performance advantage is its focus on efficiency: zero copy principle.  The diagram illustrates how the data is transmitted between producer and consumer, and what zero-copy means.  Step 1.1 - 1.3: Producer writes data to the disk Step 2: Consumer reads data without zero-copy  2.1 The data is loaded from disk to OS cache 2.2 The data is copied from OS cache to Kafka application 2.3 Kafka application copies the data into the socket buffer 2.4 The data is copied from socket buffer to network card 2.5 The network card sends data out to the consumer  Step 3: Consumer reads data with zero-copy  3.1: The data is loaded from disk to OS cache 3.2 OS cache directly copies the data to the network card via sendfile() command 3.3 The network card sends data out to the consumer Zero copy is a shortcut to save the multiple data copies between application context and kernel context. Payment systems How to learn payment systems?    Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? The diagram below shows the economics of the credit card payment flow.    1.  The cardholder pays a merchant $100 to buy a product. 2. The merchant benefits from the use of the credit card with higher sales volume and needs to compensate the issuer and the card network for providing the payment service. The acquiring bank sets a fee with the merchant, called the “merchant discount fee.” 3 - 4. The acquiring bank keeps $0.25 as the acquiring markup, and $1.75 is paid to the issuing bank as the interchange fee. The merchant discount fee should cover the interchange fee. The interchange fee is set by the card network because it is less efficient for each issuing bank to negotiate fees with each merchant. 5.  The card network sets up the network assessments and fees with each bank, which pays the card network for its services every month. For example, VISA charges a 0.11% assessment, plus a $0.0195 usage fee, for every swipe. 6.  The cardholder pays the issuing bank for its services. Why should the issuing bank be compensated?  The issuer pays the merchant even if the cardholder fails to pay the issuer. The issuer pays the merchant before the cardholder pays the issuer. The issuer has other operating costs, including managing customer accounts, providing statements, fraud detection, risk management, clearing &amp; settlement, etc.  How does VISA work when we swipe a credit card at a merchant’s shop?    VISA, Mastercard, and American Express act as card networks for the clearing and settling of funds. The card acquiring bank and the card issuing bank can be – and often are – different. If banks were to settle transactions one by one without an intermediary, each bank would have to settle the transactions with all the other banks. This is quite inefficient. The diagram below shows VISA’s role in the credit card payment process. There are two flows involved. Authorization flow happens when the customer swipes the credit card. Capture and settlement flow happens when the merchant wants to get the money at the end of the day.  Authorization Flow  Step 0: The card issuing bank issues credit cards to its customers. Step 1: The cardholder wants to buy a product and swipes the credit card at the Point of Sale (POS) terminal in the merchant’s shop. Step 2: The POS terminal sends the transaction to the acquiring bank, which has provided the POS terminal. Steps 3 and 4: The acquiring bank sends the transaction to the card network, also called the card scheme. The card network sends the transaction to the issuing bank for approval. Steps 4.1, 4.2 and 4.3: The issuing bank freezes the money if the transaction is approved. The approval or rejection is sent back to the acquirer, as well as the POS terminal.  Capture and Settlement Flow  Steps 1 and 2: The merchant wants to collect the money at the end of the day, so they hit ”capture” on the POS terminal. The transactions are sent to the acquirer in batch. The acquirer sends the batch file with transactions to the card network. Step 3: The card network performs clearing for the transactions collected from different acquirers, and sends the clearing files to different issuing banks. Step 4: The issuing banks confirm the correctness of the clearing files, and transfer money to the relevant acquiring banks. Step 5: The acquiring bank then transfers money to the merchant’s bank. Step 4: The card network clears the transactions from different acquiring banks. Clearing is a process in which mutual offset transactions are netted, so the number of total transactions is reduced. In the process, the card network takes on the burden of talking to each bank and receives service fees in return. Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India What’s UPI? UPI is an instant real-time payment system developed by the National Payments Corporation of India. It accounts for 60% of digital retail transactions in India today. UPI = payment markup language + standard for interoperable payments    DevOps DevOps vs. SRE vs. Platform Engineering. What is the difference? The concepts of DevOps, SRE, and Platform Engineering have emerged at different times and have been developed by various individuals and organizations.    DevOps as a concept was introduced in 2009 by Patrick Debois and Andrew Shafer at the Agile conference. They sought to bridge the gap between software development and operations by promoting a collaborative culture and shared responsibility for the entire software development lifecycle. SRE, or Site Reliability Engineering, was pioneered by Google in the early 2000s to address operational challenges in managing large-scale, complex systems. Google developed SRE practices and tools, such as the Borg cluster management system and the Monarch monitoring system, to improve the reliability and efficiency of their services. Platform Engineering is a more recent concept, building on the foundation of SRE engineering. The precise origins of Platform Engineering are less clear, but it is generally understood to be an extension of the DevOps and SRE practices, with a focus on delivering a comprehensive platform for product development that supports the entire business perspective. It's worth noting that while these concepts emerged at different times. They are all related to the broader trend of improving collaboration, automation, and efficiency in software development and operations. What is k8s (Kubernetes)? K8s is a container orchestration system. It is used for container deployment and management. Its design is greatly impacted by Google’s internal system Borg.    A k8s cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers, and a cluster usually runs multiple nodes, providing fault tolerance and high availability.  Control Plane Components    API Server The API server talks to all the components in the k8s cluster. All the operations on pods are executed by talking to the API server.   Scheduler The scheduler watches pod workloads and assigns loads on newly created pods.   Controller Manager The controller manager runs the controllers, including Node Controller, Job Controller, EndpointSlice Controller, and ServiceAccount Controller.   Etcd etcd is a key-value store used as Kubernetes' backing store for all cluster data.    Nodes    Pods A pod is a group of containers and is the smallest unit that k8s administers. Pods have a single IP address applied to every container within the pod.   Kubelet An agent that runs on each node in the cluster. It ensures containers are running in a Pod.   Kube Proxy Kube-proxy is a network proxy that runs on each node in your cluster. It routes traffic coming into a node from the service. It forwards requests for work to the correct containers.   Docker vs. Kubernetes. Which one should we use?    What is Docker ? Docker is an open-source platform that allows you to package, distribute, and run applications in isolated containers. It focuses on containerization, providing lightweight environments that encapsulate applications and their dependencies. What is Kubernetes ? Kubernetes, often referred to as K8s, is an open-source container orchestration platform. It provides a framework for automating the deployment, scaling, and management of containerized applications across a cluster of nodes. How are both different from each other ? Docker: Docker operates at the individual container level on a single operating system host. You must manually manage each host and setting up networks, security policies, and storage for multiple related containers can be complex. Kubernetes: Kubernetes operates at the cluster level. It manages multiple containerized applications across multiple hosts, providing automation for tasks like load balancing, scaling, and ensuring the desired state of applications. In short, Docker focuses on containerization and running containers on individual hosts, while Kubernetes specializes in managing and orchestrating containers at scale across a cluster of hosts. How does Docker work? The diagram below shows the architecture of Docker and how it works when we run “docker build”, “docker pull” and “docker run”.    There are 3 components in Docker architecture:   Docker client The docker client talks to the Docker daemon.   Docker host The Docker daemon listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.   Docker registry A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use.   Let’s take the “docker run” command as an example.  Docker pulls the image from the registry. Docker creates a new container. Docker allocates a read-write filesystem to the container. Docker creates a network interface to connect the container to the default network. Docker starts the container.  GIT How Git Commands work To begin with, it's essential to identify where our code is stored. The common assumption is that there are only two locations - one on a remote server like Github and the other on our local machine. However, this isn't entirely accurate. Git maintains three local storages on our machine, which means that our code can be found in four places:     Working directory: where we edit files Staging area: a temporary location where files are kept for the next commit Local repository: contains the code that has been committed Remote repository: the remote server that stores the code  Most Git commands primarily move files between these four locations. How does Git Work? The diagram below shows the Git workflow.    Git is a distributed version control system. Every developer maintains a local copy of the main repository and edits and commits to the local copy. The commit is very fast because the operation doesn’t interact with the remote repository. If the remote repository crashes, the files can be recovered from the local repositories. Git merge vs. Git rebase What are the differences?    When we merge changes from one Git branch to another, we can use ‘git merge’ or ‘git rebase’. The diagram below shows how the two commands work. Git merge This creates a new commit G’ in the main branch. G’ ties the histories of both main and feature branches. Git merge is non-destructive. Neither the main nor the feature branch is changed. Git rebase Git rebase moves the feature branch histories to the head of the main branch. It creates new commits E’, F’, and G’ for each commit in the feature branch. The benefit of rebase is that it has a linear commit history. Rebase can be dangerous if “the golden rule of git rebase” is not followed. The Golden Rule of Git Rebase Never use it on public branches! Cloud Services A nice cheat sheet of different cloud services (2023 edition)    What is cloud native? Below is a diagram showing the evolution of architecture and processes since the 1980s.    Organizations can build and run scalable applications on public, private, and hybrid clouds using cloud native technologies. This means the applications are designed to leverage cloud features, so they are resilient to load and easy to scale. Cloud native includes 4 aspects:   Development process This has progressed from waterfall to agile to DevOps.   Application Architecture The architecture has gone from monolithic to microservices. Each service is designed to be small, adaptive to the limited resources in cloud containers.   Deployment &amp; packaging The applications used to be deployed on physical servers. Then around 2000, the applications that were not sensitive to latency were usually deployed on virtual servers. With cloud native applications, they are packaged into docker images and deployed in containers.   Application infrastructure The applications are massively deployed on cloud infrastructure instead of self-hosted servers.   Developer productivity tools Visualize JSON files Nested JSON files are hard to read. JsonCrack generates graph diagrams from JSON files and makes them easy to read. Additionally, the generated diagrams can be downloaded as images.    Automatically turn code into architecture diagrams    What does it do?  Draw the cloud system architecture in Python code. Diagrams can also be rendered directly inside the Jupyter Notebooks. No design tools are needed. Supports the following providers: AWS, Azure, GCP, Kubernetes, Alibaba Cloud, Oracle Cloud, etc.  Github repo Linux Linux file system explained    The Linux file system used to resemble an unorganized town where individuals constructed their houses wherever they pleased. However, in 1994, the Filesystem Hierarchy Standard (FHS) was introduced to bring order to the Linux file system. By implementing a standard like the FHS, software can ensure a consistent layout across various Linux distributions. Nonetheless, not all Linux distributions strictly adhere to this standard. They often incorporate their own unique elements or cater to specific requirements. To become proficient in this standard, you can begin by exploring. Utilize commands such as "cd" for navigation and "ls" for listing directory contents. Imagine the file system as a tree, starting from the root (/). With time, it will become second nature to you, transforming you into a skilled Linux administrator. 18 Most-used Linux Commands You Should Know Linux commands are instructions for interacting with the operating system. They help manage files, directories, system processes, and many other aspects of the system. You need to become familiar with these commands in order to navigate and maintain Linux-based systems efficiently and effectively. This diagram below shows popular Linux commands:     ls - List files and directories cd - Change the current directory mkdir - Create a new directory rm - Remove files or directories cp - Copy files or directories mv - Move or rename files or directories chmod - Change file or directory permissions grep - Search for a pattern in files find - Search for files and directories tar - manipulate tarball archive files vi - Edit files using text editors cat - display the content of files top - Display processes and resource usage ps - Display processes information kill - Terminate a process by sending a signal du - Estimate file space usage ifconfig - Configure network interfaces ping - Test network connectivity between hosts  Security How does HTTPS work? Hypertext Transfer Protocol Secure (HTTPS) is an extension of the Hypertext Transfer Protocol (HTTP.) HTTPS transmits encrypted data using Transport Layer Security (TLS.) If the data is hijacked online, all the hijacker gets is binary code.    How is the data encrypted and decrypted? Step 1 - The client (browser) and the server establish a TCP connection. Step 2 - The client sends a “client hello” to the server. The message contains a set of necessary encryption algorithms (cipher suites) and the latest TLS version it can support. The server responds with a “server hello” so the browser knows whether it can support the algorithms and TLS version. The server then sends the SSL certificate to the client. The certificate contains the public key, host name, expiry dates, etc. The client validates the certificate. Step 3 - After validating the SSL certificate, the client generates a session key and encrypts it using the public key. The server receives the encrypted session key and decrypts it with the private key. Step 4 - Now that both the client and the server hold the same session key (symmetric encryption), the encrypted data is transmitted in a secure bi-directional channel. Why does HTTPS switch to symmetric encryption during data transmission? There are two main reasons:   Security: The asymmetric encryption goes only one way. This means that if the server tries to send the encrypted data back to the client, anyone can decrypt the data using the public key.   Server resources: The asymmetric encryption adds quite a lot of mathematical overhead. It is not suitable for data transmissions in long sessions.   Oauth 2.0 Explained With Simple Terms. OAuth 2.0 is a powerful and secure framework that allows different applications to securely interact with each other on behalf of users without sharing sensitive credentials.    The entities involved in OAuth are the User, the Server, and the Identity Provider (IDP). What Can an OAuth Token Do? When you use OAuth, you get an OAuth token that represents your identity and permissions. This token can do a few important things: Single Sign-On (SSO): With an OAuth token, you can log into multiple services or apps using just one login, making life easier and safer. Authorization Across Systems: The OAuth token allows you to share your authorization or access rights across various systems, so you don't have to log in separately everywhere. Accessing User Profile: Apps with an OAuth token can access certain parts of your user profile that you allow, but they won't see everything. Remember, OAuth 2.0 is all about keeping you and your data safe while making your online experiences seamless and hassle-free across different applications and services. Top 4 Forms of Authentication Mechanisms      SSH Keys: Cryptographic keys are used to access remote systems and servers securely   OAuth Tokens: Tokens that provide limited access to user data on third-party applications   SSL Certificates: Digital certificates ensure secure and encrypted communication between servers and clients   Credentials: User authentication information is used to verify and grant access to various systems and services   Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? These terms are all related to user identity management. When you log into a website, you declare who you are (identification). Your identity is verified (authentication), and you are granted the necessary permissions (authorization). Many solutions have been proposed in the past, and the list keeps growing.    From simple to complex, here is my understanding of user identity management:   WWW-Authenticate is the most basic method. You are asked for the username and password by the browser. As a result of the inability to control the login life cycle, it is seldom used today.   A finer control over the login life cycle is session-cookie. The server maintains session storage, and the browser keeps the ID of the session. A cookie usually only works with browsers and is not mobile app friendly.   To address the compatibility issue, the token can be used. The client sends the token to the server, and the server validates the token. The downside is that the token needs to be encrypted and decrypted, which may be time-consuming.   JWT is a standard way of representing tokens. This information can be verified and trusted because it is digitally signed. Since JWT contains the signature, there is no need to save session information on the server side.   By using SSO (single sign-on), you can sign on only once and log in to multiple websites. It uses CAS (central authentication service) to maintain cross-site information   By using OAuth 2.0, you can authorize one website to access your information on another website.   How to store passwords safely in the database and how to validate a password?    Things NOT to do   Storing passwords in plain text is not a good idea because anyone with internal access can see them.   Storing password hashes directly is not sufficient because it is pruned to precomputation attacks, such as rainbow tables.   To mitigate precomputation attacks, we salt the passwords.   What is salt? According to OWASP guidelines, “a salt is a unique, randomly generated string that is added to each password as part of the hashing process”. How to store a password and salt?  the hash result is unique to each password. The password can be stored in the database using the following format: hash(password + salt).  How to validate a password? To validate a password, it can go through the following process:  A client enters the password. The system fetches the corresponding salt from the database. The system appends the salt to the password and hashes it. Let’s call the hashed value H1. The system compares H1 and H2, where H2 is the hash stored in the database. If they are the same, the password is valid.  Explaining JSON Web Token (JWT) to a 10 year old Kid    Imagine you have a special box called a JWT. Inside this box, there are three parts: a header, a payload, and a signature. The header is like the label on the outside of the box. It tells us what type of box it is and how it's secured. It's usually written in a format called JSON, which is just a way to organize information using curly braces { } and colons : . The payload is like the actual message or information you want to send. It could be your name, age, or any other data you want to share. It's also written in JSON format, so it's easy to understand and work with. Now, the signature is what makes the JWT secure. It's like a special seal that only the sender knows how to create. The signature is created using a secret code, kind of like a password. This signature ensures that nobody can tamper with the contents of the JWT without the sender knowing about it. When you want to send the JWT to a server, you put the header, payload, and signature inside the box. Then you send it over to the server. The server can easily read the header and payload to understand who you are and what you want to do. How does Google Authenticator (or other types of 2-factor authenticators) work? Google Authenticator is commonly used for logging into our accounts when 2-factor authentication is enabled. How does it guarantee security? Google Authenticator is a software-based authenticator that implements a two-step verification service. The diagram below provides detail.    There are two stages involved:  Stage 1 - The user enables Google two-step verification Stage 2 - The user uses the authenticator for logging in, etc.  Let’s look at these stages. Stage 1 Steps 1 and 2: Bob opens the web page to enable two-step verification. The front end requests a secret key. The authentication service generates the secret key for Bob and stores it in the database. Step 3: The authentication service returns a URI to the front end. The URI is composed of a key issuer, username, and secret key. The URI is displayed in the form of a QR code on the web page. Step 4: Bob then uses Google Authenticator to scan the generated QR code. The secret key is stored in the authenticator. Stage 2 Steps 1 and 2: Bob wants to log into a website with Google two-step verification. For this, he needs the password. Every 30 seconds, Google Authenticator generates a 6-digit password using TOTP (Time-based One Time Password) algorithm. Bob uses the password to enter the website. Steps 3 and 4: The frontend sends the password Bob enters to the backend for authentication. The authentication service reads the secret key from the database and generates a 6-digit password using the same TOTP algorithm as the client. Step 5: The authentication service compares the two passwords generated by the client and the server, and returns the comparison result to the frontend. Bob can proceed with the login process only if the two passwords match. Is this authentication mechanism safe?   Can the secret key be obtained by others? We need to make sure the secret key is transmitted using HTTPS. The authenticator client and the database store the secret key, and we need to make sure the secret keys are encrypted.   Can the 6-digit password be guessed by hackers? No. The password has 6 digits, so the generated password has 1 million potential combinations. Plus, the password changes every 30 seconds. If hackers want to guess the password in 30 seconds, they need to enter 30,000 combinations per second.   Real World Case Studies Netflix's Tech Stack This post is based on research from many Netflix engineering blogs and open-source projects. If you come across any inaccuracies, please feel free to inform us.    Mobile and web: Netflix has adopted Swift and Kotlin to build native mobile apps. For its web application, it uses React. Frontend/server communication: Netflix uses GraphQL. Backend services: Netflix relies on ZUUL, Eureka, the Spring Boot framework, and other technologies. Databases: Netflix utilizes EV cache, Cassandra, CockroachDB, and other databases. Messaging/streaming: Netflix employs Apache Kafka and Fink for messaging and streaming purposes. Video storage: Netflix uses S3 and Open Connect for video storage. Data processing: Netflix utilizes Flink and Spark for data processing, which is then visualized using Tableau. Redshift is used for processing structured data warehouse information. CI/CD: Netflix employs various tools such as JIRA, Confluence, PagerDuty, Jenkins, Gradle, Chaos Monkey, Spinnaker, Altas, and more for CI/CD processes. Twitter Architecture 2022 Yes, this is the real Twitter architecture. It is posted by Elon Musk and redrawn by us for better readability.    Evolution of Airbnb’s microservice architecture over the past 15 years Airbnb’s microservice architecture went through 3 main stages.    Monolith (2008 - 2017) Airbnb began as a simple marketplace for hosts and guests. This is built in a Ruby on Rails application - the monolith. What’s the challenge?  Confusing team ownership + unowned code Slow deployment  Microservices (2017 - 2020) Microservice aims to solve those challenges. In the microservcie architecture, key services include:  Data fetching service Business logic data service Write workflow service UI aggregation service Each service had one owning team  What’s the challenge? Hundreds of services and dependencies were difficult for humans to manage. Micro + macroservices (2020 - present) This is what Airbnb is working on now. The micro and macroservice hybrid model focuses on the unification of APIs. Monorepo vs. Microrepo. Which is the best? Why do different companies choose different options?    Monorepo isn't new; Linux and Windows were both created using Monorepo. To improve scalability and build speed, Google developed its internal dedicated toolchain to scale it faster and strict coding quality standards to keep it consistent. Amazon and Netflix are major ambassadors of the Microservice philosophy. This approach naturally separates the service code into separate repositories. It scales faster but can lead to governance pain points later on. Within Monorepo, each service is a folder, and every folder has a BUILD config and OWNERS permission control. Every service member is responsible for their own folder. On the other hand, in Microrepo, each service is responsible for its repository, with the build config and permissions typically set for the entire repository. In Monorepo, dependencies are shared across the entire codebase regardless of your business, so when there's a version upgrade, every codebase upgrades their version. In Microrepo, dependencies are controlled within each repository. Businesses choose when to upgrade their versions based on their own schedules. Monorepo has a standard for check-ins. Google's code review process is famously known for setting a high bar, ensuring a coherent quality standard for Monorepo, regardless of the business. Microrepo can either set its own standard or adopt a shared standard by incorporating best practices. It can scale faster for business, but the code quality might be a bit different. Google engineers built Bazel, and Meta built Buck. There are other open-source tools available, including Nix, Lerna, and others. Over the years, Microrepo has had more supported tools, including Maven and Gradle for Java, NPM for NodeJS, and CMake for C/C++, among others. How will you design the Stack Overflow website? If your answer is on-premise servers and monolith (on the right), you would likely fail the interview, but that's how it is built in reality!    What people think it should look like The interviewer is probably expecting something on the left side.  Microservice is used to decompose the system into small components. Each service has its own database. Use cache heavily. The service is sharded. The services talk to each other asynchronously through message queues. The service is implemented using Event Sourcing with CQRS. Showing off knowledge in distributed systems such as eventual consistency, CAP theorem, etc.  What it actually is Stack Overflow serves all the traffic with only 9 on-premise web servers, and it’s on monolith! It has its own servers and does not run on the cloud. This is contrary to all our popular beliefs these days. Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? The diagram below shows the architecture comparison before and after the migration.    What is Amazon Prime Video Monitoring Service? Prime Video service needs to monitor the quality of thousands of live streams. The monitoring tool automatically analyzes the streams in real time and identifies quality issues like block corruption, video freeze, and sync problems. This is an important process for customer satisfaction. There are 3 steps: media converter, defect detector, and real-time notification.   What is the problem with the old architecture? The old architecture was based on Amazon Lambda, which was good for building services quickly. However, it was not cost-effective when running the architecture at a high scale. The two most expensive operations are:     The orchestration workflow - AWS step functions charge users by state transitions and the orchestration performs multiple state transitions every second.   Data passing between distributed components - the intermediate data is stored in Amazon S3 so that the next stage can download. The download can be costly when the volume is high.     Monolithic architecture saves 90% cost A monolithic architecture is designed to address the cost issues. There are still 3 components, but the media converter and defect detector are deployed in the same process, saving the cost of passing data over the network. Surprisingly, this approach to deployment architecture change led to 90% cost savings!   This is an interesting and unique case study because microservices have become a go-to and fashionable choice in the tech industry. It's good to see that we are having more discussions about evolving the architecture and having more honest discussions about its pros and cons. Decomposing components into distributed microservices comes with a cost.   What did Amazon leaders say about this? Amazon CTO Werner Vogels: “Building evolvable software systems is a strategy, not a religion. And revisiting your architecture with an open mind is a must.”   Ex Amazon VP Sustainability Adrian Cockcroft: “The Prime Video team had followed a path I call Serverless First…I don’t advocate Serverless Only”. How does Disney Hotstar capture 5 Billion Emojis during a tournament?      Clients send emojis through standard HTTP requests. You can think of Golang Service as a typical Web Server. Golang is chosen because it supports concurrency well. Threads in GoLang are lightweight.   Since the write volume is very high, Kafka (message queue) is used as a buffer.   Emoji data are aggregated by a streaming processing service called Spark. It aggregates data every 2 seconds, which is configurable. There is a trade-off to be made based on the interval. A shorter interval means emojis are delivered to other clients faster but it also means more computing resources are needed.   Aggregated data is written to another Kafka.   The PubSub consumers pull aggregated emoji data from Kafka.   Emojis are delivered to other clients in real-time through the PubSub infrastructure. The PubSub infrastructure is interesting. Hotstar considered the following protocols: Socketio, NATS, MQTT, and gRPC, and settled with MQTT.    A similar design is adopted by LinkedIn which streams a million likes/sec. How Discord Stores Trillions Of Messages The diagram below shows the evolution of message storage at Discord:    MongoDB ➡️ Cassandra ➡️ ScyllaDB In 2015, the first version of Discord was built on top of a single MongoDB replica. Around Nov 2015, MongoDB stored 100 million messages and the RAM couldn’t hold the data and index any longer. The latency became unpredictable. Message storage needs to be moved to another database. Cassandra was chosen. In 2017, Discord had 12 Cassandra nodes and stored billions of messages. At the beginning of 2022, it had 177 nodes with trillions of messages. At this point, latency was unpredictable, and maintenance operations became too expensive to run. There are several reasons for the issue:  Cassandra uses the LSM tree for the internal data structure. The reads are more expensive than the writes. There can be many concurrent reads on a server with hundreds of users, resulting in hotspots. Maintaining clusters, such as compacting SSTables, impacts performance. Garbage collection pauses would cause significant latency spikes  ScyllaDB is Cassandra compatible database written in C++. Discord redesigned its architecture to have a monolithic API, a data service written in Rust, and ScyllaDB-based storage. The p99 read latency in ScyllaDB is 15ms compared to 40-125ms in Cassandra. The p99 write latency is 5ms compared to 5-70ms in Cassandra. How do video live streamings work on YouTube, TikTok live, or Twitch? Live streaming differs from regular streaming because the video content is sent via the internet in real-time, usually with a latency of just a few seconds. The diagram below explains what happens behind the scenes to make this possible.    Step 1: The raw video data is captured by a microphone and camera. The data is sent to the server side. Step 2: The video data is compressed and encoded. For example, the compressing algorithm separates the background and other video elements. After compression, the video is encoded to standards such as H.264. The size of the video data is much smaller after this step. Step 3: The encoded data is divided into smaller segments, usually seconds in length, so it takes much less time to download or stream. Step 4: The segmented data is sent to the streaming server. The streaming server needs to support different devices and network conditions. This is called ‘Adaptive Bitrate Streaming.’ This means we need to produce multiple files at different bitrates in steps 2 and 3. Step 5: The live streaming data is pushed to edge servers supported by CDN (Content Delivery Network.) Millions of viewers can watch the video from an edge server nearby. CDN significantly lowers data transmission latency. Step 6: The viewers’ devices decode and decompress the video data and play the video in a video player. Steps 7 and 8: If the video needs to be stored for replay, the encoded data is sent to a storage server, and viewers can request a replay from it later. Standard protocols for live streaming include:  RTMP (Real-Time Messaging Protocol): This was originally developed by Macromedia to transmit data between a Flash player and a server. Now it is used for streaming video data over the internet. Note that video conferencing applications like Skype use RTC (Real-Time Communication) protocol for lower latency. HLS (HTTP Live Streaming): It requires the H.264 or H.265 encoding. Apple devices accept only HLS format. DASH (Dynamic Adaptive Streaming over HTTP): DASH does not support Apple devices. Both HLS and DASH support adaptive bitrate streaming.  License This work is licensed under CC BY-NC-ND 4.0   </readme><commitcount>3</commitcount><languages /><tags /><about>Explain complex systems using visuals and simple terms. Help you prepare for system design interviews.</about><starcount>0</starcount><watchcount>0</watchcount></repository><repository><username>wx1993</username><reponame>system-design-101</reponame><readme>               System Design 101 Table of Contents Communication protocols REST API vs. GraphQL How does gRPC work? What is a webhook? How to improve API performance? HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) SOAP vs REST vs GraphQL vs RPC Code First vs. API First HTTP status codes What does API gateway do? How do we design effective and safe APIs? TCP/IP encapsulation Why is Nginx called a “reverse” proxy? What are the common load-balancing algorithms? URL, URI, URN - Do you know the differences? CI/CD CI/CD Pipeline Explained in Simple Terms Netflix Tech Stack (CI/CD Pipeline) Architecture patterns MVC, MVP, MVVM, MVVM-C, and VIPER 18 Key Design Patterns Every Developer Should Know Database A nice cheat sheet of different databases in cloud services 8 Data Structures That Power Your Databases How is an SQL statement executed in the database? CAP theorem Types of Memory and Storage Visualizing a SQL query SQL language Cache Data is cached everywhere Why is Redis so fast? How can Redis be used? Top caching strategies Microservice architecture What does a typical microservice architecture look like? Microservice Best Practices What tech stack is commonly used for microservices? Why is Kafka fast Payment systems How to learn payment systems? Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? How does VISA work when we swipe a credit card at a merchant’s shop? Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India DevOps DevOps vs. SRE vs. Platform Engineering. What is the difference? What is k8s (Kubernetes)? Docker vs. Kubernetes. Which one should we use? How does Docker work? GIT How Git Commands work How does Git Work? Git merge vs. Git rebase Cloud Services A nice cheat sheet of different cloud services (2023 edition) What is cloud native? Developer productivity tools Visualize JSON files Automatically turn code into architecture diagrams Linux Linux file system explained 18 Most-used Linux Commands You Should Know Security How does HTTPS work? Oauth 2.0 Explained With Simple Terms. Top 4 Forms of Authentication Mechanisms Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? How to store passwords safely in the database and how to validate a password? Explaining JSON Web Token (JWT) to a 10 year old Kid How does Google Authenticator (or other types of 2-factor authenticators) work? Real World Case Studies Netflix's Tech Stack Twitter Architecture 2022 Evolution of Airbnb’s microservice architecture over the past 15 years Monorepo vs. Microrepo. How will you design the Stack Overflow website? Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? How does Disney Hotstar capture 5 Billion Emojis during a tournament? How Discord Stores Trillions Of Messages How do video live streamings work on YouTube, TikTok live, or Twitch? License      README.md            【        👨🏻‍💻 YouTube    |         📮 Newsletter    】  System Design 101 Explain complex systems using visuals and simple terms. Whether you're preparing for a System Design Interview or you simply want to understand how systems work beneath the surface, we hope this repository will help you achieve that. Table of Contents  Communication protocols  REST API vs. GraphQL How does gRPC work? What is a webhook? How to improve API performance? HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) SOAP vs REST vs GraphQL vs RPC Code First vs. API First HTTP status codes What does API gateway do? How do we design effective and safe APIs? TCP/IP encapsulation Why is Nginx called a “reverse” proxy? What are the common load-balancing algorithms? URL, URI, URN - Do you know the differences?   CI/CD  CI/CD Pipeline Explained in Simple Terms Netflix Tech Stack (CI/CD Pipeline)   Architecture patterns  MVC, MVP, MVVM, MVVM-C, and VIPER 18 Key Design Patterns Every Developer Should Know   Database  A nice cheat sheet of different databases in cloud services 8 Data Structures That Power Your Databases How is an SQL statement executed in the database? CAP theorem Types of Memory and Storage Visualizing a SQL query SQL language   Cache  Data is cached everywhere Why is Redis so fast? How can Redis be used? Top caching strategies   Microservice architecture  What does a typical microservice architecture look like? Microservice Best Practices What tech stack is commonly used for microservices? Why is Kafka fast   Payment systems  How to learn payment systems? Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? How does VISA work when we swipe a credit card at a merchant’s shop? Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India   DevOps  DevOps vs. SRE vs. Platform Engineering. What is the difference? What is k8s (Kubernetes)? Docker vs. Kubernetes. Which one should we use? How does Docker work?   GIT  How Git Commands work How does Git Work? Git merge vs. Git rebase   Cloud Services  A nice cheat sheet of different cloud services (2023 edition) What is cloud native?   Developer productivity tools  Visualize JSON files Automatically turn code into architecture diagrams   Linux  Linux file system explained 18 Most-used Linux Commands You Should Know   Security  How does HTTPS work? Oauth 2.0 Explained With Simple Terms. Top 4 Forms of Authentication Mechanisms Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? How to store passwords safely in the database and how to validate a password? Explaining JSON Web Token (JWT) to a 10 year old Kid How does Google Authenticator (or other types of 2-factor authenticators) work?   Real World Case Studies  Netflix's Tech Stack Twitter Architecture 2022 Evolution of Airbnb’s microservice architecture over the past 15 years Monorepo vs. Microrepo. How will you design the Stack Overflow website? Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? How does Disney Hotstar capture 5 Billion Emojis during a tournament? How Discord Stores Trillions Of Messages How do video live streamings work on YouTube, TikTok live, or Twitch?    Communication protocols Architecture styles define how different components of an application programming interface (API) interact with one another. As a result, they ensure efficiency, reliability, and ease of integration with other systems by providing a standard approach to designing and building APIs. Here are the most used styles:      SOAP:  Mature, comprehensive, XML-based Best for enterprise applications    RESTful:  Popular, easy-to-implement, HTTP methods  Ideal for web services    GraphQL:  Query language, request specific data  Reduces network overhead, faster responses    gRPC:  Modern, high-performance, Protocol Buffers  Suitable for microservices architectures    WebSocket:  Real-time, bidirectional, persistent connections  Perfect for low-latency data exchange    Webhook:  Event-driven, HTTP callbacks, asynchronous  Notifies systems when events occur   REST API vs. GraphQL The diagram below shows a quick comparison between REST and GraphQL.      GraphQL is a query language for APIs developed by Meta. It provides a complete description of the data in the API and gives clients the power to ask for exactly what they need.   GraphQL servers sit in between the client and the backend services. GraphQL can aggregate multiple REST requests into one query. GraphQL server organizes the resources in a graph.   GraphQL supports queries, mutations (applying data modifications to resources), and subscriptions (receiving notifications on schema modifications).   How does gRPC work?    RPC (Remote Procedure Call) is called “remote” because it enables communications between remote services when services are deployed to different servers under microservice architecture. From the user’s point of view, it acts like a local function call. The diagram below illustrates the overall data flow for gRPC. Step 1: A REST call is made from the client. The request body is usually in JSON format. Steps 2 - 4: The order service (gRPC client) receives the REST call, transforms it, and makes an RPC call to the payment service. gPRC encodes the client stub into a binary format and sends it to the low-level transport layer. Step 5: gRPC sends the packets over the network via HTTP2. Because of binary encoding and network optimizations, gRPC is said to be 5X faster than JSON. Steps 6 - 8: The payment service (gRPC server) receives the packets from the network, decodes them, and invokes the server application. Steps 9 - 11: The result is returned from the server application, and gets encoded and sent to the transport layer. Steps 12 - 14: The order service receives the packets, decodes them, and sends the result to the client application. What is a webhook? The diagram below shows a comparison between polling and Webhook.     Assume we run an eCommerce website. The clients send orders to the order service via the API gateway, which goes to the payment service for payment transactions. The payment service then talks to an external payment service provider (PSP) to complete the transactions.  There are two ways to handle communications with the external PSP.  1. Short polling  After sending the payment request to the PSP, the payment service keeps asking the PSP about the payment status. After several rounds, the PSP finally returns with the status.  Short polling has two drawbacks:   Constant polling of the status requires resources from the payment service.  The External service communicates directly with the payment service, creating security vulnerabilities.   2. Webhook  We can register a webhook with the external service. It means: call me back at a certain URL when you have updates on the request. When the PSP has completed the processing, it will invoke the HTTP request to update the payment status. In this way, the programming paradigm is changed, and the payment service doesn’t need to waste resources to poll the payment status anymore. What if the PSP never calls back? We can set up a housekeeping job to check payment status every hour. Webhooks are often referred to as reverse APIs or push APIs because the server sends HTTP requests to the client. We need to pay attention to 3 things when using a webhook:  We need to design a proper API for the external service to call. We need to set up proper rules in the API gateway for security reasons. We need to register the correct URL at the external service.  How to improve API performance? The diagram below shows 5 common tricks to improve API performance.    Pagination This is a common optimization when the size of the result is large. The results are streaming back to the client to improve the service responsiveness. Asynchronous Logging Synchronous logging deals with the disk for every call and can slow down the system. Asynchronous logging sends logs to a lock-free buffer first and immediately returns. The logs will be flushed to the disk periodically. This significantly reduces the I/O overhead. Caching We can cache frequently accessed data into a cache. The client can query the cache first instead of visiting the database directly. If there is a cache miss, the client can query from the database. Caches like Redis store data in memory, so the data access is much faster than the database. Payload Compression The requests and responses can be compressed using gzip etc so that the transmitted data size is much smaller. This speeds up the upload and download. Connection Pool When accessing resources, we often need to load data from the database. Opening the closing db connections adds significant overhead. So we should connect to the db via a pool of open connections. The connection pool is responsible for managing the connection lifecycle. HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) What problem does each generation of HTTP solve? The diagram below illustrates the key features.      HTTP 1.0 was finalized and fully documented in 1996. Every request to the same server requires a separate TCP connection.   HTTP 1.1 was published in 1997. A TCP connection can be left open for reuse (persistent connection), but it doesn’t solve the HOL (head-of-line) blocking issue. HOL blocking - when the number of allowed parallel requests in the browser is used up, subsequent requests need to wait for the former ones to complete.   HTTP 2.0 was published in 2015. It addresses HOL issue through request multiplexing, which eliminates HOL blocking at the application layer, but HOL still exists at the transport (TCP) layer. As you can see in the diagram, HTTP 2.0 introduced the concept of HTTP “streams”: an abstraction that allows multiplexing different HTTP exchanges onto the same TCP connection. Each stream doesn’t need to be sent in order.   HTTP 3.0 first draft was published in 2020. It is the proposed successor to HTTP 2.0. It uses QUIC instead of TCP for the underlying transport protocol, thus removing HOL blocking in the transport layer.   QUIC is based on UDP. It introduces streams as first-class citizens at the transport layer. QUIC streams share the same QUIC connection, so no additional handshakes and slow starts are required to create new ones, but QUIC streams are delivered independently such that in most cases packet loss affecting one stream doesn't affect others. SOAP vs REST vs GraphQL vs RPC The diagram below illustrates the API timeline and API styles comparison. Over time, different API architectural styles are released. Each of them has its own patterns of standardizing data exchange. You can check out the use cases of each style in the diagram.    Code First vs. API First The diagram below shows the differences between code-first development and API-first development. Why do we want to consider API first design?     Microservices increase system complexity We have separate services to serve different functions of the system. While this kind of architecture facilitates decoupling and segregation of duty, we need to handle the various communications among services.  It is better to think through the system's complexity before writing the code and carefully defining the boundaries of the services.  Separate functional teams need to speak the same language The dedicated functional teams are only responsible for their own components and services. It is recommended that the organization speak the same language via API design.  We can mock requests and responses to validate the API design before writing code.  Improve software quality and developer productivity Since we have ironed out most of the uncertainties when the project starts, the overall development process is smoother, and the software quality is greatly improved.  Developers are happy about the process as well because they can focus on functional development instead of negotiating sudden changes. The possibility of having surprises toward the end of the project lifecycle is reduced. Because we have designed the API first, the tests can be designed while the code is being developed. In a way, we also have TDD (Test Driven Design) when using API first development. HTTP status codes    The response codes for HTTP are divided into five categories: Informational (100-199) Success (200-299) Redirection (300-399) Client Error (400-499) Server Error (500-599) What does API gateway do? The diagram below shows the details.    Step 1 - The client sends an HTTP request to the API gateway. Step 2 - The API gateway parses and validates the attributes in the HTTP request. Step 3 - The API gateway performs allow-list/deny-list checks. Step 4 - The API gateway talks to an identity provider for authentication and authorization. Step 5 - The rate limiting rules are applied to the request. If it is over the limit, the request is rejected. Steps 6 and 7 - Now that the request has passed basic checks, the API gateway finds the relevant service to route to by path matching. Step 8 - The API gateway transforms the request into the appropriate protocol and sends it to backend microservices. Steps 9-12: The API gateway can handle errors properly, and deals with faults if the error takes a longer time to recover (circuit break). It can also leverage ELK (Elastic-Logstash-Kibana) stack for logging and monitoring. We sometimes cache data in the API gateway. How do we design effective and safe APIs? The diagram below shows typical API designs with a shopping cart example.    Note that API design is not just URL path design. Most of the time, we need to choose the proper resource names, identifiers, and path patterns. It is equally important to design proper HTTP header fields or to design effective rate-limiting rules within the API gateway. TCP/IP encapsulation How is data sent over the network? Why do we need so many layers in the OSI model?    The diagram below shows how data is encapsulated and de-encapsulated when transmitting over the network. Step 1: When Device A sends data to Device B over the network via the HTTP protocol, it is first added an HTTP header at the application layer. Step 2: Then a TCP or a UDP header is added to the data. It is encapsulated into TCP segments at the transport layer. The header contains the source port, destination port, and sequence number. Step 3: The segments are then encapsulated with an IP header at the network layer. The IP header contains the source/destination IP addresses. Step 4: The IP datagram is added a MAC header at the data link layer, with source/destination MAC addresses. Step 5: The encapsulated frames are sent to the physical layer and sent over the network in binary bits. Steps 6-10: When Device B receives the bits from the network, it performs the de-encapsulation process, which is a reverse processing of the encapsulation process. The headers are removed layer by layer, and eventually, Device B can read the data. We need layers in the network model because each layer focuses on its own responsibilities. Each layer can rely on the headers for processing instructions and does not need to know the meaning of the data from the last layer. Why is Nginx called a “reverse” proxy? The diagram below shows the differences between a 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐱𝐲 and a 𝐫𝐞𝐯𝐞𝐫𝐬𝐞 𝐩𝐫𝐨𝐱𝐲.    A forward proxy is a server that sits between user devices and the internet. A forward proxy is commonly used for:  Protect clients Avoid browsing restrictions Block access to certain content  A reverse proxy is a server that accepts a request from the client, forwards the request to web servers, and returns the results to the client as if the proxy server had processed the request. A reverse proxy is good for:  Protect servers Load balancing Cache static contents Encrypt and decrypt SSL communications  What are the common load-balancing algorithms? The diagram below shows 6 common algorithms.     Static Algorithms    Round robin The client requests are sent to different service instances in sequential order. The services are usually required to be stateless.   Sticky round-robin This is an improvement of the round-robin algorithm. If Alice’s first request goes to service A, the following requests go to service A as well.   Weighted round-robin The admin can specify the weight for each service. The ones with a higher weight handle more requests than others.   Hash This algorithm applies a hash function on the incoming requests’ IP or URL. The requests are routed to relevant instances based on the hash function result.   Dynamic Algorithms Least connections A new request is sent to the service instance with the least concurrent connections. Least response time A new request is sent to the service instance with the fastest response time. URL, URI, URN - Do you know the differences? The diagram below shows a comparison of URL, URI, and URN.     URI  URI stands for Uniform Resource Identifier. It identifies a logical or physical resource on the web. URL and URN are subtypes of URI. URL locates a resource, while URN names a resource. A URI is composed of the following parts: scheme:[//authority]path[?query][#fragment]  URL  URL stands for Uniform Resource Locator, the key concept of HTTP. It is the address of a unique resource on the web. It can be used with other protocols like FTP and JDBC.  URN  URN stands for Uniform Resource Name. It uses the urn scheme. URNs cannot be used to locate a resource. A simple example given in the diagram is composed of a namespace and a namespace-specific string. If you would like to learn more detail on the subject, I would recommend W3C’s clarification. CI/CD CI/CD Pipeline Explained in Simple Terms    Section 1 - SDLC with CI/CD The software development life cycle (SDLC) consists of several key stages: development, testing, deployment, and maintenance. CI/CD automates and integrates these stages to enable faster and more reliable releases. When code is pushed to a git repository, it triggers an automated build and test process. End-to-end (e2e) test cases are run to validate the code. If tests pass, the code can be automatically deployed to staging/production. If issues are found, the code is sent back to development for bug fixing. This automation provides fast feedback to developers and reduces the risk of bugs in production. Section 2 - Difference between CI and CD Continuous Integration (CI) automates the build, test, and merge process. It runs tests whenever code is committed to detect integration issues early. This encourages frequent code commits and rapid feedback. Continuous Delivery (CD) automates release processes like infrastructure changes and deployment. It ensures software can be released reliably at any time through automated workflows. CD may also automate the manual testing and approval steps required before production deployment. Section 3 - CI/CD Pipeline A typical CI/CD pipeline has several connected stages:  The developer commits code changes to the source control CI server detects changes and triggers the build Code is compiled, and tested (unit, integration tests) Test results reported to the developer On success, artifacts are deployed to staging environments Further testing may be done on staging before release CD system deploys approved changes to production  Netflix Tech Stack (CI/CD Pipeline)    Planning: Netflix Engineering uses JIRA for planning and Confluence for documentation. Coding: Java is the primary programming language for the backend service, while other languages are used for different use cases. Build: Gradle is mainly used for building, and Gradle plugins are built to support various use cases. Packaging: Package and dependencies are packed into an Amazon Machine Image (AMI) for release. Testing: Testing emphasizes the production culture's focus on building chaos tools. Deployment: Netflix uses its self-built Spinnaker for canary rollout deployment. Monitoring: The monitoring metrics are centralized in Atlas, and Kayenta is used to detect anomalies. Incident report: Incidents are dispatched according to priority, and PagerDuty is used for incident handling. Architecture patterns MVC, MVP, MVVM, MVVM-C, and VIPER These architecture patterns are among the most commonly used in app development, whether on iOS or Android platforms. Developers have introduced them to overcome the limitations of earlier patterns. So, how do they differ?     MVC, the oldest pattern, dates back almost 50 years Every pattern has a "view" (V) responsible for displaying content and receiving user input Most patterns include a "model" (M) to manage business data "Controller," "presenter," and "view-model" are translators that mediate between the view and the model ("entity" in the VIPER pattern)  18 Key Design Patterns Every Developer Should Know Patterns are reusable solutions to common design problems, resulting in a smoother, more efficient development process. They serve as blueprints for building better software structures. These are some of the most popular patterns:     Abstract Factory: Family Creator - Makes groups of related items. Builder: Lego Master - Builds objects step by step, keeping creation and appearance separate. Prototype: Clone Maker - Creates copies of fully prepared examples. Singleton: One and Only - A special class with just one instance. Adapter: Universal Plug - Connects things with different interfaces. Bridge: Function Connector - Links how an object works to what it does. Composite: Tree Builder - Forms tree-like structures of simple and complex parts. Decorator: Customizer - Adds features to objects without changing their core. Facade: One-Stop-Shop - Represents a whole system with a single, simplified interface. Flyweight: Space Saver - Shares small, reusable items efficiently. Proxy: Stand-In Actor - Represents another object, controlling access or actions. Chain of Responsibility: Request Relay - Passes a request through a chain of objects until handled. Command: Task Wrapper - Turns a request into an object, ready for action. Iterator: Collection Explorer - Accesses elements in a collection one by one. Mediator: Communication Hub - Simplifies interactions between different classes. Memento: Time Capsule - Captures and restores an object's state. Observer: News Broadcaster - Notifies classes about changes in other objects. Visitor: Skillful Guest - Adds new operations to a class without altering it.  Database A nice cheat sheet of different databases in cloud services    Choosing the right database for your project is a complex task. Many database options, each suited to distinct use cases, can quickly lead to decision fatigue. We hope this cheat sheet provides high-level direction to pinpoint the right service that aligns with your project's needs and avoid potential pitfalls. Note: Google has limited documentation for their database use cases. Even though we did our best to look at what was available and arrived at the best option, some of the entries may need to be more accurate. 8 Data Structures That Power Your Databases The answer will vary depending on your use case. Data can be indexed in memory or on disk. Similarly, data formats vary, such as numbers, strings, geographic coordinates, etc. The system might be write-heavy or read-heavy. All of these factors affect your choice of database index format.    The following are some of the most popular data structures used for indexing data:  Skiplist: a common in-memory index type. Used in Redis Hash index: a very common implementation of the “Map” data structure (or “Collection”) SSTable: immutable on-disk “Map” implementation LSM tree: Skiplist + SSTable. High write throughput B-tree: disk-based solution. Consistent read/write performance Inverted index: used for document indexing. Used in Lucene Suffix tree: for string pattern search R-tree: multi-dimension search, such as finding the nearest neighbor  How is an SQL statement executed in the database? The diagram below shows the process. Note that the architectures for different databases are different, the diagram demonstrates some common designs.    Step 1 - A SQL statement is sent to the database via a transport layer protocol (e.g.TCP). Step 2 - The SQL statement is sent to the command parser, where it goes through syntactic and semantic analysis, and a query tree is generated afterward. Step 3 - The query tree is sent to the optimizer. The optimizer creates an execution plan. Step 4 - The execution plan is sent to the executor. The executor retrieves data from the execution. Step 5 - Access methods provide the data fetching logic required for execution, retrieving data from the storage engine. Step 6 - Access methods decide whether the SQL statement is read-only. If the query is read-only (SELECT statement), it is passed to the buffer manager for further processing. The buffer manager looks for the data in the cache or data files. Step 7 - If the statement is an UPDATE or INSERT, it is passed to the transaction manager for further processing. Step 8 - During a transaction, the data is in lock mode. This is guaranteed by the lock manager. It also ensures the transaction’s ACID properties. CAP theorem The CAP theorem is one of the most famous terms in computer science, but I bet different developers have different understandings. Let’s examine what it is and why it can be confusing.    CAP theorem states that a distributed system can't provide more than two of these three guarantees simultaneously. Consistency: consistency means all clients see the same data at the same time no matter which node they connect to. Availability: availability means any client that requests data gets a response even if some of the nodes are down. Partition Tolerance: a partition indicates a communication break between two nodes. Partition tolerance means the system continues to operate despite network partitions. The “2 of 3” formulation can be useful, but this simplification could be misleading.   Picking a database is not easy. Justifying our choice purely based on the CAP theorem is not enough. For example, companies don't choose Cassandra for chat applications simply because it is an AP system. There is a list of good characteristics that make Cassandra a desirable option for storing chat messages. We need to dig deeper.   “CAP prohibits only a tiny part of the design space: perfect availability and consistency in the presence of partitions, which are rare”. Quoted from the paper: CAP Twelve Years Later: How the “Rules” Have Changed.   The theorem is about 100% availability and consistency. A more realistic discussion would be the trade-offs between latency and consistency when there is no network partition. See PACELC theorem for more details.   Is the CAP theorem actually useful? I think it is still useful as it opens our minds to a set of tradeoff discussions, but it is only part of the story. We need to dig deeper when picking the right database. Types of Memory and Storage    Visualizing a SQL query    SQL statements are executed by the database system in several steps, including:  Parsing the SQL statement and checking its validity Transforming the SQL into an internal representation, such as relational algebra Optimizing the internal representation and creating an execution plan that utilizes index information Executing the plan and returning the results  The execution of SQL is highly complex and involves many considerations, such as:  The use of indexes and caches The order of table joins Concurrency control Transaction management  SQL language In 1986, SQL (Structured Query Language) became a standard. Over the next 40 years, it became the dominant language for relational database management systems. Reading the latest standard (ANSI SQL 2016) can be time-consuming. How can I learn it?    There are 5 components of the SQL language:  DDL: data definition language, such as CREATE, ALTER, DROP DQL: data query language, such as SELECT DML: data manipulation language, such as INSERT, UPDATE, DELETE DCL: data control language, such as GRANT, REVOKE TCL: transaction control language, such as COMMIT, ROLLBACK  For a backend engineer, you may need to know most of it. As a data analyst, you may need to have a good understanding of DQL. Select the topics that are most relevant to you. Cache Data is cached everywhere This diagram illustrates where we cache data in a typical architecture.    There are multiple layers along the flow.  Client apps: HTTP responses can be cached by the browser. We request data over HTTP for the first time, and it is returned with an expiry policy in the HTTP header; we request data again, and the client app tries to retrieve the data from the browser cache first. CDN: CDN caches static web resources. The clients can retrieve data from a CDN node nearby. Load Balancer: The load Balancer can cache resources as well. Messaging infra: Message brokers store messages on disk first, and then consumers retrieve them at their own pace. Depending on the retention policy, the data is cached in Kafka clusters for a period of time. Services: There are multiple layers of cache in a service. If the data is not cached in the CPU cache, the service will try to retrieve the data from memory. Sometimes the service has a second-level cache to store data on disk. Distributed Cache: Distributed cache like Redis holds key-value pairs for multiple services in memory. It provides much better read/write performance than the database. Full-text Search: we sometimes need to use full-text searches like Elastic Search for document search or log search. A copy of data is indexed in the search engine as well. Database: Even in the database, we have different levels of caches:   WAL(Write-ahead Log): data is written to WAL first before building the B tree index Bufferpool: A memory area allocated to cache query results Materialized View: Pre-compute query results and store them in the database tables for better query performance Transaction log: record all the transactions and database updates Replication Log: used to record the replication state in a database cluster  Why is Redis so fast? There are 3 main reasons as shown in the diagram below.     Redis is a RAM-based data store. RAM access is at least 1000 times faster than random disk access. Redis leverages IO multiplexing and single-threaded execution loop for execution efficiency. Redis leverages several efficient lower-level data structures.  Question: Another popular in-memory store is Memcached. Do you know the differences between Redis and Memcached? You might have noticed the style of this diagram is different from my previous posts. Please let me know which one you prefer. How can Redis be used?    There is more to Redis than just caching. Redis can be used in a variety of scenarios as shown in the diagram.   Session We can use Redis to share user session data among different services.   Cache We can use Redis to cache objects or pages, especially for hotspot data.   Distributed lock We can use a Redis string to acquire locks among distributed services.   Counter We can count how many likes or how many reads for articles.   Rate limiter We can apply a rate limiter for certain user IPs.   Global ID generator We can use Redis Int for global ID.   Shopping cart We can use Redis Hash to represent key-value pairs in a shopping cart.   Calculate user retention We can use Bitmap to represent the user login daily and calculate user retention.   Message queue We can use List for a message queue.   Ranking We can use ZSet to sort the articles.   Top caching strategies Designing large-scale systems usually requires careful consideration of caching. Below are five caching strategies that are frequently utilized.    Microservice architecture What does a typical microservice architecture look like?    The diagram below shows a typical microservice architecture.  Load Balancer: This distributes incoming traffic across multiple backend services. CDN (Content Delivery Network): CDN is a group of geographically distributed servers that hold static content for faster delivery. The clients look for content in CDN first, then progress  to backend services. API Gateway: This handles incoming requests and routes them to the relevant services. It talks to the identity provider and service discovery. Identity Provider: This handles authentication and authorization for users. Service Registry &amp; Discovery: Microservice registration and discovery happen in this component, and the API gateway looks for relevant services in this component to talk to. Management: This component is responsible for monitoring the services. Microservices: Microservices are designed and deployed in different domains. Each domain has its own database. The API gateway talks to the microservices via REST API or other protocols, and the microservices within the same domain talk to each other using RPC (Remote Procedure Call).  Benefits of microservices:  They can be quickly designed, deployed, and horizontally scaled. Each domain can be independently maintained by a dedicated team. Business requirements can be customized in each domain and better supported, as a result.  Microservice Best Practices A picture is worth a thousand words: 9 best practices for developing microservices.    When we develop microservices, we need to follow the following best practices:  Use separate data storage for each microservice Keep code at a similar level of maturity Separate build for each microservice Assign each microservice with a single responsibility Deploy into containers Design stateless services Adopt domain-driven design Design micro frontend Orchestrating microservices  What tech stack is commonly used for microservices? Below you will find a diagram showing the microservice tech stack, both for the development phase and for production.    ▶️ 𝐏𝐫𝐞-𝐏𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧  Define API - This establishes a contract between frontend and backend. We can use Postman or OpenAPI for this. Development - Node.js or react is popular for frontend development, and java/python/go for backend development. Also, we need to change the configurations in the API gateway according to API definitions. Continuous Integration - JUnit and Jenkins for automated testing. The code is packaged into a Docker image and deployed as microservices.  ▶️ 𝐏𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧  NGinx is a common choice for load balancers. Cloudflare provides CDN (Content Delivery Network). API Gateway - We can use spring boot for the gateway, and use Eureka/Zookeeper for service discovery. The microservices are deployed on clouds. We have options among AWS, Microsoft Azure, or Google GCP. Cache and Full-text Search - Redis is a common choice for caching key-value pairs. ElasticSearch is used for full-text search. Communications - For services to talk to each other, we can use messaging infra Kafka or RPC. Persistence - We can use MySQL or PostgreSQL for a relational database, and Amazon S3 for object store. We can also use Cassandra for the wide-column store if necessary. Management &amp; Monitoring - To manage so many microservices, the common Ops tools include Prometheus, Elastic Stack, and Kubernetes.  Why is Kafka fast There are many design decisions that contributed to Kafka’s performance. In this post, we’ll focus on two. We think these two carried the most weight.     The first one is Kafka’s reliance on Sequential I/O. The second design choice that gives Kafka its performance advantage is its focus on efficiency: zero copy principle.  The diagram illustrates how the data is transmitted between producer and consumer, and what zero-copy means.  Step 1.1 - 1.3: Producer writes data to the disk Step 2: Consumer reads data without zero-copy  2.1 The data is loaded from disk to OS cache 2.2 The data is copied from OS cache to Kafka application 2.3 Kafka application copies the data into the socket buffer 2.4 The data is copied from socket buffer to network card 2.5 The network card sends data out to the consumer  Step 3: Consumer reads data with zero-copy  3.1: The data is loaded from disk to OS cache 3.2 OS cache directly copies the data to the network card via sendfile() command 3.3 The network card sends data out to the consumer Zero copy is a shortcut to save the multiple data copies between application context and kernel context. Payment systems How to learn payment systems?    Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? The diagram below shows the economics of the credit card payment flow.    1.  The cardholder pays a merchant $100 to buy a product. 2. The merchant benefits from the use of the credit card with higher sales volume and needs to compensate the issuer and the card network for providing the payment service. The acquiring bank sets a fee with the merchant, called the “merchant discount fee.” 3 - 4. The acquiring bank keeps $0.25 as the acquiring markup, and $1.75 is paid to the issuing bank as the interchange fee. The merchant discount fee should cover the interchange fee. The interchange fee is set by the card network because it is less efficient for each issuing bank to negotiate fees with each merchant. 5.  The card network sets up the network assessments and fees with each bank, which pays the card network for its services every month. For example, VISA charges a 0.11% assessment, plus a $0.0195 usage fee, for every swipe. 6.  The cardholder pays the issuing bank for its services. Why should the issuing bank be compensated?  The issuer pays the merchant even if the cardholder fails to pay the issuer. The issuer pays the merchant before the cardholder pays the issuer. The issuer has other operating costs, including managing customer accounts, providing statements, fraud detection, risk management, clearing &amp; settlement, etc.  How does VISA work when we swipe a credit card at a merchant’s shop?    VISA, Mastercard, and American Express act as card networks for the clearing and settling of funds. The card acquiring bank and the card issuing bank can be – and often are – different. If banks were to settle transactions one by one without an intermediary, each bank would have to settle the transactions with all the other banks. This is quite inefficient. The diagram below shows VISA’s role in the credit card payment process. There are two flows involved. Authorization flow happens when the customer swipes the credit card. Capture and settlement flow happens when the merchant wants to get the money at the end of the day.  Authorization Flow  Step 0: The card issuing bank issues credit cards to its customers. Step 1: The cardholder wants to buy a product and swipes the credit card at the Point of Sale (POS) terminal in the merchant’s shop. Step 2: The POS terminal sends the transaction to the acquiring bank, which has provided the POS terminal. Steps 3 and 4: The acquiring bank sends the transaction to the card network, also called the card scheme. The card network sends the transaction to the issuing bank for approval. Steps 4.1, 4.2 and 4.3: The issuing bank freezes the money if the transaction is approved. The approval or rejection is sent back to the acquirer, as well as the POS terminal.  Capture and Settlement Flow  Steps 1 and 2: The merchant wants to collect the money at the end of the day, so they hit ”capture” on the POS terminal. The transactions are sent to the acquirer in batch. The acquirer sends the batch file with transactions to the card network. Step 3: The card network performs clearing for the transactions collected from different acquirers, and sends the clearing files to different issuing banks. Step 4: The issuing banks confirm the correctness of the clearing files, and transfer money to the relevant acquiring banks. Step 5: The acquiring bank then transfers money to the merchant’s bank. Step 4: The card network clears the transactions from different acquiring banks. Clearing is a process in which mutual offset transactions are netted, so the number of total transactions is reduced. In the process, the card network takes on the burden of talking to each bank and receives service fees in return. Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India What’s UPI? UPI is an instant real-time payment system developed by the National Payments Corporation of India. It accounts for 60% of digital retail transactions in India today. UPI = payment markup language + standard for interoperable payments    DevOps DevOps vs. SRE vs. Platform Engineering. What is the difference? The concepts of DevOps, SRE, and Platform Engineering have emerged at different times and have been developed by various individuals and organizations.    DevOps as a concept was introduced in 2009 by Patrick Debois and Andrew Shafer at the Agile conference. They sought to bridge the gap between software development and operations by promoting a collaborative culture and shared responsibility for the entire software development lifecycle. SRE, or Site Reliability Engineering, was pioneered by Google in the early 2000s to address operational challenges in managing large-scale, complex systems. Google developed SRE practices and tools, such as the Borg cluster management system and the Monarch monitoring system, to improve the reliability and efficiency of their services. Platform Engineering is a more recent concept, building on the foundation of SRE engineering. The precise origins of Platform Engineering are less clear, but it is generally understood to be an extension of the DevOps and SRE practices, with a focus on delivering a comprehensive platform for product development that supports the entire business perspective. It's worth noting that while these concepts emerged at different times. They are all related to the broader trend of improving collaboration, automation, and efficiency in software development and operations. What is k8s (Kubernetes)? K8s is a container orchestration system. It is used for container deployment and management. Its design is greatly impacted by Google’s internal system Borg.    A k8s cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers, and a cluster usually runs multiple nodes, providing fault tolerance and high availability.  Control Plane Components    API Server The API server talks to all the components in the k8s cluster. All the operations on pods are executed by talking to the API server.   Scheduler The scheduler watches pod workloads and assigns loads on newly created pods.   Controller Manager The controller manager runs the controllers, including Node Controller, Job Controller, EndpointSlice Controller, and ServiceAccount Controller.   Etcd etcd is a key-value store used as Kubernetes' backing store for all cluster data.    Nodes    Pods A pod is a group of containers and is the smallest unit that k8s administers. Pods have a single IP address applied to every container within the pod.   Kubelet An agent that runs on each node in the cluster. It ensures containers are running in a Pod.   Kube Proxy Kube-proxy is a network proxy that runs on each node in your cluster. It routes traffic coming into a node from the service. It forwards requests for work to the correct containers.   Docker vs. Kubernetes. Which one should we use?    What is Docker ? Docker is an open-source platform that allows you to package, distribute, and run applications in isolated containers. It focuses on containerization, providing lightweight environments that encapsulate applications and their dependencies. What is Kubernetes ? Kubernetes, often referred to as K8s, is an open-source container orchestration platform. It provides a framework for automating the deployment, scaling, and management of containerized applications across a cluster of nodes. How are both different from each other ? Docker: Docker operates at the individual container level on a single operating system host. You must manually manage each host and setting up networks, security policies, and storage for multiple related containers can be complex. Kubernetes: Kubernetes operates at the cluster level. It manages multiple containerized applications across multiple hosts, providing automation for tasks like load balancing, scaling, and ensuring the desired state of applications. In short, Docker focuses on containerization and running containers on individual hosts, while Kubernetes specializes in managing and orchestrating containers at scale across a cluster of hosts. How does Docker work? The diagram below shows the architecture of Docker and how it works when we run “docker build”, “docker pull” and “docker run”.    There are 3 components in Docker architecture:   Docker client The docker client talks to the Docker daemon.   Docker host The Docker daemon listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.   Docker registry A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use.   Let’s take the “docker run” command as an example.  Docker pulls the image from the registry. Docker creates a new container. Docker allocates a read-write filesystem to the container. Docker creates a network interface to connect the container to the default network. Docker starts the container.  GIT How Git Commands work To begin with, it's essential to identify where our code is stored. The common assumption is that there are only two locations - one on a remote server like Github and the other on our local machine. However, this isn't entirely accurate. Git maintains three local storages on our machine, which means that our code can be found in four places:     Working directory: where we edit files Staging area: a temporary location where files are kept for the next commit Local repository: contains the code that has been committed Remote repository: the remote server that stores the code  Most Git commands primarily move files between these four locations. How does Git Work? The diagram below shows the Git workflow.    Git is a distributed version control system. Every developer maintains a local copy of the main repository and edits and commits to the local copy. The commit is very fast because the operation doesn’t interact with the remote repository. If the remote repository crashes, the files can be recovered from the local repositories. Git merge vs. Git rebase What are the differences?    When we merge changes from one Git branch to another, we can use ‘git merge’ or ‘git rebase’. The diagram below shows how the two commands work. Git merge This creates a new commit G’ in the main branch. G’ ties the histories of both main and feature branches. Git merge is non-destructive. Neither the main nor the feature branch is changed. Git rebase Git rebase moves the feature branch histories to the head of the main branch. It creates new commits E’, F’, and G’ for each commit in the feature branch. The benefit of rebase is that it has a linear commit history. Rebase can be dangerous if “the golden rule of git rebase” is not followed. The Golden Rule of Git Rebase Never use it on public branches! Cloud Services A nice cheat sheet of different cloud services (2023 edition)    What is cloud native? Below is a diagram showing the evolution of architecture and processes since the 1980s.    Organizations can build and run scalable applications on public, private, and hybrid clouds using cloud native technologies. This means the applications are designed to leverage cloud features, so they are resilient to load and easy to scale. Cloud native includes 4 aspects:   Development process This has progressed from waterfall to agile to DevOps.   Application Architecture The architecture has gone from monolithic to microservices. Each service is designed to be small, adaptive to the limited resources in cloud containers.   Deployment &amp; packaging The applications used to be deployed on physical servers. Then around 2000, the applications that were not sensitive to latency were usually deployed on virtual servers. With cloud native applications, they are packaged into docker images and deployed in containers.   Application infrastructure The applications are massively deployed on cloud infrastructure instead of self-hosted servers.   Developer productivity tools Visualize JSON files Nested JSON files are hard to read. JsonCrack generates graph diagrams from JSON files and makes them easy to read. Additionally, the generated diagrams can be downloaded as images.    Automatically turn code into architecture diagrams    What does it do?  Draw the cloud system architecture in Python code. Diagrams can also be rendered directly inside the Jupyter Notebooks. No design tools are needed. Supports the following providers: AWS, Azure, GCP, Kubernetes, Alibaba Cloud, Oracle Cloud, etc.  Github repo Linux Linux file system explained    The Linux file system used to resemble an unorganized town where individuals constructed their houses wherever they pleased. However, in 1994, the Filesystem Hierarchy Standard (FHS) was introduced to bring order to the Linux file system. By implementing a standard like the FHS, software can ensure a consistent layout across various Linux distributions. Nonetheless, not all Linux distributions strictly adhere to this standard. They often incorporate their own unique elements or cater to specific requirements. To become proficient in this standard, you can begin by exploring. Utilize commands such as "cd" for navigation and "ls" for listing directory contents. Imagine the file system as a tree, starting from the root (/). With time, it will become second nature to you, transforming you into a skilled Linux administrator. 18 Most-used Linux Commands You Should Know Linux commands are instructions for interacting with the operating system. They help manage files, directories, system processes, and many other aspects of the system. You need to become familiar with these commands in order to navigate and maintain Linux-based systems efficiently and effectively. This diagram below shows popular Linux commands:     ls - List files and directories cd - Change the current directory mkdir - Create a new directory rm - Remove files or directories cp - Copy files or directories mv - Move or rename files or directories chmod - Change file or directory permissions grep - Search for a pattern in files find - Search for files and directories tar - manipulate tarball archive files vi - Edit files using text editors cat - display the content of files top - Display processes and resource usage ps - Display processes information kill - Terminate a process by sending a signal du - Estimate file space usage ifconfig - Configure network interfaces ping - Test network connectivity between hosts  Security How does HTTPS work? Hypertext Transfer Protocol Secure (HTTPS) is an extension of the Hypertext Transfer Protocol (HTTP.) HTTPS transmits encrypted data using Transport Layer Security (TLS.) If the data is hijacked online, all the hijacker gets is binary code.    How is the data encrypted and decrypted? Step 1 - The client (browser) and the server establish a TCP connection. Step 2 - The client sends a “client hello” to the server. The message contains a set of necessary encryption algorithms (cipher suites) and the latest TLS version it can support. The server responds with a “server hello” so the browser knows whether it can support the algorithms and TLS version. The server then sends the SSL certificate to the client. The certificate contains the public key, host name, expiry dates, etc. The client validates the certificate. Step 3 - After validating the SSL certificate, the client generates a session key and encrypts it using the public key. The server receives the encrypted session key and decrypts it with the private key. Step 4 - Now that both the client and the server hold the same session key (symmetric encryption), the encrypted data is transmitted in a secure bi-directional channel. Why does HTTPS switch to symmetric encryption during data transmission? There are two main reasons:   Security: The asymmetric encryption goes only one way. This means that if the server tries to send the encrypted data back to the client, anyone can decrypt the data using the public key.   Server resources: The asymmetric encryption adds quite a lot of mathematical overhead. It is not suitable for data transmissions in long sessions.   Oauth 2.0 Explained With Simple Terms. OAuth 2.0 is a powerful and secure framework that allows different applications to securely interact with each other on behalf of users without sharing sensitive credentials.    The entities involved in OAuth are the User, the Server, and the Identity Provider (IDP). What Can an OAuth Token Do? When you use OAuth, you get an OAuth token that represents your identity and permissions. This token can do a few important things: Single Sign-On (SSO): With an OAuth token, you can log into multiple services or apps using just one login, making life easier and safer. Authorization Across Systems: The OAuth token allows you to share your authorization or access rights across various systems, so you don't have to log in separately everywhere. Accessing User Profile: Apps with an OAuth token can access certain parts of your user profile that you allow, but they won't see everything. Remember, OAuth 2.0 is all about keeping you and your data safe while making your online experiences seamless and hassle-free across different applications and services. Top 4 Forms of Authentication Mechanisms      SSH Keys: Cryptographic keys are used to access remote systems and servers securely   OAuth Tokens: Tokens that provide limited access to user data on third-party applications   SSL Certificates: Digital certificates ensure secure and encrypted communication between servers and clients   Credentials: User authentication information is used to verify and grant access to various systems and services   Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? These terms are all related to user identity management. When you log into a website, you declare who you are (identification). Your identity is verified (authentication), and you are granted the necessary permissions (authorization). Many solutions have been proposed in the past, and the list keeps growing.    From simple to complex, here is my understanding of user identity management:   WWW-Authenticate is the most basic method. You are asked for the username and password by the browser. As a result of the inability to control the login life cycle, it is seldom used today.   A finer control over the login life cycle is session-cookie. The server maintains session storage, and the browser keeps the ID of the session. A cookie usually only works with browsers and is not mobile app friendly.   To address the compatibility issue, the token can be used. The client sends the token to the server, and the server validates the token. The downside is that the token needs to be encrypted and decrypted, which may be time-consuming.   JWT is a standard way of representing tokens. This information can be verified and trusted because it is digitally signed. Since JWT contains the signature, there is no need to save session information on the server side.   By using SSO (single sign-on), you can sign on only once and log in to multiple websites. It uses CAS (central authentication service) to maintain cross-site information   By using OAuth 2.0, you can authorize one website to access your information on another website.   How to store passwords safely in the database and how to validate a password?    Things NOT to do   Storing passwords in plain text is not a good idea because anyone with internal access can see them.   Storing password hashes directly is not sufficient because it is pruned to precomputation attacks, such as rainbow tables.   To mitigate precomputation attacks, we salt the passwords.   What is salt? According to OWASP guidelines, “a salt is a unique, randomly generated string that is added to each password as part of the hashing process”. How to store a password and salt?  the hash result is unique to each password. The password can be stored in the database using the following format: hash(password + salt).  How to validate a password? To validate a password, it can go through the following process:  A client enters the password. The system fetches the corresponding salt from the database. The system appends the salt to the password and hashes it. Let’s call the hashed value H1. The system compares H1 and H2, where H2 is the hash stored in the database. If they are the same, the password is valid.  Explaining JSON Web Token (JWT) to a 10 year old Kid    Imagine you have a special box called a JWT. Inside this box, there are three parts: a header, a payload, and a signature. The header is like the label on the outside of the box. It tells us what type of box it is and how it's secured. It's usually written in a format called JSON, which is just a way to organize information using curly braces { } and colons : . The payload is like the actual message or information you want to send. It could be your name, age, or any other data you want to share. It's also written in JSON format, so it's easy to understand and work with. Now, the signature is what makes the JWT secure. It's like a special seal that only the sender knows how to create. The signature is created using a secret code, kind of like a password. This signature ensures that nobody can tamper with the contents of the JWT without the sender knowing about it. When you want to send the JWT to a server, you put the header, payload, and signature inside the box. Then you send it over to the server. The server can easily read the header and payload to understand who you are and what you want to do. How does Google Authenticator (or other types of 2-factor authenticators) work? Google Authenticator is commonly used for logging into our accounts when 2-factor authentication is enabled. How does it guarantee security? Google Authenticator is a software-based authenticator that implements a two-step verification service. The diagram below provides detail.    There are two stages involved:  Stage 1 - The user enables Google two-step verification Stage 2 - The user uses the authenticator for logging in, etc.  Let’s look at these stages. Stage 1 Steps 1 and 2: Bob opens the web page to enable two-step verification. The front end requests a secret key. The authentication service generates the secret key for Bob and stores it in the database. Step 3: The authentication service returns a URI to the front end. The URI is composed of a key issuer, username, and secret key. The URI is displayed in the form of a QR code on the web page. Step 4: Bob then uses Google Authenticator to scan the generated QR code. The secret key is stored in the authenticator. Stage 2 Steps 1 and 2: Bob wants to log into a website with Google two-step verification. For this, he needs the password. Every 30 seconds, Google Authenticator generates a 6-digit password using TOTP (Time-based One Time Password) algorithm. Bob uses the password to enter the website. Steps 3 and 4: The frontend sends the password Bob enters to the backend for authentication. The authentication service reads the secret key from the database and generates a 6-digit password using the same TOTP algorithm as the client. Step 5: The authentication service compares the two passwords generated by the client and the server, and returns the comparison result to the frontend. Bob can proceed with the login process only if the two passwords match. Is this authentication mechanism safe?   Can the secret key be obtained by others? We need to make sure the secret key is transmitted using HTTPS. The authenticator client and the database store the secret key, and we need to make sure the secret keys are encrypted.   Can the 6-digit password be guessed by hackers? No. The password has 6 digits, so the generated password has 1 million potential combinations. Plus, the password changes every 30 seconds. If hackers want to guess the password in 30 seconds, they need to enter 30,000 combinations per second.   Real World Case Studies Netflix's Tech Stack This post is based on research from many Netflix engineering blogs and open-source projects. If you come across any inaccuracies, please feel free to inform us.    Mobile and web: Netflix has adopted Swift and Kotlin to build native mobile apps. For its web application, it uses React. Frontend/server communication: Netflix uses GraphQL. Backend services: Netflix relies on ZUUL, Eureka, the Spring Boot framework, and other technologies. Databases: Netflix utilizes EV cache, Cassandra, CockroachDB, and other databases. Messaging/streaming: Netflix employs Apache Kafka and Fink for messaging and streaming purposes. Video storage: Netflix uses S3 and Open Connect for video storage. Data processing: Netflix utilizes Flink and Spark for data processing, which is then visualized using Tableau. Redshift is used for processing structured data warehouse information. CI/CD: Netflix employs various tools such as JIRA, Confluence, PagerDuty, Jenkins, Gradle, Chaos Monkey, Spinnaker, Atlas, and more for CI/CD processes. Twitter Architecture 2022 Yes, this is the real Twitter architecture. It is posted by Elon Musk and redrawn by us for better readability.    Evolution of Airbnb’s microservice architecture over the past 15 years Airbnb’s microservice architecture went through 3 main stages.    Monolith (2008 - 2017) Airbnb began as a simple marketplace for hosts and guests. This is built in a Ruby on Rails application - the monolith. What’s the challenge?  Confusing team ownership + unowned code Slow deployment  Microservices (2017 - 2020) Microservice aims to solve those challenges. In the microservice architecture, key services include:  Data fetching service Business logic data service Write workflow service UI aggregation service Each service had one owning team  What’s the challenge? Hundreds of services and dependencies were difficult for humans to manage. Micro + macroservices (2020 - present) This is what Airbnb is working on now. The micro and macroservice hybrid model focuses on the unification of APIs. Monorepo vs. Microrepo. Which is the best? Why do different companies choose different options?    Monorepo isn't new; Linux and Windows were both created using Monorepo. To improve scalability and build speed, Google developed its internal dedicated toolchain to scale it faster and strict coding quality standards to keep it consistent. Amazon and Netflix are major ambassadors of the Microservice philosophy. This approach naturally separates the service code into separate repositories. It scales faster but can lead to governance pain points later on. Within Monorepo, each service is a folder, and every folder has a BUILD config and OWNERS permission control. Every service member is responsible for their own folder. On the other hand, in Microrepo, each service is responsible for its repository, with the build config and permissions typically set for the entire repository. In Monorepo, dependencies are shared across the entire codebase regardless of your business, so when there's a version upgrade, every codebase upgrades their version. In Microrepo, dependencies are controlled within each repository. Businesses choose when to upgrade their versions based on their own schedules. Monorepo has a standard for check-ins. Google's code review process is famously known for setting a high bar, ensuring a coherent quality standard for Monorepo, regardless of the business. Microrepo can either set its own standard or adopt a shared standard by incorporating best practices. It can scale faster for business, but the code quality might be a bit different. Google engineers built Bazel, and Meta built Buck. There are other open-source tools available, including Nix, Lerna, and others. Over the years, Microrepo has had more supported tools, including Maven and Gradle for Java, NPM for NodeJS, and CMake for C/C++, among others. How will you design the Stack Overflow website? If your answer is on-premise servers and monolith (on the right), you would likely fail the interview, but that's how it is built in reality!    What people think it should look like The interviewer is probably expecting something on the left side.  Microservice is used to decompose the system into small components. Each service has its own database. Use cache heavily. The service is sharded. The services talk to each other asynchronously through message queues. The service is implemented using Event Sourcing with CQRS. Showing off knowledge in distributed systems such as eventual consistency, CAP theorem, etc.  What it actually is Stack Overflow serves all the traffic with only 9 on-premise web servers, and it’s on monolith! It has its own servers and does not run on the cloud. This is contrary to all our popular beliefs these days. Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? The diagram below shows the architecture comparison before and after the migration.    What is Amazon Prime Video Monitoring Service? Prime Video service needs to monitor the quality of thousands of live streams. The monitoring tool automatically analyzes the streams in real time and identifies quality issues like block corruption, video freeze, and sync problems. This is an important process for customer satisfaction. There are 3 steps: media converter, defect detector, and real-time notification.   What is the problem with the old architecture? The old architecture was based on Amazon Lambda, which was good for building services quickly. However, it was not cost-effective when running the architecture at a high scale. The two most expensive operations are:     The orchestration workflow - AWS step functions charge users by state transitions and the orchestration performs multiple state transitions every second.   Data passing between distributed components - the intermediate data is stored in Amazon S3 so that the next stage can download. The download can be costly when the volume is high.     Monolithic architecture saves 90% cost A monolithic architecture is designed to address the cost issues. There are still 3 components, but the media converter and defect detector are deployed in the same process, saving the cost of passing data over the network. Surprisingly, this approach to deployment architecture change led to 90% cost savings!   This is an interesting and unique case study because microservices have become a go-to and fashionable choice in the tech industry. It's good to see that we are having more discussions about evolving the architecture and having more honest discussions about its pros and cons. Decomposing components into distributed microservices comes with a cost.   What did Amazon leaders say about this? Amazon CTO Werner Vogels: “Building evolvable software systems is a strategy, not a religion. And revisiting your architecture with an open mind is a must.”   Ex Amazon VP Sustainability Adrian Cockcroft: “The Prime Video team had followed a path I call Serverless First…I don’t advocate Serverless Only”. How does Disney Hotstar capture 5 Billion Emojis during a tournament?      Clients send emojis through standard HTTP requests. You can think of Golang Service as a typical Web Server. Golang is chosen because it supports concurrency well. Threads in Golang are lightweight.   Since the write volume is very high, Kafka (message queue) is used as a buffer.   Emoji data are aggregated by a streaming processing service called Spark. It aggregates data every 2 seconds, which is configurable. There is a trade-off to be made based on the interval. A shorter interval means emojis are delivered to other clients faster but it also means more computing resources are needed.   Aggregated data is written to another Kafka.   The PubSub consumers pull aggregated emoji data from Kafka.   Emojis are delivered to other clients in real-time through the PubSub infrastructure. The PubSub infrastructure is interesting. Hotstar considered the following protocols: Socketio, NATS, MQTT, and gRPC, and settled with MQTT.   A similar design is adopted by LinkedIn which streams a million likes/sec. How Discord Stores Trillions Of Messages The diagram below shows the evolution of message storage at Discord:    MongoDB ➡️ Cassandra ➡️ ScyllaDB In 2015, the first version of Discord was built on top of a single MongoDB replica. Around Nov 2015, MongoDB stored 100 million messages and the RAM couldn’t hold the data and index any longer. The latency became unpredictable. Message storage needs to be moved to another database. Cassandra was chosen. In 2017, Discord had 12 Cassandra nodes and stored billions of messages. At the beginning of 2022, it had 177 nodes with trillions of messages. At this point, latency was unpredictable, and maintenance operations became too expensive to run. There are several reasons for the issue:  Cassandra uses the LSM tree for the internal data structure. The reads are more expensive than the writes. There can be many concurrent reads on a server with hundreds of users, resulting in hotspots. Maintaining clusters, such as compacting SSTables, impacts performance. Garbage collection pauses would cause significant latency spikes  ScyllaDB is Cassandra compatible database written in C++. Discord redesigned its architecture to have a monolithic API, a data service written in Rust, and ScyllaDB-based storage. The p99 read latency in ScyllaDB is 15ms compared to 40-125ms in Cassandra. The p99 write latency is 5ms compared to 5-70ms in Cassandra. How do video live streamings work on YouTube, TikTok live, or Twitch? Live streaming differs from regular streaming because the video content is sent via the internet in real-time, usually with a latency of just a few seconds. The diagram below explains what happens behind the scenes to make this possible.    Step 1: The raw video data is captured by a microphone and camera. The data is sent to the server side. Step 2: The video data is compressed and encoded. For example, the compressing algorithm separates the background and other video elements. After compression, the video is encoded to standards such as H.264. The size of the video data is much smaller after this step. Step 3: The encoded data is divided into smaller segments, usually seconds in length, so it takes much less time to download or stream. Step 4: The segmented data is sent to the streaming server. The streaming server needs to support different devices and network conditions. This is called ‘Adaptive Bitrate Streaming.’ This means we need to produce multiple files at different bitrates in steps 2 and 3. Step 5: The live streaming data is pushed to edge servers supported by CDN (Content Delivery Network.) Millions of viewers can watch the video from an edge server nearby. CDN significantly lowers data transmission latency. Step 6: The viewers’ devices decode and decompress the video data and play the video in a video player. Steps 7 and 8: If the video needs to be stored for replay, the encoded data is sent to a storage server, and viewers can request a replay from it later. Standard protocols for live streaming include:  RTMP (Real-Time Messaging Protocol): This was originally developed by Macromedia to transmit data between a Flash player and a server. Now it is used for streaming video data over the internet. Note that video conferencing applications like Skype use RTC (Real-Time Communication) protocol for lower latency. HLS (HTTP Live Streaming): It requires the H.264 or H.265 encoding. Apple devices accept only HLS format. DASH (Dynamic Adaptive Streaming over HTTP): DASH does not support Apple devices. Both HLS and DASH support adaptive bitrate streaming.  License This work is licensed under CC BY-NC-ND 4.0   </readme><commitcount>5</commitcount><languages /><tags /><about>Explain complex systems using visuals and simple terms. Help you prepare for system design interviews.</about><starcount>0</starcount><watchcount>0</watchcount></repository><repository><username>regchiu</username><reponame>spotify-currently-playing-track</reponame><readme>               Spotify Currently Playing Track Overview How to use 1. Go to Spotify Dashboard create an app. 2. Installation 3. Get Authorize URL 4. Get Refresh Token 5. Run start Deploy on your Vercel Known Bug 🐛      README.md        Spotify Currently Playing Track Dynamically generate Spotify currently playing tracks for your github readme or everywhere. Overview  How to use 1. Go to Spotify Dashboard create an app.   Go edit settings and add Redirect URIs. i.e. White-listed addresses to redirect to after authentication success OR failure. For example development is http://localhost:3000, production is https://spotify-currently-playing-track.vercel.app/api.   Rename .env.example to .env file. Copy Redirect URIs, Client ID, Client Secret and paste into YOUR_REDIRECT_URI ,YOUR_CLIENT_ID and YOUR_CLIENT_SECRET field.   2. Installation $ npm install  3. Get Authorize URL $ npm run get-authorize-url  Paste the entire link into your browser and copy the url query string named "code". 4. Get Refresh Token $ npm run get-refresh-token &lt;code&gt;  Copy and paste it into the YOUR_REFRESH_TOKEN field. 5. Run start $ npm run start  Deploy on your Vercel  Sign in with GitHub by Continue with GitHub.     Fork this repo.   Import project and select Import Git Repository.    Allow access to your repository, if prompted.  Select root and keep everything default, then add YOUR_CLIENT_ID, YOUR_CLIENT_SECRET and YOUR_REFRESH_TOKEN of Environment Variables.    Click deploy, and see your domains to use the API! Replace overview link with your URL. enjoy! 🎉  Known Bug 🐛 Layer content inside HTML in SVG foreignObject renders in the wrong place   </readme><commitcount>43</commitcount><languages>JavaScript 100.0%</languages><tags>svg	readme	spotify	image	dynamic	spotify-api	spotify-web-api	readme-generator	vercel	github-readme	github-readme-profile</tags><about>Dynamically generate Spotify currently playing tracks for your github readme or everywhere.</about><starcount>10</starcount><watchcount>0</watchcount></repository><repository><username>regchiu</username><reponame>nuxt3-openai-image-generation</reponame><readme>            Nuxt 3 OpenAI Image Generation Preview Setup Development Server Check your DALL·E API usage      README.md     Nuxt 3 OpenAI Image Generation Preview   Setup Go https://beta.openai.com/account/api-keys to create API Key. Copy .env.example to .env or you can enter your API key on the website. # Go https://beta.openai.com/account/api-keys to create NUXT_OPENAI_API_KEY=openai_api_key Make sure to install the dependencies: yarn install Development Server Start the development server on http://localhost:3000 yarn dev Check your DALL·E API usage See: https://beta.openai.com/account/usage   </readme><commitcount>11</commitcount><languages>Vue 47.0%	TypeScript 45.0%	SCSS 7.6%	Shell 0.4%</languages><tags>image	ai	openai	image-generation	nuxtjs	ai-drawing	chatgpt</tags><about>Nuxt 3 OpenAI Image Generation</about><starcount>12</starcount><watchcount>0</watchcount></repository><repository><username>regchiu</username><reponame>diablo4-map</reponame><readme>    README.md          Diablo4 Map Diablo4 map built with Leaflet.   </readme><commitcount>31</commitcount><languages>TypeScript 95.6%	Vue 4.1%</languages><tags>i18n	map	typescript	leaflet	diablo	vue3	vite	diablo4</tags><about>Diablo4 map built with Leaflet</about><starcount>5</starcount><watchcount>0</watchcount></repository><repository><username>regchiu</username><reponame>next-todo-list-app</reponame><readme>            Todo List Setup Getting Started Lint Format Learn More Deploy on Vercel      README.md     Todo List Todo list app built with Next.js. This is a Next.js project bootstrapped with create-next-app. Setup Make sure to install the dependencies: npm install Getting Started First, run the development server: npm run dev Open http://localhost:3000 with your browser to see the result. This project uses next/font to automatically optimize and load Inter, a custom Google Font. Lint npm run lint Format npm run format Learn More To learn more about Next.js, take a look at the following resources:  Next.js Documentation - learn about Next.js features and API. Learn Next.js - an interactive Next.js tutorial. Flowbite Documentation - learn about Flowbite. Flowbite React Components - learn about Flowbite React Components. React Icons - learn about React Icons. Classnames - learn about Classnames.  Deploy on Vercel The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js. Check out our Next.js deployment documentation for more details.   </readme><commitcount>40</commitcount><languages>TypeScript 91.1%	JavaScript 7.1%	CSS 1.2%	Shell 0.6%</languages><tags>nextjs	dark-mode	todo-list	tailwindcss	todo-list-app	flowbite	flowbite-react</tags><about>Todo list app built with Next.js.</about><starcount>2</starcount><watchcount>0</watchcount></repository><repository><username>type-challenges</username><reponame>type-challenges</reponame><readme>               Intro Challenges warm-up (1) easy (13) medium (95) hard (46) extreme (14) Recommended Readings Official Articles Talks Projects / Solutions Books How to Contribute Play Locally Thanks Inspired by Contributors License      README.md        Collection of TypeScript type challenges             English | 简体中文 | 日本語 | 한국어  Intro  by the power of TypeScript's well-known Turing Completed type system  High-quality types can help improve projects' maintainability while avoiding potential bugs. There are a bunch of awesome type utility libraries that may boost your works on types, like ts-toolbelt, utility-types, SimplyTyped, etc., which you can already use. This project is aimed at helping you better understand how the type system works, writing your own utilities, or just having fun with the challenges. We are also trying to form a community where you can ask questions and get answers you have faced in the real world - they may become part of the challenges! Challenges  Click the following badges to see details of the challenges.   Note: Challenges work in the strict mode.                                                                                                                                                                            By Tags                                                                                                                                                                                                                                      By Plain Textwarm-up (1)13・Hello World easy (13)4・Pick 7・Readonly 11・Tuple to Object 14・First of Array 18・Length of Tuple 43・Exclude 189・Awaited 268・If 533・Concat 898・Includes 3057・Push 3060・Unshift 3312・Parameters medium (95)2・Get Return Type 3・Omit 8・Readonly 2 9・Deep Readonly 10・Tuple to Union 12・Chainable Options 15・Last of Array 16・Pop 20・Promise.all 62・Type Lookup 106・Trim Left 108・Trim 110・Capitalize 116・Replace 119・ReplaceAll 191・Append Argument 296・Permutation 298・Length of String 459・Flatten 527・Append to object 529・Absolute 531・String to Union 599・Merge 612・KebabCase 645・Diff 949・AnyOf 1042・IsNever 1097・IsUnion 1130・ReplaceKeys 1367・Remove Index Signature 1978・Percentage Parser 2070・Drop Char 2257・MinusOne 2595・PickByType 2688・StartsWith 2693・EndsWith 2757・PartialByKeys 2759・RequiredByKeys 2793・Mutable 2852・OmitByType 2946・ObjectEntries 3062・Shift 3188・Tuple to Nested Object 3192・Reverse 3196・Flip Arguments 3243・FlattenDepth 3326・BEM style string 3376・InorderTraversal 4179・Flip 4182・Fibonacci Sequence 4260・AllCombinations 4425・Greater Than 4471・Zip 4484・IsTuple 4499・Chunk 4518・Fill 4803・Trim Right 5117・Without 5140・Trunc 5153・IndexOf 5310・Join 5317・LastIndexOf 5360・Unique 5821・MapTypes 7544・Construct Tuple 8640・Number Range 8767・Combination 8987・Subsequence 9142・CheckRepeatedChars 9286・FirstUniqueCharIndex 9616・Parse URL Params 9896・GetMiddleElement 9898・Appear only once 9989・Count Element Number To Object 10969・Integer 16259・ToPrimitive 17973・DeepMutable 18142・All 18220・Filter 21104・FindAll 21106・Combination key type 21220・Permutations of Tuple 25170・Replace First 25270・Transpose 26401・JSON Schema to TypeScript 27133・Square 27152・Triangular number 27862・CartesianProduct 27932・MergeAll 27958・CheckRepeatedTuple 28333・Public Type 29650・ExtractToObject 29785・Deep Omit 30301・IsOdd 30430・Tower of hanoi hard (46)6・Simple Vue 17・Currying 1 55・Union to Intersection 57・Get Required 59・Get Optional 89・Required Keys 90・Optional Keys 112・Capitalize Words 114・CamelCase 147・C-printf Parser 213・Vue Basic Props 223・IsAny 270・Typed Get 300・String to Number 399・Tuple Filter 472・Tuple to Enum Object 545・printf 553・Deep object to unique 651・Length of String 2 730・Union to Tuple 847・String Join 956・DeepPick 1290・Pinia 1383・Camelize 2059・Drop String 2822・Split 2828・ClassPublicKeys 2857・IsRequiredKey 2949・ObjectFromEntries 4037・IsPalindrome 5181・Mutable Keys 5423・Intersection 6141・Binary to Decimal 7258・Object Key Paths 8804・Two Sum 9155・ValidDate 9160・Assign 9384・Maximum 9775・Capitalize Nest Object Keys 13580・Replace Union 14080・FizzBuzz 14188・Run-length encoding 15260・Tree path array 19458・SnakeCase 25747・IsNegativeNumber 28143・OptionalUndefined extreme (14)5・Get Readonly Keys 151・Query String Parser 216・Slice 274・Integers Comparator 462・Currying 2 476・Sum 517・Multiply 697・Tag 734・Inclusive Range 741・Sort 869・DistributeUnions 925・Assert Array Index 6228・JSON Parser 7561・Subtract   ✨ Upcoming challenges   🔥 Start the challenge in TypeScript Playground   🚀 Start the challenge locally in your IDE or text editor with TypeScript language support   ⚡️ Start the challenge in VS Code Extension  Recommended Readings Official   The TypeScript Handbook  Unions and Intersection Types Literal Types Utility Types Advanced Types    The New Handbook   Articles  Learn Advanced TypeScript Types The Art of Type Programming Type Query: jQuery Style Type Manipulation TypeScript Deep Dive  Talks  Type Level Programming in Typescript  Projects / Solutions  🎥 Video Explanations and Solutions for every challenge! Type Challenges Solutions Type Gymnastics TypeType Examples  Books  Effective TypeScript Learning TypeScript  How to Contribute There are several ways you can contribute to this project  Share your answers / solutions Propose new challenges Add more test cases to the existing challenges Provide learning resources or ideas of how to solve challenges Share the problems you have faced in real-world projects, regardless you having the solution or not - the community would help you as well Help with others by discussion in issues Contribute the infra of this project TODOs.md  Just open an issue and choose the corresponding template. Thanks! Play Locally You can build the challenges and play locally using your preferred IDE or text editor with TypeScript language support. To do that, you will need the latest version of Node.js and pnpm installed. After cloning the repo, installed the dependencies by: pnpm install Then and run the generate script: pnpm generate It will prompt you to select the desired language, then you can find the generated challenges in the ./playground folder. Later if you want to update playground while keeping your changes: pnpm generate --keep-changes OR pnpm generate -K Thanks This project was born from solving real-world types problem with @hardfist and @MeCKodo. And great thanks to @sinoon who contributed a lot while giving early feedback on this project. Inspired by  piotrwitek/utility-types psmyrdek/typescript-challenges andnp/SimplyTyped  Contributors  License MIT   </readme><commitcount>900</commitcount><languages>TypeScript 97.9%	HTML 1.3%</languages><tags>typescript	challenges	type	type-system</tags><about>Collection of TypeScript type challenges with online judge</about><starcount>36.8k</starcount><watchcount>0</watchcount></repository><repository><username>regchiu</username><reponame>type-challenges-saves</reponame><readme>            type-challenges-saves Setup      README.md          type-challenges-saves My saves of VSCode type challenges extension. Setup Install Type Challenges then set your path in settings.json of VScode. "typeChallenges.workspaceFolder": "/path/type-challenges-saves"   </readme><commitcount>23</commitcount><languages>TypeScript 100.0%</languages><tags>challenge	typescript	type-challenges</tags><about>My saves of vscode type challenges extension</about><starcount>1</starcount><watchcount>0</watchcount></repository><repository><username>regchiu</username><reponame>regchiu</reponame><readme>    README.md     👋 Hi! I'm Reg. I am a frontend engineer, I love JavaScript, I love learning new technologies, and I am dedicated to developing websites or applications with beautiful interfaces and a good user experience. 🌱 I'm currently learning:              🚀 I'm use these tools:      📫 How to reach me:    🎵 I'm currently listening:  📊 GitHub stats     </readme><commitcount>30</commitcount><languages /><tags /><about>My saves of vscode type challenges extension</about><starcount>1</starcount><watchcount>0</watchcount></repository><repository><username>regchiu</username><reponame>system-design-101</reponame><readme>               System Design 101 Table of Contents Communication protocols REST API vs. GraphQL How does gRPC work? What is a webhook? How to improve API performance? HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) SOAP vs REST vs GraphQL vs RPC Code First vs. API First HTTP status codes What does API gateway do? How do we design effective and safe APIs? TCP/IP encapsulation Why is Nginx called a “reverse” proxy? What are the common load-balancing algorithms? URL, URI, URN - Do you know the differences? CI/CD CI/CD Pipeline Explained in Simple Terms Netflix Tech Stack (CI/CD Pipeline) Architecture patterns MVC, MVP, MVVM, MVVM-C, and VIPER 18 Key Design Patterns Every Developer Should Know Database A nice cheat sheet of different databases in cloud services 8 Data Structures That Power Your Databases How is an SQL statement executed in the database? CAP theorem Types of Memory and Storage Visualizing a SQL query SQL language Cache Data is cached everywhere Why is Redis so fast? How can Redis be used? Top caching strategies Microservice architecture What does a typical microservice architecture look like? Microservice Best Practices What tech stack is commonly used for microservices? Why is Kafka fast Payment systems How to learn payment systems? Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? How does VISA work when we swipe a credit card at a merchant’s shop? Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India DevOps DevOps vs. SRE vs. Platform Engineering. What is the difference? What is k8s (Kubernetes)? Docker vs. Kubernetes. Which one should we use? How does Docker work? GIT How Git Commands work How does Git Work? Git merge vs. Git rebase Cloud Services A nice cheat sheet of different cloud services (2023 edition) What is cloud native? Developer productivity tools Visualize JSON files Automatically turn code into architecture diagrams Linux Linux file system explained 18 Most-used Linux Commands You Should Know Security How does HTTPS work? Oauth 2.0 Explained With Simple Terms. Top 4 Forms of Authentication Mechanisms Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? How to store passwords safely in the database and how to validate a password? Explaining JSON Web Token (JWT) to a 10 year old Kid How does Google Authenticator (or other types of 2-factor authenticators) work? Real World Case Studies Netflix's Tech Stack Twitter Architecture 2022 Evolution of Airbnb’s microservice architecture over the past 15 years Monorepo vs. Microrepo. How will you design the Stack Overflow website? Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? How does Disney Hotstar capture 5 Billion Emojis during a tournament? How Discord Stores Trillions Of Messages How do video live streamings work on YouTube, TikTok live, or Twitch? License      README.md            【        👨🏻‍💻 YouTube    |         📮 Newsletter    】  System Design 101 Explain complex systems using visuals and simple terms. Whether you're preparing for a System Design Interview or you simply want to understand how systems work beneath the surface, we hope this repository will help you achieve that. Table of Contents  Communication protocols  REST API vs. GraphQL How does gRPC work? What is a webhook? How to improve API performance? HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) SOAP vs REST vs GraphQL vs RPC Code First vs. API First HTTP status codes What does API gateway do? How do we design effective and safe APIs? TCP/IP encapsulation Why is Nginx called a “reverse” proxy? What are the common load-balancing algorithms? URL, URI, URN - Do you know the differences?   CI/CD  CI/CD Pipeline Explained in Simple Terms Netflix Tech Stack (CI/CD Pipeline)   Architecture patterns  MVC, MVP, MVVM, MVVM-C, and VIPER 18 Key Design Patterns Every Developer Should Know   Database  A nice cheat sheet of different databases in cloud services 8 Data Structures That Power Your Databases How is an SQL statement executed in the database? CAP theorem Types of Memory and Storage Visualizing a SQL query SQL language   Cache  Data is cached everywhere Why is Redis so fast? How can Redis be used? Top caching strategies   Microservice architecture  What does a typical microservice architecture look like? Microservice Best Practices What tech stack is commonly used for microservices? Why is Kafka fast   Payment systems  How to learn payment systems? Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? How does VISA work when we swipe a credit card at a merchant’s shop? Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India   DevOps  DevOps vs. SRE vs. Platform Engineering. What is the difference? What is k8s (Kubernetes)? Docker vs. Kubernetes. Which one should we use? How does Docker work?   GIT  How Git Commands work How does Git Work? Git merge vs. Git rebase   Cloud Services  A nice cheat sheet of different cloud services (2023 edition) What is cloud native?   Developer productivity tools  Visualize JSON files Automatically turn code into architecture diagrams   Linux  Linux file system explained 18 Most-used Linux Commands You Should Know   Security  How does HTTPS work? Oauth 2.0 Explained With Simple Terms. Top 4 Forms of Authentication Mechanisms Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? How to store passwords safely in the database and how to validate a password? Explaining JSON Web Token (JWT) to a 10 year old Kid How does Google Authenticator (or other types of 2-factor authenticators) work?   Real World Case Studies  Netflix's Tech Stack Twitter Architecture 2022 Evolution of Airbnb’s microservice architecture over the past 15 years Monorepo vs. Microrepo. How will you design the Stack Overflow website? Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? How does Disney Hotstar capture 5 Billion Emojis during a tournament? How Discord Stores Trillions Of Messages How do video live streamings work on YouTube, TikTok live, or Twitch?    Communication protocols Architecture styles define how different components of an application programming interface (API) interact with one another. As a result, they ensure efficiency, reliability, and ease of integration with other systems by providing a standard approach to designing and building APIs. Here are the most used styles:      SOAP:  Mature, comprehensive, XML-based Best for enterprise applications    RESTful:  Popular, easy-to-implement, HTTP methods  Ideal for web services    GraphQL:  Query language, request specific data  Reduces network overhead, faster responses    gRPC:  Modern, high-performance, Protocol Buffers  Suitable for microservices architectures    WebSocket:  Real-time, bidirectional, persistent connections  Perfect for low-latency data exchange    Webhook:  Event-driven, HTTP callbacks, asynchronous  Notifies systems when events occur   REST API vs. GraphQL The diagram below shows a quick comparison between REST and GraphQL.      GraphQL is a query language for APIs developed by Meta. It provides a complete description of the data in the API and gives clients the power to ask for exactly what they need.   GraphQL servers sit in between the client and the backend services. GraphQL can aggregate multiple REST requests into one query. GraphQL server organizes the resources in a graph.   GraphQL supports queries, mutations (applying data modifications to resources), and subscriptions (receiving notifications on schema modifications).   How does gRPC work?    RPC (Remote Procedure Call) is called “remote” because it enables communications between remote services when services are deployed to different servers under microservice architecture. From the user’s point of view, it acts like a local function call. The diagram below illustrates the overall data flow for gRPC. Step 1: A REST call is made from the client. The request body is usually in JSON format. Steps 2 - 4: The order service (gRPC client) receives the REST call, transforms it, and makes an RPC call to the payment service. gPRC encodes the client stub into a binary format and sends it to the low-level transport layer. Step 5: gRPC sends the packets over the network via HTTP2. Because of binary encoding and network optimizations, gRPC is said to be 5X faster than JSON. Steps 6 - 8: The payment service (gRPC server) receives the packets from the network, decodes them, and invokes the server application. Steps 9 - 11: The result is returned from the server application, and gets encoded and sent to the transport layer. Steps 12 - 14: The order service receives the packets, decodes them, and sends the result to the client application. What is a webhook? The diagram below shows a comparison between polling and Webhook.     Assume we run an eCommerce website. The clients send orders to the order service via the API gateway, which goes to the payment service for payment transactions. The payment service then talks to an external payment service provider (PSP) to complete the transactions.  There are two ways to handle communications with the external PSP.  1. Short polling  After sending the payment request to the PSP, the payment service keeps asking the PSP about the payment status. After several rounds, the PSP finally returns with the status.  Short polling has two drawbacks:   Constant polling of the status requires resources from the payment service.  The External service communicates directly with the payment service, creating security vulnerabilities.   2. Webhook  We can register a webhook with the external service. It means: call me back at a certain URL when you have updates on the request. When the PSP has completed the processing, it will invoke the HTTP request to update the payment status. In this way, the programming paradigm is changed, and the payment service doesn’t need to waste resources to poll the payment status anymore. What if the PSP never calls back? We can set up a housekeeping job to check payment status every hour. Webhooks are often referred to as reverse APIs or push APIs because the server sends HTTP requests to the client. We need to pay attention to 3 things when using a webhook:  We need to design a proper API for the external service to call. We need to set up proper rules in the API gateway for security reasons. We need to register the correct URL at the external service.  How to improve API performance? The diagram below shows 5 common tricks to improve API performance.    Pagination This is a common optimization when the size of the result is large. The results are streaming back to the client to improve the service responsiveness. Asynchronous Logging Synchronous logging deals with the disk for every call and can slow down the system. Asynchronous logging sends logs to a lock-free buffer first and immediately returns. The logs will be flushed to the disk periodically. This significantly reduces the I/O overhead. Caching We can cache frequently accessed data into a cache. The client can query the cache first instead of visiting the database directly. If there is a cache miss, the client can query from the database. Caches like Redis store data in memory, so the data access is much faster than the database. Payload Compression The requests and responses can be compressed using gzip etc so that the transmitted data size is much smaller. This speeds up the upload and download. Connection Pool When accessing resources, we often need to load data from the database. Opening the closing db connections adds significant overhead. So we should connect to the db via a pool of open connections. The connection pool is responsible for managing the connection lifecycle. HTTP 1.0 -&gt; HTTP 1.1 -&gt; HTTP 2.0 -&gt; HTTP 3.0 (QUIC) What problem does each generation of HTTP solve? The diagram below illustrates the key features.      HTTP 1.0 was finalized and fully documented in 1996. Every request to the same server requires a separate TCP connection.   HTTP 1.1 was published in 1997. A TCP connection can be left open for reuse (persistent connection), but it doesn’t solve the HOL (head-of-line) blocking issue. HOL blocking - when the number of allowed parallel requests in the browser is used up, subsequent requests need to wait for the former ones to complete.   HTTP 2.0 was published in 2015. It addresses HOL issue through request multiplexing, which eliminates HOL blocking at the application layer, but HOL still exists at the transport (TCP) layer. As you can see in the diagram, HTTP 2.0 introduced the concept of HTTP “streams”: an abstraction that allows multiplexing different HTTP exchanges onto the same TCP connection. Each stream doesn’t need to be sent in order.   HTTP 3.0 first draft was published in 2020. It is the proposed successor to HTTP 2.0. It uses QUIC instead of TCP for the underlying transport protocol, thus removing HOL blocking in the transport layer.   QUIC is based on UDP. It introduces streams as first-class citizens at the transport layer. QUIC streams share the same QUIC connection, so no additional handshakes and slow starts are required to create new ones, but QUIC streams are delivered independently such that in most cases packet loss affecting one stream doesn't affect others. SOAP vs REST vs GraphQL vs RPC The diagram below illustrates the API timeline and API styles comparison. Over time, different API architectural styles are released. Each of them has its own patterns of standardizing data exchange. You can check out the use cases of each style in the diagram.    Code First vs. API First The diagram below shows the differences between code-first development and API-first development. Why do we want to consider API first design?     Microservices increase system complexity We have separate services to serve different functions of the system. While this kind of architecture facilitates decoupling and segregation of duty, we need to handle the various communications among services.  It is better to think through the system's complexity before writing the code and carefully defining the boundaries of the services.  Separate functional teams need to speak the same language The dedicated functional teams are only responsible for their own components and services. It is recommended that the organization speak the same language via API design.  We can mock requests and responses to validate the API design before writing code.  Improve software quality and developer productivity Since we have ironed out most of the uncertainties when the project starts, the overall development process is smoother, and the software quality is greatly improved.  Developers are happy about the process as well because they can focus on functional development instead of negotiating sudden changes. The possibility of having surprises toward the end of the project lifecycle is reduced. Because we have designed the API first, the tests can be designed while the code is being developed. In a way, we also have TDD (Test Driven Design) when using API first development. HTTP status codes    The response codes for HTTP are divided into five categories: Informational (100-199) Success (200-299) Redirection (300-399) Client Error (400-499) Server Error (500-599) What does API gateway do? The diagram below shows the details.    Step 1 - The client sends an HTTP request to the API gateway. Step 2 - The API gateway parses and validates the attributes in the HTTP request. Step 3 - The API gateway performs allow-list/deny-list checks. Step 4 - The API gateway talks to an identity provider for authentication and authorization. Step 5 - The rate limiting rules are applied to the request. If it is over the limit, the request is rejected. Steps 6 and 7 - Now that the request has passed basic checks, the API gateway finds the relevant service to route to by path matching. Step 8 - The API gateway transforms the request into the appropriate protocol and sends it to backend microservices. Steps 9-12: The API gateway can handle errors properly, and deals with faults if the error takes a longer time to recover (circuit break). It can also leverage ELK (Elastic-Logstash-Kibana) stack for logging and monitoring. We sometimes cache data in the API gateway. How do we design effective and safe APIs? The diagram below shows typical API designs with a shopping cart example.    Note that API design is not just URL path design. Most of the time, we need to choose the proper resource names, identifiers, and path patterns. It is equally important to design proper HTTP header fields or to design effective rate-limiting rules within the API gateway. TCP/IP encapsulation How is data sent over the network? Why do we need so many layers in the OSI model?    The diagram below shows how data is encapsulated and de-encapsulated when transmitting over the network. Step 1: When Device A sends data to Device B over the network via the HTTP protocol, it is first added an HTTP header at the application layer. Step 2: Then a TCP or a UDP header is added to the data. It is encapsulated into TCP segments at the transport layer. The header contains the source port, destination port, and sequence number. Step 3: The segments are then encapsulated with an IP header at the network layer. The IP header contains the source/destination IP addresses. Step 4: The IP datagram is added a MAC header at the data link layer, with source/destination MAC addresses. Step 5: The encapsulated frames are sent to the physical layer and sent over the network in binary bits. Steps 6-10: When Device B receives the bits from the network, it performs the de-encapsulation process, which is a reverse processing of the encapsulation process. The headers are removed layer by layer, and eventually, Device B can read the data. We need layers in the network model because each layer focuses on its own responsibilities. Each layer can rely on the headers for processing instructions and does not need to know the meaning of the data from the last layer. Why is Nginx called a “reverse” proxy? The diagram below shows the differences between a 𝐟𝐨𝐫𝐰𝐚𝐫𝐝 𝐩𝐫𝐨𝐱𝐲 and a 𝐫𝐞𝐯𝐞𝐫𝐬𝐞 𝐩𝐫𝐨𝐱𝐲.    A forward proxy is a server that sits between user devices and the internet. A forward proxy is commonly used for:  Protect clients Avoid browsing restrictions Block access to certain content  A reverse proxy is a server that accepts a request from the client, forwards the request to web servers, and returns the results to the client as if the proxy server had processed the request. A reverse proxy is good for:  Protect servers Load balancing Cache static contents Encrypt and decrypt SSL communications  What are the common load-balancing algorithms? The diagram below shows 6 common algorithms.     Static Algorithms    Round robin The client requests are sent to different service instances in sequential order. The services are usually required to be stateless.   Sticky round-robin This is an improvement of the round-robin algorithm. If Alice’s first request goes to service A, the following requests go to service A as well.   Weighted round-robin The admin can specify the weight for each service. The ones with a higher weight handle more requests than others.   Hash This algorithm applies a hash function on the incoming requests’ IP or URL. The requests are routed to relevant instances based on the hash function result.    Dynamic Algorithms    Least connections A new request is sent to the service instance with the least concurrent connections.   Least response time A new request is sent to the service instance with the fastest response time.   URL, URI, URN - Do you know the differences? The diagram below shows a comparison of URL, URI, and URN.     URI  URI stands for Uniform Resource Identifier. It identifies a logical or physical resource on the web. URL and URN are subtypes of URI. URL locates a resource, while URN names a resource. A URI is composed of the following parts: scheme:[//authority]path[?query][#fragment]  URL  URL stands for Uniform Resource Locator, the key concept of HTTP. It is the address of a unique resource on the web. It can be used with other protocols like FTP and JDBC.  URN  URN stands for Uniform Resource Name. It uses the urn scheme. URNs cannot be used to locate a resource. A simple example given in the diagram is composed of a namespace and a namespace-specific string. If you would like to learn more detail on the subject, I would recommend W3C’s clarification. CI/CD CI/CD Pipeline Explained in Simple Terms    Section 1 - SDLC with CI/CD The software development life cycle (SDLC) consists of several key stages: development, testing, deployment, and maintenance. CI/CD automates and integrates these stages to enable faster and more reliable releases. When code is pushed to a git repository, it triggers an automated build and test process. End-to-end (e2e) test cases are run to validate the code. If tests pass, the code can be automatically deployed to staging/production. If issues are found, the code is sent back to development for bug fixing. This automation provides fast feedback to developers and reduces the risk of bugs in production. Section 2 - Difference between CI and CD Continuous Integration (CI) automates the build, test, and merge process. It runs tests whenever code is committed to detect integration issues early. This encourages frequent code commits and rapid feedback. Continuous Delivery (CD) automates release processes like infrastructure changes and deployment. It ensures software can be released reliably at any time through automated workflows. CD may also automate the manual testing and approval steps required before production deployment. Section 3 - CI/CD Pipeline A typical CI/CD pipeline has several connected stages:  The developer commits code changes to the source control CI server detects changes and triggers the build Code is compiled, and tested (unit, integration tests) Test results reported to the developer On success, artifacts are deployed to staging environments Further testing may be done on staging before release CD system deploys approved changes to production  Netflix Tech Stack (CI/CD Pipeline)    Planning: Netflix Engineering uses JIRA for planning and Confluence for documentation. Coding: Java is the primary programming language for the backend service, while other languages are used for different use cases. Build: Gradle is mainly used for building, and Gradle plugins are built to support various use cases. Packaging: Package and dependencies are packed into an Amazon Machine Image (AMI) for release. Testing: Testing emphasizes the production culture's focus on building chaos tools. Deployment: Netflix uses its self-built Spinnaker for canary rollout deployment. Monitoring: The monitoring metrics are centralized in Atlas, and Kayenta is used to detect anomalies. Incident report: Incidents are dispatched according to priority, and PagerDuty is used for incident handling. Architecture patterns MVC, MVP, MVVM, MVVM-C, and VIPER These architecture patterns are among the most commonly used in app development, whether on iOS or Android platforms. Developers have introduced them to overcome the limitations of earlier patterns. So, how do they differ?     MVC, the oldest pattern, dates back almost 50 years Every pattern has a "view" (V) responsible for displaying content and receiving user input Most patterns include a "model" (M) to manage business data "Controller," "presenter," and "view-model" are translators that mediate between the view and the model ("entity" in the VIPER pattern)  18 Key Design Patterns Every Developer Should Know Patterns are reusable solutions to common design problems, resulting in a smoother, more efficient development process. They serve as blueprints for building better software structures. These are some of the most popular patterns:     Abstract Factory: Family Creator - Makes groups of related items. Builder: Lego Master - Builds objects step by step, keeping creation and appearance separate. Prototype: Clone Maker - Creates copies of fully prepared examples. Singleton: One and Only - A special class with just one instance. Adapter: Universal Plug - Connects things with different interfaces. Bridge: Function Connector - Links how an object works to what it does. Composite: Tree Builder - Forms tree-like structures of simple and complex parts. Decorator: Customizer - Adds features to objects without changing their core. Facade: One-Stop-Shop - Represents a whole system with a single, simplified interface. Flyweight: Space Saver - Shares small, reusable items efficiently. Proxy: Stand-In Actor - Represents another object, controlling access or actions. Chain of Responsibility: Request Relay - Passes a request through a chain of objects until handled. Command: Task Wrapper - Turns a request into an object, ready for action. Iterator: Collection Explorer - Accesses elements in a collection one by one. Mediator: Communication Hub - Simplifies interactions between different classes. Memento: Time Capsule - Captures and restores an object's state. Observer: News Broadcaster - Notifies classes about changes in other objects. Visitor: Skillful Guest - Adds new operations to a class without altering it.  Database A nice cheat sheet of different databases in cloud services    Choosing the right database for your project is a complex task. Many database options, each suited to distinct use cases, can quickly lead to decision fatigue. We hope this cheat sheet provides high-level direction to pinpoint the right service that aligns with your project's needs and avoid potential pitfalls. Note: Google has limited documentation for their database use cases. Even though we did our best to look at what was available and arrived at the best option, some of the entries may need to be more accurate. 8 Data Structures That Power Your Databases The answer will vary depending on your use case. Data can be indexed in memory or on disk. Similarly, data formats vary, such as numbers, strings, geographic coordinates, etc. The system might be write-heavy or read-heavy. All of these factors affect your choice of database index format.    The following are some of the most popular data structures used for indexing data:  Skiplist: a common in-memory index type. Used in Redis Hash index: a very common implementation of the “Map” data structure (or “Collection”) SSTable: immutable on-disk “Map” implementation LSM tree: Skiplist + SSTable. High write throughput B-tree: disk-based solution. Consistent read/write performance Inverted index: used for document indexing. Used in Lucene Suffix tree: for string pattern search R-tree: multi-dimension search, such as finding the nearest neighbor  How is an SQL statement executed in the database? The diagram below shows the process. Note that the architectures for different databases are different, the diagram demonstrates some common designs.    Step 1 - A SQL statement is sent to the database via a transport layer protocol (e.g.TCP). Step 2 - The SQL statement is sent to the command parser, where it goes through syntactic and semantic analysis, and a query tree is generated afterward. Step 3 - The query tree is sent to the optimizer. The optimizer creates an execution plan. Step 4 - The execution plan is sent to the executor. The executor retrieves data from the execution. Step 5 - Access methods provide the data fetching logic required for execution, retrieving data from the storage engine. Step 6 - Access methods decide whether the SQL statement is read-only. If the query is read-only (SELECT statement), it is passed to the buffer manager for further processing. The buffer manager looks for the data in the cache or data files. Step 7 - If the statement is an UPDATE or INSERT, it is passed to the transaction manager for further processing. Step 8 - During a transaction, the data is in lock mode. This is guaranteed by the lock manager. It also ensures the transaction’s ACID properties. CAP theorem The CAP theorem is one of the most famous terms in computer science, but I bet different developers have different understandings. Let’s examine what it is and why it can be confusing.    CAP theorem states that a distributed system can't provide more than two of these three guarantees simultaneously. Consistency: consistency means all clients see the same data at the same time no matter which node they connect to. Availability: availability means any client that requests data gets a response even if some of the nodes are down. Partition Tolerance: a partition indicates a communication break between two nodes. Partition tolerance means the system continues to operate despite network partitions. The “2 of 3” formulation can be useful, but this simplification could be misleading.   Picking a database is not easy. Justifying our choice purely based on the CAP theorem is not enough. For example, companies don't choose Cassandra for chat applications simply because it is an AP system. There is a list of good characteristics that make Cassandra a desirable option for storing chat messages. We need to dig deeper.   “CAP prohibits only a tiny part of the design space: perfect availability and consistency in the presence of partitions, which are rare”. Quoted from the paper: CAP Twelve Years Later: How the “Rules” Have Changed.   The theorem is about 100% availability and consistency. A more realistic discussion would be the trade-offs between latency and consistency when there is no network partition. See PACELC theorem for more details.   Is the CAP theorem actually useful? I think it is still useful as it opens our minds to a set of tradeoff discussions, but it is only part of the story. We need to dig deeper when picking the right database. Types of Memory and Storage    Visualizing a SQL query    SQL statements are executed by the database system in several steps, including:  Parsing the SQL statement and checking its validity Transforming the SQL into an internal representation, such as relational algebra Optimizing the internal representation and creating an execution plan that utilizes index information Executing the plan and returning the results  The execution of SQL is highly complex and involves many considerations, such as:  The use of indexes and caches The order of table joins Concurrency control Transaction management  SQL language In 1986, SQL (Structured Query Language) became a standard. Over the next 40 years, it became the dominant language for relational database management systems. Reading the latest standard (ANSI SQL 2016) can be time-consuming. How can I learn it?    There are 5 components of the SQL language:  DDL: data definition language, such as CREATE, ALTER, DROP DQL: data query language, such as SELECT DML: data manipulation language, such as INSERT, UPDATE, DELETE DCL: data control language, such as GRANT, REVOKE TCL: transaction control language, such as COMMIT, ROLLBACK  For a backend engineer, you may need to know most of it. As a data analyst, you may need to have a good understanding of DQL. Select the topics that are most relevant to you. Cache Data is cached everywhere This diagram illustrates where we cache data in a typical architecture.    There are multiple layers along the flow.  Client apps: HTTP responses can be cached by the browser. We request data over HTTP for the first time, and it is returned with an expiry policy in the HTTP header; we request data again, and the client app tries to retrieve the data from the browser cache first. CDN: CDN caches static web resources. The clients can retrieve data from a CDN node nearby. Load Balancer: The load Balancer can cache resources as well. Messaging infra: Message brokers store messages on disk first, and then consumers retrieve them at their own pace. Depending on the retention policy, the data is cached in Kafka clusters for a period of time. Services: There are multiple layers of cache in a service. If the data is not cached in the CPU cache, the service will try to retrieve the data from memory. Sometimes the service has a second-level cache to store data on disk. Distributed Cache: Distributed cache like Redis holds key-value pairs for multiple services in memory. It provides much better read/write performance than the database. Full-text Search: we sometimes need to use full-text searches like Elastic Search for document search or log search. A copy of data is indexed in the search engine as well. Database: Even in the database, we have different levels of caches:   WAL(Write-ahead Log): data is written to WAL first before building the B tree index Bufferpool: A memory area allocated to cache query results Materialized View: Pre-compute query results and store them in the database tables for better query performance Transaction log: record all the transactions and database updates Replication Log: used to record the replication state in a database cluster  Why is Redis so fast? There are 3 main reasons as shown in the diagram below.     Redis is a RAM-based data store. RAM access is at least 1000 times faster than random disk access. Redis leverages IO multiplexing and single-threaded execution loop for execution efficiency. Redis leverages several efficient lower-level data structures.  Question: Another popular in-memory store is Memcached. Do you know the differences between Redis and Memcached? You might have noticed the style of this diagram is different from my previous posts. Please let me know which one you prefer. How can Redis be used?    There is more to Redis than just caching. Redis can be used in a variety of scenarios as shown in the diagram.   Session We can use Redis to share user session data among different services.   Cache We can use Redis to cache objects or pages, especially for hotspot data.   Distributed lock We can use a Redis string to acquire locks among distributed services.   Counter We can count how many likes or how many reads for articles.   Rate limiter We can apply a rate limiter for certain user IPs.   Global ID generator We can use Redis Int for global ID.   Shopping cart We can use Redis Hash to represent key-value pairs in a shopping cart.   Calculate user retention We can use Bitmap to represent the user login daily and calculate user retention.   Message queue We can use List for a message queue.   Ranking We can use ZSet to sort the articles.   Top caching strategies Designing large-scale systems usually requires careful consideration of caching. Below are five caching strategies that are frequently utilized.    Microservice architecture What does a typical microservice architecture look like?    The diagram below shows a typical microservice architecture.  Load Balancer: This distributes incoming traffic across multiple backend services. CDN (Content Delivery Network): CDN is a group of geographically distributed servers that hold static content for faster delivery. The clients look for content in CDN first, then progress  to backend services. API Gateway: This handles incoming requests and routes them to the relevant services. It talks to the identity provider and service discovery. Identity Provider: This handles authentication and authorization for users. Service Registry &amp; Discovery: Microservice registration and discovery happen in this component, and the API gateway looks for relevant services in this component to talk to. Management: This component is responsible for monitoring the services. Microservices: Microservices are designed and deployed in different domains. Each domain has its own database. The API gateway talks to the microservices via REST API or other protocols, and the microservices within the same domain talk to each other using RPC (Remote Procedure Call).  Benefits of microservices:  They can be quickly designed, deployed, and horizontally scaled. Each domain can be independently maintained by a dedicated team. Business requirements can be customized in each domain and better supported, as a result.  Microservice Best Practices A picture is worth a thousand words: 9 best practices for developing microservices.    When we develop microservices, we need to follow the following best practices:  Use separate data storage for each microservice Keep code at a similar level of maturity Separate build for each microservice Assign each microservice with a single responsibility Deploy into containers Design stateless services Adopt domain-driven design Design micro frontend Orchestrating microservices  What tech stack is commonly used for microservices? Below you will find a diagram showing the microservice tech stack, both for the development phase and for production.    ▶️ 𝐏𝐫𝐞-𝐏𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧  Define API - This establishes a contract between frontend and backend. We can use Postman or OpenAPI for this. Development - Node.js or react is popular for frontend development, and java/python/go for backend development. Also, we need to change the configurations in the API gateway according to API definitions. Continuous Integration - JUnit and Jenkins for automated testing. The code is packaged into a Docker image and deployed as microservices.  ▶️ 𝐏𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧  NGinx is a common choice for load balancers. Cloudflare provides CDN (Content Delivery Network). API Gateway - We can use spring boot for the gateway, and use Eureka/Zookeeper for service discovery. The microservices are deployed on clouds. We have options among AWS, Microsoft Azure, or Google GCP. Cache and Full-text Search - Redis is a common choice for caching key-value pairs. ElasticSearch is used for full-text search. Communications - For services to talk to each other, we can use messaging infra Kafka or RPC. Persistence - We can use MySQL or PostgreSQL for a relational database, and Amazon S3 for object store. We can also use Cassandra for the wide-column store if necessary. Management &amp; Monitoring - To manage so many microservices, the common Ops tools include Prometheus, Elastic Stack, and Kubernetes.  Why is Kafka fast There are many design decisions that contributed to Kafka’s performance. In this post, we’ll focus on two. We think these two carried the most weight.     The first one is Kafka’s reliance on Sequential I/O. The second design choice that gives Kafka its performance advantage is its focus on efficiency: zero copy principle.  The diagram illustrates how the data is transmitted between producer and consumer, and what zero-copy means.  Step 1.1 - 1.3: Producer writes data to the disk Step 2: Consumer reads data without zero-copy  2.1 The data is loaded from disk to OS cache 2.2 The data is copied from OS cache to Kafka application 2.3 Kafka application copies the data into the socket buffer 2.4 The data is copied from socket buffer to network card 2.5 The network card sends data out to the consumer  Step 3: Consumer reads data with zero-copy  3.1: The data is loaded from disk to OS cache 3.2 OS cache directly copies the data to the network card via sendfile() command 3.3 The network card sends data out to the consumer Zero copy is a shortcut to save the multiple data copies between application context and kernel context. Payment systems How to learn payment systems?    Why is the credit card called “the most profitable product in banks”? How does VISA/Mastercard make money? The diagram below shows the economics of the credit card payment flow.    1.  The cardholder pays a merchant $100 to buy a product. 2. The merchant benefits from the use of the credit card with higher sales volume and needs to compensate the issuer and the card network for providing the payment service. The acquiring bank sets a fee with the merchant, called the “merchant discount fee.” 3 - 4. The acquiring bank keeps $0.25 as the acquiring markup, and $1.75 is paid to the issuing bank as the interchange fee. The merchant discount fee should cover the interchange fee. The interchange fee is set by the card network because it is less efficient for each issuing bank to negotiate fees with each merchant. 5.  The card network sets up the network assessments and fees with each bank, which pays the card network for its services every month. For example, VISA charges a 0.11% assessment, plus a $0.0195 usage fee, for every swipe. 6.  The cardholder pays the issuing bank for its services. Why should the issuing bank be compensated?  The issuer pays the merchant even if the cardholder fails to pay the issuer. The issuer pays the merchant before the cardholder pays the issuer. The issuer has other operating costs, including managing customer accounts, providing statements, fraud detection, risk management, clearing &amp; settlement, etc.  How does VISA work when we swipe a credit card at a merchant’s shop?    VISA, Mastercard, and American Express act as card networks for the clearing and settling of funds. The card acquiring bank and the card issuing bank can be – and often are – different. If banks were to settle transactions one by one without an intermediary, each bank would have to settle the transactions with all the other banks. This is quite inefficient. The diagram below shows VISA’s role in the credit card payment process. There are two flows involved. Authorization flow happens when the customer swipes the credit card. Capture and settlement flow happens when the merchant wants to get the money at the end of the day.  Authorization Flow  Step 0: The card issuing bank issues credit cards to its customers. Step 1: The cardholder wants to buy a product and swipes the credit card at the Point of Sale (POS) terminal in the merchant’s shop. Step 2: The POS terminal sends the transaction to the acquiring bank, which has provided the POS terminal. Steps 3 and 4: The acquiring bank sends the transaction to the card network, also called the card scheme. The card network sends the transaction to the issuing bank for approval. Steps 4.1, 4.2 and 4.3: The issuing bank freezes the money if the transaction is approved. The approval or rejection is sent back to the acquirer, as well as the POS terminal.  Capture and Settlement Flow  Steps 1 and 2: The merchant wants to collect the money at the end of the day, so they hit ”capture” on the POS terminal. The transactions are sent to the acquirer in batch. The acquirer sends the batch file with transactions to the card network. Step 3: The card network performs clearing for the transactions collected from different acquirers, and sends the clearing files to different issuing banks. Step 4: The issuing banks confirm the correctness of the clearing files, and transfer money to the relevant acquiring banks. Step 5: The acquiring bank then transfers money to the merchant’s bank. Step 4: The card network clears the transactions from different acquiring banks. Clearing is a process in which mutual offset transactions are netted, so the number of total transactions is reduced. In the process, the card network takes on the burden of talking to each bank and receives service fees in return. Payment Systems Around The World Series (Part 1): Unified Payments Interface (UPI) in India What’s UPI? UPI is an instant real-time payment system developed by the National Payments Corporation of India. It accounts for 60% of digital retail transactions in India today. UPI = payment markup language + standard for interoperable payments    DevOps DevOps vs. SRE vs. Platform Engineering. What is the difference? The concepts of DevOps, SRE, and Platform Engineering have emerged at different times and have been developed by various individuals and organizations.    DevOps as a concept was introduced in 2009 by Patrick Debois and Andrew Shafer at the Agile conference. They sought to bridge the gap between software development and operations by promoting a collaborative culture and shared responsibility for the entire software development lifecycle. SRE, or Site Reliability Engineering, was pioneered by Google in the early 2000s to address operational challenges in managing large-scale, complex systems. Google developed SRE practices and tools, such as the Borg cluster management system and the Monarch monitoring system, to improve the reliability and efficiency of their services. Platform Engineering is a more recent concept, building on the foundation of SRE engineering. The precise origins of Platform Engineering are less clear, but it is generally understood to be an extension of the DevOps and SRE practices, with a focus on delivering a comprehensive platform for product development that supports the entire business perspective. It's worth noting that while these concepts emerged at different times. They are all related to the broader trend of improving collaboration, automation, and efficiency in software development and operations. What is k8s (Kubernetes)? K8s is a container orchestration system. It is used for container deployment and management. Its design is greatly impacted by Google’s internal system Borg.    A k8s cluster consists of a set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node. The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers, and a cluster usually runs multiple nodes, providing fault tolerance and high availability.  Control Plane Components    API Server The API server talks to all the components in the k8s cluster. All the operations on pods are executed by talking to the API server.   Scheduler The scheduler watches pod workloads and assigns loads on newly created pods.   Controller Manager The controller manager runs the controllers, including Node Controller, Job Controller, EndpointSlice Controller, and ServiceAccount Controller.   Etcd etcd is a key-value store used as Kubernetes' backing store for all cluster data.    Nodes    Pods A pod is a group of containers and is the smallest unit that k8s administers. Pods have a single IP address applied to every container within the pod.   Kubelet An agent that runs on each node in the cluster. It ensures containers are running in a Pod.   Kube Proxy Kube-proxy is a network proxy that runs on each node in your cluster. It routes traffic coming into a node from the service. It forwards requests for work to the correct containers.   Docker vs. Kubernetes. Which one should we use?    What is Docker ? Docker is an open-source platform that allows you to package, distribute, and run applications in isolated containers. It focuses on containerization, providing lightweight environments that encapsulate applications and their dependencies. What is Kubernetes ? Kubernetes, often referred to as K8s, is an open-source container orchestration platform. It provides a framework for automating the deployment, scaling, and management of containerized applications across a cluster of nodes. How are both different from each other ? Docker: Docker operates at the individual container level on a single operating system host. You must manually manage each host and setting up networks, security policies, and storage for multiple related containers can be complex. Kubernetes: Kubernetes operates at the cluster level. It manages multiple containerized applications across multiple hosts, providing automation for tasks like load balancing, scaling, and ensuring the desired state of applications. In short, Docker focuses on containerization and running containers on individual hosts, while Kubernetes specializes in managing and orchestrating containers at scale across a cluster of hosts. How does Docker work? The diagram below shows the architecture of Docker and how it works when we run “docker build”, “docker pull” and “docker run”.    There are 3 components in Docker architecture:   Docker client The docker client talks to the Docker daemon.   Docker host The Docker daemon listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.   Docker registry A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use.   Let’s take the “docker run” command as an example.  Docker pulls the image from the registry. Docker creates a new container. Docker allocates a read-write filesystem to the container. Docker creates a network interface to connect the container to the default network. Docker starts the container.  GIT How Git Commands work To begin with, it's essential to identify where our code is stored. The common assumption is that there are only two locations - one on a remote server like Github and the other on our local machine. However, this isn't entirely accurate. Git maintains three local storages on our machine, which means that our code can be found in four places:     Working directory: where we edit files Staging area: a temporary location where files are kept for the next commit Local repository: contains the code that has been committed Remote repository: the remote server that stores the code  Most Git commands primarily move files between these four locations. How does Git Work? The diagram below shows the Git workflow.    Git is a distributed version control system. Every developer maintains a local copy of the main repository and edits and commits to the local copy. The commit is very fast because the operation doesn’t interact with the remote repository. If the remote repository crashes, the files can be recovered from the local repositories. Git merge vs. Git rebase What are the differences?    When we merge changes from one Git branch to another, we can use ‘git merge’ or ‘git rebase’. The diagram below shows how the two commands work. Git merge This creates a new commit G’ in the main branch. G’ ties the histories of both main and feature branches. Git merge is non-destructive. Neither the main nor the feature branch is changed. Git rebase Git rebase moves the feature branch histories to the head of the main branch. It creates new commits E’, F’, and G’ for each commit in the feature branch. The benefit of rebase is that it has a linear commit history. Rebase can be dangerous if “the golden rule of git rebase” is not followed. The Golden Rule of Git Rebase Never use it on public branches! Cloud Services A nice cheat sheet of different cloud services (2023 edition)    What is cloud native? Below is a diagram showing the evolution of architecture and processes since the 1980s.    Organizations can build and run scalable applications on public, private, and hybrid clouds using cloud native technologies. This means the applications are designed to leverage cloud features, so they are resilient to load and easy to scale. Cloud native includes 4 aspects:   Development process This has progressed from waterfall to agile to DevOps.   Application Architecture The architecture has gone from monolithic to microservices. Each service is designed to be small, adaptive to the limited resources in cloud containers.   Deployment &amp; packaging The applications used to be deployed on physical servers. Then around 2000, the applications that were not sensitive to latency were usually deployed on virtual servers. With cloud native applications, they are packaged into docker images and deployed in containers.   Application infrastructure The applications are massively deployed on cloud infrastructure instead of self-hosted servers.   Developer productivity tools Visualize JSON files Nested JSON files are hard to read. JsonCrack generates graph diagrams from JSON files and makes them easy to read. Additionally, the generated diagrams can be downloaded as images.    Automatically turn code into architecture diagrams    What does it do?  Draw the cloud system architecture in Python code. Diagrams can also be rendered directly inside the Jupyter Notebooks. No design tools are needed. Supports the following providers: AWS, Azure, GCP, Kubernetes, Alibaba Cloud, Oracle Cloud, etc.  Github repo Linux Linux file system explained    The Linux file system used to resemble an unorganized town where individuals constructed their houses wherever they pleased. However, in 1994, the Filesystem Hierarchy Standard (FHS) was introduced to bring order to the Linux file system. By implementing a standard like the FHS, software can ensure a consistent layout across various Linux distributions. Nonetheless, not all Linux distributions strictly adhere to this standard. They often incorporate their own unique elements or cater to specific requirements. To become proficient in this standard, you can begin by exploring. Utilize commands such as "cd" for navigation and "ls" for listing directory contents. Imagine the file system as a tree, starting from the root (/). With time, it will become second nature to you, transforming you into a skilled Linux administrator. 18 Most-used Linux Commands You Should Know Linux commands are instructions for interacting with the operating system. They help manage files, directories, system processes, and many other aspects of the system. You need to become familiar with these commands in order to navigate and maintain Linux-based systems efficiently and effectively. This diagram below shows popular Linux commands:     ls - List files and directories cd - Change the current directory mkdir - Create a new directory rm - Remove files or directories cp - Copy files or directories mv - Move or rename files or directories chmod - Change file or directory permissions grep - Search for a pattern in files find - Search for files and directories tar - manipulate tarball archive files vi - Edit files using text editors cat - display the content of files top - Display processes and resource usage ps - Display processes information kill - Terminate a process by sending a signal du - Estimate file space usage ifconfig - Configure network interfaces ping - Test network connectivity between hosts  Security How does HTTPS work? Hypertext Transfer Protocol Secure (HTTPS) is an extension of the Hypertext Transfer Protocol (HTTP.) HTTPS transmits encrypted data using Transport Layer Security (TLS.) If the data is hijacked online, all the hijacker gets is binary code.    How is the data encrypted and decrypted? Step 1 - The client (browser) and the server establish a TCP connection. Step 2 - The client sends a “client hello” to the server. The message contains a set of necessary encryption algorithms (cipher suites) and the latest TLS version it can support. The server responds with a “server hello” so the browser knows whether it can support the algorithms and TLS version. The server then sends the SSL certificate to the client. The certificate contains the public key, host name, expiry dates, etc. The client validates the certificate. Step 3 - After validating the SSL certificate, the client generates a session key and encrypts it using the public key. The server receives the encrypted session key and decrypts it with the private key. Step 4 - Now that both the client and the server hold the same session key (symmetric encryption), the encrypted data is transmitted in a secure bi-directional channel. Why does HTTPS switch to symmetric encryption during data transmission? There are two main reasons:   Security: The asymmetric encryption goes only one way. This means that if the server tries to send the encrypted data back to the client, anyone can decrypt the data using the public key.   Server resources: The asymmetric encryption adds quite a lot of mathematical overhead. It is not suitable for data transmissions in long sessions.   Oauth 2.0 Explained With Simple Terms. OAuth 2.0 is a powerful and secure framework that allows different applications to securely interact with each other on behalf of users without sharing sensitive credentials.    The entities involved in OAuth are the User, the Server, and the Identity Provider (IDP). What Can an OAuth Token Do? When you use OAuth, you get an OAuth token that represents your identity and permissions. This token can do a few important things: Single Sign-On (SSO): With an OAuth token, you can log into multiple services or apps using just one login, making life easier and safer. Authorization Across Systems: The OAuth token allows you to share your authorization or access rights across various systems, so you don't have to log in separately everywhere. Accessing User Profile: Apps with an OAuth token can access certain parts of your user profile that you allow, but they won't see everything. Remember, OAuth 2.0 is all about keeping you and your data safe while making your online experiences seamless and hassle-free across different applications and services. Top 4 Forms of Authentication Mechanisms      SSH Keys: Cryptographic keys are used to access remote systems and servers securely   OAuth Tokens: Tokens that provide limited access to user data on third-party applications   SSL Certificates: Digital certificates ensure secure and encrypted communication between servers and clients   Credentials: User authentication information is used to verify and grant access to various systems and services   Session, cookie, JWT, token, SSO, and OAuth 2.0 - what are they? These terms are all related to user identity management. When you log into a website, you declare who you are (identification). Your identity is verified (authentication), and you are granted the necessary permissions (authorization). Many solutions have been proposed in the past, and the list keeps growing.    From simple to complex, here is my understanding of user identity management:   WWW-Authenticate is the most basic method. You are asked for the username and password by the browser. As a result of the inability to control the login life cycle, it is seldom used today.   A finer control over the login life cycle is session-cookie. The server maintains session storage, and the browser keeps the ID of the session. A cookie usually only works with browsers and is not mobile app friendly.   To address the compatibility issue, the token can be used. The client sends the token to the server, and the server validates the token. The downside is that the token needs to be encrypted and decrypted, which may be time-consuming.   JWT is a standard way of representing tokens. This information can be verified and trusted because it is digitally signed. Since JWT contains the signature, there is no need to save session information on the server side.   By using SSO (single sign-on), you can sign on only once and log in to multiple websites. It uses CAS (central authentication service) to maintain cross-site information   By using OAuth 2.0, you can authorize one website to access your information on another website.   How to store passwords safely in the database and how to validate a password?    Things NOT to do   Storing passwords in plain text is not a good idea because anyone with internal access can see them.   Storing password hashes directly is not sufficient because it is pruned to precomputation attacks, such as rainbow tables.   To mitigate precomputation attacks, we salt the passwords.   What is salt? According to OWASP guidelines, “a salt is a unique, randomly generated string that is added to each password as part of the hashing process”. How to store a password and salt?  the hash result is unique to each password. The password can be stored in the database using the following format: hash(password + salt).  How to validate a password? To validate a password, it can go through the following process:  A client enters the password. The system fetches the corresponding salt from the database. The system appends the salt to the password and hashes it. Let’s call the hashed value H1. The system compares H1 and H2, where H2 is the hash stored in the database. If they are the same, the password is valid.  Explaining JSON Web Token (JWT) to a 10 year old Kid    Imagine you have a special box called a JWT. Inside this box, there are three parts: a header, a payload, and a signature. The header is like the label on the outside of the box. It tells us what type of box it is and how it's secured. It's usually written in a format called JSON, which is just a way to organize information using curly braces { } and colons : . The payload is like the actual message or information you want to send. It could be your name, age, or any other data you want to share. It's also written in JSON format, so it's easy to understand and work with. Now, the signature is what makes the JWT secure. It's like a special seal that only the sender knows how to create. The signature is created using a secret code, kind of like a password. This signature ensures that nobody can tamper with the contents of the JWT without the sender knowing about it. When you want to send the JWT to a server, you put the header, payload, and signature inside the box. Then you send it over to the server. The server can easily read the header and payload to understand who you are and what you want to do. How does Google Authenticator (or other types of 2-factor authenticators) work? Google Authenticator is commonly used for logging into our accounts when 2-factor authentication is enabled. How does it guarantee security? Google Authenticator is a software-based authenticator that implements a two-step verification service. The diagram below provides detail.    There are two stages involved:  Stage 1 - The user enables Google two-step verification Stage 2 - The user uses the authenticator for logging in, etc.  Let’s look at these stages. Stage 1 Steps 1 and 2: Bob opens the web page to enable two-step verification. The front end requests a secret key. The authentication service generates the secret key for Bob and stores it in the database. Step 3: The authentication service returns a URI to the front end. The URI is composed of a key issuer, username, and secret key. The URI is displayed in the form of a QR code on the web page. Step 4: Bob then uses Google Authenticator to scan the generated QR code. The secret key is stored in the authenticator. Stage 2 Steps 1 and 2: Bob wants to log into a website with Google two-step verification. For this, he needs the password. Every 30 seconds, Google Authenticator generates a 6-digit password using TOTP (Time-based One Time Password) algorithm. Bob uses the password to enter the website. Steps 3 and 4: The frontend sends the password Bob enters to the backend for authentication. The authentication service reads the secret key from the database and generates a 6-digit password using the same TOTP algorithm as the client. Step 5: The authentication service compares the two passwords generated by the client and the server, and returns the comparison result to the frontend. Bob can proceed with the login process only if the two passwords match. Is this authentication mechanism safe?   Can the secret key be obtained by others? We need to make sure the secret key is transmitted using HTTPS. The authenticator client and the database store the secret key, and we need to make sure the secret keys are encrypted.   Can the 6-digit password be guessed by hackers? No. The password has 6 digits, so the generated password has 1 million potential combinations. Plus, the password changes every 30 seconds. If hackers want to guess the password in 30 seconds, they need to enter 30,000 combinations per second.   Real World Case Studies Netflix's Tech Stack This post is based on research from many Netflix engineering blogs and open-source projects. If you come across any inaccuracies, please feel free to inform us.    Mobile and web: Netflix has adopted Swift and Kotlin to build native mobile apps. For its web application, it uses React. Frontend/server communication: Netflix uses GraphQL. Backend services: Netflix relies on ZUUL, Eureka, the Spring Boot framework, and other technologies. Databases: Netflix utilizes EV cache, Cassandra, CockroachDB, and other databases. Messaging/streaming: Netflix employs Apache Kafka and Fink for messaging and streaming purposes. Video storage: Netflix uses S3 and Open Connect for video storage. Data processing: Netflix utilizes Flink and Spark for data processing, which is then visualized using Tableau. Redshift is used for processing structured data warehouse information. CI/CD: Netflix employs various tools such as JIRA, Confluence, PagerDuty, Jenkins, Gradle, Chaos Monkey, Spinnaker, Atlas, and more for CI/CD processes. Twitter Architecture 2022 Yes, this is the real Twitter architecture. It is posted by Elon Musk and redrawn by us for better readability.    Evolution of Airbnb’s microservice architecture over the past 15 years Airbnb’s microservice architecture went through 3 main stages.    Monolith (2008 - 2017) Airbnb began as a simple marketplace for hosts and guests. This is built in a Ruby on Rails application - the monolith. What’s the challenge?  Confusing team ownership + unowned code Slow deployment  Microservices (2017 - 2020) Microservice aims to solve those challenges. In the microservice architecture, key services include:  Data fetching service Business logic data service Write workflow service UI aggregation service Each service had one owning team  What’s the challenge? Hundreds of services and dependencies were difficult for humans to manage. Micro + macroservices (2020 - present) This is what Airbnb is working on now. The micro and macroservice hybrid model focuses on the unification of APIs. Monorepo vs. Microrepo. Which is the best? Why do different companies choose different options?    Monorepo isn't new; Linux and Windows were both created using Monorepo. To improve scalability and build speed, Google developed its internal dedicated toolchain to scale it faster and strict coding quality standards to keep it consistent. Amazon and Netflix are major ambassadors of the Microservice philosophy. This approach naturally separates the service code into separate repositories. It scales faster but can lead to governance pain points later on. Within Monorepo, each service is a folder, and every folder has a BUILD config and OWNERS permission control. Every service member is responsible for their own folder. On the other hand, in Microrepo, each service is responsible for its repository, with the build config and permissions typically set for the entire repository. In Monorepo, dependencies are shared across the entire codebase regardless of your business, so when there's a version upgrade, every codebase upgrades their version. In Microrepo, dependencies are controlled within each repository. Businesses choose when to upgrade their versions based on their own schedules. Monorepo has a standard for check-ins. Google's code review process is famously known for setting a high bar, ensuring a coherent quality standard for Monorepo, regardless of the business. Microrepo can either set its own standard or adopt a shared standard by incorporating best practices. It can scale faster for business, but the code quality might be a bit different. Google engineers built Bazel, and Meta built Buck. There are other open-source tools available, including Nix, Lerna, and others. Over the years, Microrepo has had more supported tools, including Maven and Gradle for Java, NPM for NodeJS, and CMake for C/C++, among others. How will you design the Stack Overflow website? If your answer is on-premise servers and monolith (on the right), you would likely fail the interview, but that's how it is built in reality!    What people think it should look like The interviewer is probably expecting something on the left side.  Microservice is used to decompose the system into small components. Each service has its own database. Use cache heavily. The service is sharded. The services talk to each other asynchronously through message queues. The service is implemented using Event Sourcing with CQRS. Showing off knowledge in distributed systems such as eventual consistency, CAP theorem, etc.  What it actually is Stack Overflow serves all the traffic with only 9 on-premise web servers, and it’s on monolith! It has its own servers and does not run on the cloud. This is contrary to all our popular beliefs these days. Why did Amazon Prime Video monitoring move from serverless to monolithic? How can it save 90% cost? The diagram below shows the architecture comparison before and after the migration.    What is Amazon Prime Video Monitoring Service? Prime Video service needs to monitor the quality of thousands of live streams. The monitoring tool automatically analyzes the streams in real time and identifies quality issues like block corruption, video freeze, and sync problems. This is an important process for customer satisfaction. There are 3 steps: media converter, defect detector, and real-time notification.   What is the problem with the old architecture? The old architecture was based on Amazon Lambda, which was good for building services quickly. However, it was not cost-effective when running the architecture at a high scale. The two most expensive operations are:     The orchestration workflow - AWS step functions charge users by state transitions and the orchestration performs multiple state transitions every second.   Data passing between distributed components - the intermediate data is stored in Amazon S3 so that the next stage can download. The download can be costly when the volume is high.     Monolithic architecture saves 90% cost A monolithic architecture is designed to address the cost issues. There are still 3 components, but the media converter and defect detector are deployed in the same process, saving the cost of passing data over the network. Surprisingly, this approach to deployment architecture change led to 90% cost savings!   This is an interesting and unique case study because microservices have become a go-to and fashionable choice in the tech industry. It's good to see that we are having more discussions about evolving the architecture and having more honest discussions about its pros and cons. Decomposing components into distributed microservices comes with a cost.   What did Amazon leaders say about this? Amazon CTO Werner Vogels: “Building evolvable software systems is a strategy, not a religion. And revisiting your architecture with an open mind is a must.”   Ex Amazon VP Sustainability Adrian Cockcroft: “The Prime Video team had followed a path I call Serverless First…I don’t advocate Serverless Only”. How does Disney Hotstar capture 5 Billion Emojis during a tournament?      Clients send emojis through standard HTTP requests. You can think of Golang Service as a typical Web Server. Golang is chosen because it supports concurrency well. Threads in Golang are lightweight.   Since the write volume is very high, Kafka (message queue) is used as a buffer.   Emoji data are aggregated by a streaming processing service called Spark. It aggregates data every 2 seconds, which is configurable. There is a trade-off to be made based on the interval. A shorter interval means emojis are delivered to other clients faster but it also means more computing resources are needed.   Aggregated data is written to another Kafka.   The PubSub consumers pull aggregated emoji data from Kafka.   Emojis are delivered to other clients in real-time through the PubSub infrastructure. The PubSub infrastructure is interesting. Hotstar considered the following protocols: Socketio, NATS, MQTT, and gRPC, and settled with MQTT.   A similar design is adopted by LinkedIn which streams a million likes/sec. How Discord Stores Trillions Of Messages The diagram below shows the evolution of message storage at Discord:    MongoDB ➡️ Cassandra ➡️ ScyllaDB In 2015, the first version of Discord was built on top of a single MongoDB replica. Around Nov 2015, MongoDB stored 100 million messages and the RAM couldn’t hold the data and index any longer. The latency became unpredictable. Message storage needs to be moved to another database. Cassandra was chosen. In 2017, Discord had 12 Cassandra nodes and stored billions of messages. At the beginning of 2022, it had 177 nodes with trillions of messages. At this point, latency was unpredictable, and maintenance operations became too expensive to run. There are several reasons for the issue:  Cassandra uses the LSM tree for the internal data structure. The reads are more expensive than the writes. There can be many concurrent reads on a server with hundreds of users, resulting in hotspots. Maintaining clusters, such as compacting SSTables, impacts performance. Garbage collection pauses would cause significant latency spikes  ScyllaDB is Cassandra compatible database written in C++. Discord redesigned its architecture to have a monolithic API, a data service written in Rust, and ScyllaDB-based storage. The p99 read latency in ScyllaDB is 15ms compared to 40-125ms in Cassandra. The p99 write latency is 5ms compared to 5-70ms in Cassandra. How do video live streamings work on YouTube, TikTok live, or Twitch? Live streaming differs from regular streaming because the video content is sent via the internet in real-time, usually with a latency of just a few seconds. The diagram below explains what happens behind the scenes to make this possible.    Step 1: The raw video data is captured by a microphone and camera. The data is sent to the server side. Step 2: The video data is compressed and encoded. For example, the compressing algorithm separates the background and other video elements. After compression, the video is encoded to standards such as H.264. The size of the video data is much smaller after this step. Step 3: The encoded data is divided into smaller segments, usually seconds in length, so it takes much less time to download or stream. Step 4: The segmented data is sent to the streaming server. The streaming server needs to support different devices and network conditions. This is called ‘Adaptive Bitrate Streaming.’ This means we need to produce multiple files at different bitrates in steps 2 and 3. Step 5: The live streaming data is pushed to edge servers supported by CDN (Content Delivery Network.) Millions of viewers can watch the video from an edge server nearby. CDN significantly lowers data transmission latency. Step 6: The viewers’ devices decode and decompress the video data and play the video in a video player. Steps 7 and 8: If the video needs to be stored for replay, the encoded data is sent to a storage server, and viewers can request a replay from it later. Standard protocols for live streaming include:  RTMP (Real-Time Messaging Protocol): This was originally developed by Macromedia to transmit data between a Flash player and a server. Now it is used for streaming video data over the internet. Note that video conferencing applications like Skype use RTC (Real-Time Communication) protocol for lower latency. HLS (HTTP Live Streaming): It requires the H.264 or H.265 encoding. Apple devices accept only HLS format. DASH (Dynamic Adaptive Streaming over HTTP): DASH does not support Apple devices. Both HLS and DASH support adaptive bitrate streaming.  License This work is licensed under CC BY-NC-ND 4.0   </readme><commitcount>8</commitcount><languages /><tags /><about>Explain complex systems using visuals and simple terms. Help you prepare for system design interviews.</about><starcount>0</starcount><watchcount>0</watchcount></repository><repository><username>regchiu</username><reponame>deno-docs</reponame><readme>               Deno Docs Local development Editing content Versioning docs content Including version numbers in code and content Server-side code and redirects New release process for Deno runtime Contribution Special thanks for historical contributions Deployment License      README.md     Deno Docs This repository contains the website running docs.deno.com. The intent of this project is to eventually centralize all official Deno documentation content in a single website. The Deno Docs site is built using Docusaurus 2, a static site generator optimized for documentation websites. The docs.deno.com website is hosted on Deno Deploy, where it is fronted by a Hono web server that handles redirects and other dynamic content requests as they become necessary. Local development Since Docusaurus is built and maintained using Node.js, it is recommended to have Node.js and npm installed for local development. Once Node and npm are installed, install Docusaurus' dependencies with: npm install  You can then start the local development server with: npm start  This will launch a browser window open to localhost:3000, where you will see any doc content changes you make update live. To test the generated static site in a production configuration, run: npm run build  This will generate a static site to the build folder locally. To test the production server (through the actual Deno / Hono server), run this command: npm run serve  This will start a Deno server on localhost:8000, where you can preview the site as it will run on Deno Deploy. Sometimes, after making a Docusaurus config change, you will run into an error and need to clean Docusaurus' generated assets. You can do this by running: npm run clear  This will solve most errors you encounter while refactoring the site. Static assets will be rebuilt from scratch the next time you run npm run build or npm start. Editing content The actual content of the docs site is found mostly in these three folders:  runtime - docs for the Deno CLI / runtime deploy - docs for the Deno Deploy cloud service kv - docs for Deno KV, Deno's integrated database  Most files are markdown, but even markdown files are processed with MDX, which enables you to use JSX syntax within your markdown files. Left navigation for the different doc sections are configured in one of these files:  sidebars/runtime.js - sidebar config for the Runtime section sidebars/deploy.js - sidebar config for the Deno Deploy section sidebars/kv.js - sidebar config for the KV section  Static files (like screenshots) can be included directly in the runtime, deploy, or kv folders, and referenced by relative URLs in your markdown. Docusaurus provides a number of nice extensions to markdown you might want to use, like tabs, admonitions, and code blocks. Refer to the Docusaurus docs for more details. Versioning docs content Philosophically, we want to maintain as few discrete versions of the documentation as possible. This will reduce confusion for users (reduce the number of versions they need to think about), improve search indexing, and help us maintain the docs by keeping our build times faster. In general, we should only version the documentation when we want to concurrently maintain several versions of the docs, like for major/LTS versions. For example - the Node.js docs are only versioned for major releases, like 20.x and 19.x. We will adopt this pattern as well, and won't have versioned docs for patch or feature releases. For additive changes, it should usually be sufficient to indicate which version a feature or API was released in. For example - in the Node 20 docs, the register function is marked as being added in version 20.6.0. When we do want to maintain versioned docs for major releases, we currently plan to use Docusaurus versions. Including version numbers in code and content It may occasionally be desirable to dynamically include the current Deno CLI or standard library version in content or code samples. We can accomplish this using the replacements.json file at the root of this repository. Any values you would like to change once, and then have appear dynamically in a number of generated files, should be included in replacements.json. In code samples (fenced with backticks), you can include a $ character, followed by the replacement variable name, directly within the code sample. When the markdown is transformed, the current version number will be replaced within it. import { copy } from "https://deno.land/std@$STD_VERSION/fs/copy.ts"; To include version number in markdown / MDX content, we recommend using the &lt;Replacement /&gt; component: import Replacement from "@site/src/components/Replacement";  The current CLI version is **&lt;Replacement for="CLI_VERSION"/&gt;**. If you are writing inline JSX, you can also use the replacements object directly like so: import { replacements } from "@site/src/components/Replacement";  &lt;p&gt;   The current CLI version is &lt;code&gt;{ replacements.CLI_VERSION }&lt;/code&gt;. &lt;/p&gt; Server-side code and redirects The Deno code that serves the site in production is in the src-deno folder. When the npm run build command is executed for a production Docusaurus build, it also copies the contents of the src-deno folder (unchanged) into the resulting build folder, which will be our project root for Deno Deploy. Right now, there is just a very thin Hono server sitting on top of the static assets generated by Docusaurus. The only interesting job the Hono app has right now is handling redirects, of which there are several from the previous Deno doc sites. To add a redirect, open src-deno/redirects.ts and configure a new route in the default exported function. The default status code of 301 should be sufficient for most cases. New release process for Deno runtime Let's say that a new minor release is ready for Deno, with CLI version 1.99 and standard library version 0.999.0. Here's how I would recommend approaching the docs for this release right now.  Create a feature branch for the release, like release_1_99 or similar Update replacements.json with the upcoming CLI and standard lib versions As the release is developed, add docs changes to this branch When the release is ready, submit a PR to the main branch from this feature branch When the branch is merged, create a v1.99 tag from the new main branch  For patch releases, I would recommend simply submitting pull requests to the main branch with relevant updates to replacements.json as required. If we decide we'd like to have "canary" docs for upcoming versions, we can discuss how to make that possible with Docusaurus versions. Contribution We are very grateful for any help you can offer to improve Deno's documentation! For any small copy changes or fixes, please feel free to submit a pull request directly to the main branch of this repository. For larger changes, please create a GitHub issue first to describe your proposed updates. It will be better to get feedback on your concept first before going to the trouble of writing a large number of docs! Over time, we will add more in the way of linting and formatting to the pull request process. But for now, you should merely ensure that npm run build succeeds without error before submitting a pull request. This will ensure that there are no broken links or invalid MDX syntax in the content you have authored. Special thanks for historical contributions This repository was created using content from the Deno Manual, a project contributed to by hundreds of developers since 2018. You can view a list of historical contributors to the Deno documentation in this repository and the manual with this command: git shortlog -s -n  Deployment The docs.deno.com site is updated with every push to the main branch, which should be done via pull request to this repository. License MIT   </readme><commitcount>951</commitcount><languages /><tags /><about>Docusaurus site for a unified Deno docs experience</about><starcount>0</starcount><watchcount>0</watchcount></repository><repository><username>nocodb</username><reponame>nocodb</reponame><readme>                            NocoDB              The Open Source Airtable Alternative  Join Our Team Join Our Community Quick try Docker Binaries MacOS (x64) MacOS (arm64) Linux (x64) Linux (arm64) Windows (x64) Windows (arm64) Docker Compose NPX Node Application GUI Screenshots Table of Contents Features Rich Spreadsheet Interface App Store for Workflow Automations Programmatic Access Sync Schema Audit Production Setup Environment variables Development Setup Contributing Why are we building this? Our Mission License Contributors      README.md                      NocoDB               The Open Source Airtable Alternative    Turns any MySQL, PostgreSQL, SQL Server, SQLite &amp; MariaDB into a smart spreadsheet.       Website •     Discord •     Community •     Twitter •     Reddit •     Documentation              See other languages »  Join Our Team  Join Our Community     Quick try Docker # for SQLite docker run -d --name nocodb \ -v "$(pwd)"/nocodb:/usr/app/data/ \ -p 8080:8080 \ nocodb/nocodb:latest  # for MySQL docker run -d --name nocodb-mysql \ -v "$(pwd)"/nocodb:/usr/app/data/ \ -p 8080:8080 \ -e NC_DB="mysql2://host.docker.internal:3306?u=root&amp;p=password&amp;d=d1" \ -e NC_AUTH_JWT_SECRET="569a1821-0a93-45e8-87ab-eb857f20a010" \ nocodb/nocodb:latest  # for PostgreSQL docker run -d --name nocodb-postgres \ -v "$(pwd)"/nocodb:/usr/app/data/ \ -p 8080:8080 \ -e NC_DB="pg://host.docker.internal:5432?u=root&amp;p=password&amp;d=d1" \ -e NC_AUTH_JWT_SECRET="569a1821-0a93-45e8-87ab-eb857f20a010" \ nocodb/nocodb:latest  # for MSSQL docker run -d --name nocodb-mssql \ -v "$(pwd)"/nocodb:/usr/app/data/ \ -p 8080:8080 \ -e NC_DB="mssql://host.docker.internal:1433?u=root&amp;p=password&amp;d=d1" \ -e NC_AUTH_JWT_SECRET="569a1821-0a93-45e8-87ab-eb857f20a010" \ nocodb/nocodb:latest  To persist data in docker you can mount volume at /usr/app/data/ since 0.10.6. Otherwise your data will be lost after recreating the container.   If you plan to input some special characters, you may need to change the character set and collation yourself when creating the database. Please check out the examples for MySQL Docker.  Binaries MacOS (x64) curl http://get.nocodb.com/macos-x64 -o nocodb -L &amp;&amp; chmod +x nocodb &amp;&amp; ./nocodb MacOS (arm64) curl http://get.nocodb.com/macos-arm64 -o nocodb -L &amp;&amp; chmod +x nocodb &amp;&amp; ./nocodb Linux (x64) curl http://get.nocodb.com/linux-x64 -o nocodb -L &amp;&amp; chmod +x nocodb &amp;&amp; ./nocodb Linux (arm64) curl http://get.nocodb.com/linux-arm64 -o nocodb -L &amp;&amp; chmod +x nocodb &amp;&amp; ./nocodb Windows (x64) iwr http://get.nocodb.com/win-x64.exe -o Noco-win-x64.exe .\Noco-win-x64.exe Windows (arm64) iwr http://get.nocodb.com/win-arm64.exe -o Noco-win-arm64.exe .\Noco-win-arm64.exe Docker Compose We provide different docker-compose.yml files under this directory. Here are some examples. git clone https://github.com/nocodb/nocodb # for MySQL cd nocodb/docker-compose/mysql # for PostgreSQL cd nocodb/docker-compose/pg # for MSSQL cd nocodb/docker-compose/mssql docker-compose up -d  To persist data in docker, you can mount volume at /usr/app/data/ since 0.10.6. Otherwise your data will be lost after recreating the container.   If you plan to input some special characters, you may need to change the character set and collation yourself when creating the database. Please check out the examples for MySQL Docker Compose.  NPX You can run the below command if you need an interactive configuration. npx create-nocodb-app   Node Application We provide a simple NodeJS Application for getting started. git clone https://github.com/nocodb/nocodb-seed cd nocodb-seed npm install npm start GUI Access Dashboard using: http://localhost:8080/dashboard Screenshots             Table of Contents  Quick try  NPX Node Application Docker Docker Compose   GUI Join Our Community Screenshots Table of Contents Features  Rich Spreadsheet Interface App Store for Workflow Automations Programmatic Access Sync Schema Audit   Production Setup  Environment variables   Development Setup Contributing Why are we building this? Our Mission License Contributors  Features Rich Spreadsheet Interface  ⚡  Basic Operations: Create, Read, Update and Delete Tables, Columns, and Rows ⚡  Fields Operations: Sort, Filter, Hide / Unhide Columns ⚡  Multiple Views Types: Grid (By default), Gallery, Form View and Kanban View ⚡  View Permissions Types: Collaborative Views, &amp; Locked Views ⚡  Share Bases / Views: either Public or Private (with Password Protected) ⚡  Variant Cell Types: ID, LinkToAnotherRecord, Lookup, Rollup, SingleLineText, Attachment, Currency, Formula, etc ⚡  Access Control with Roles: Fine-grained Access Control at different levels ⚡  and more ...  App Store for Workflow Automations We provide different integrations in three main categories. See App Store for details.  ⚡  Chat: Slack, Discord, Mattermost, and etc ⚡  Email: AWS SES, SMTP, MailerSend, and etc ⚡  Storage: AWS S3, Google Cloud Storage, Minio, and etc  Programmatic Access We provide the following ways to let users programmatically invoke actions. You can use a token (either JWT or Social Auth) to sign your requests for authorization to NocoDB.  ⚡  REST APIs ⚡  NocoDB SDK  Sync Schema We allow you to sync schema changes if you have made changes outside NocoDB GUI. However, it has to be noted then you will have to bring your own schema migrations for moving from one environment to another. See Sync Schema for details. Audit We are keeping all the user operation logs in one place. See Audit for details. Production Setup By default, SQLite is used for storing metadata. However, you can specify your database. The connection parameters for this database can be specified in NC_DB environment variable. Moreover, we also provide the below environment variables for configuration. Environment variables Please refer to the Environment variables Development Setup Please refer to Development Setup Contributing Please refer to Contribution Guide. Why are we building this? Most internet businesses equip themselves with either spreadsheet or a database to solve their business needs. Spreadsheets are used by Billion+ humans collaboratively every single day. However, we are way off working at similar speeds on databases which are way more powerful tools when it comes to computing. Attempts to solve this with SaaS offerings have meant horrible access controls, vendor lock-in, data lock-in, abrupt price changes &amp; most importantly a glass ceiling on what's possible in the future. Our Mission Our mission is to provide the most powerful no-code interface for databases that is open source to every single internet business in the world. This would not only democratise access to a powerful computing tool but also bring forth a billion+ people who will have radical tinkering-and-building abilities on the internet. License  This project is licensed under AGPLv3.  Contributors Thank you for your contributions! We appreciate all the contributions from the community.      </readme><commitcount>17,570</commitcount><languages>TypeScript 69.5%	Vue 19.5%	JavaScript 8.8%	PLpgSQL 1.2%	SCSS 0.3%	CSS 0.3%</languages><tags>mysql	rest-api	sqlite	postgresql	swagger	spreadsheet	admin-dashboard	mariadb	admin-ui	airtable	sqlserver	restful-api	hacktoberfest	low-code	no-code	automatic-api	no-code-platform	no-code-database	airtable-alternative</tags><about>🔥 🔥 🔥 Open Source Airtable Alternative</about><starcount>38.2k</starcount><watchcount>0</watchcount></repository><repository><username>cli</username><reponame>cli</reponame><readme>               GitHub CLI Documentation Contributing Installation macOS Homebrew MacPorts Conda Spack Linux &amp; BSD Windows WinGet scoop Chocolatey Signed MSI Codespaces GitHub Actions Other platforms Build from source Comparison with hub      README.md     GitHub CLI gh is GitHub on the command line. It brings pull requests, issues, and other GitHub concepts to the terminal next to where you are already working with git and your code.  GitHub CLI is available for repositories hosted on GitHub.com and GitHub Enterprise Server 2.20+, and to install on macOS, Windows, and Linux. Documentation For installation options see below, for usage instructions see the manual. Contributing If anything feels off, or if you feel that some functionality is missing, please check out the contributing page. There you will find instructions for sharing your feedback, building the tool locally, and submitting pull requests to the project. If you are a hubber and are interested in shipping new commands for the CLI, check out our doc on internal contributions. Installation macOS gh is available via Homebrew, MacPorts, Conda, Spack, and as a downloadable binary from the releases page. Homebrew    Install: Upgrade:     brew install gh brew upgrade gh    MacPorts    Install: Upgrade:     sudo port install gh sudo port selfupdate &amp;&amp; sudo port upgrade gh    Conda    Install: Upgrade:     conda install gh --channel conda-forge conda update gh --channel conda-forge    Additional Conda installation options available on the gh-feedstock page. Spack    Install: Upgrade:     spack install gh spack uninstall gh &amp;&amp; spack install gh    Linux &amp; BSD gh is available via:  our Debian and RPM repositories; community-maintained repositories in various Linux distros; OS-agnostic package managers such as Homebrew, Conda, and Spack; and our releases page as precompiled binaries.  For more information, see Linux &amp; BSD installation. Windows gh is available via WinGet, scoop, Chocolatey, Conda, and as downloadable MSI. WinGet    Install: Upgrade:     winget install --id GitHub.cli winget upgrade --id GitHub.cli    Note The Windows installer modifies your PATH. When using Windows Terminal, you will need to open a new window for the changes to take effect. (Simply opening a new tab will not be sufficient.) scoop    Install: Upgrade:     scoop install gh scoop update gh    Chocolatey    Install: Upgrade:     choco install gh choco upgrade gh    Signed MSI MSI installers are available for download on the releases page. Codespaces To add GitHub CLI to your codespace, add the following to your devcontainer file: "features": {   "ghcr.io/devcontainers/features/github-cli:1": {} } GitHub Actions GitHub CLI comes pre-installed in all GitHub-Hosted Runners. Other platforms Download packaged binaries from the releases page. Build from source See here on how to build GitHub CLI from source. Comparison with hub For many years, hub was the unofficial GitHub CLI tool. gh is a new project that helps us explore what an official GitHub CLI tool can look like with a fundamentally different design. While both tools bring GitHub to the terminal, hub behaves as a proxy to git, and gh is a standalone tool. Check out our more detailed explanation to learn more.   </readme><commitcount>6,963</commitcount><languages>Go 99.7%</languages><tags>git	cli	golang	github-api-v4</tags><about>GitHub’s official command line tool</about><starcount>33.5k</starcount><watchcount>0</watchcount></repository><repository><username>wingkwong</username><reponame>leetcode-the-hard-way</reponame><readme>               Welcome! About Background Grinding for Interviews? Contributing Discord Community Sponsorship and Advertisement Outro      README.md     Welcome!  About Welcome to "LeetCode The Hard Way," the ultimate resource for those looking to improve their skills in data structures and algorithms! Our website is dedicated to providing comprehensive tutorials and detailed solutions to some of the most challenging problems in LeetCode. Whether you're a student looking to ace your next exam or a professional seeking to level up your coding skills, our tutorials and solutions are designed to help you succeed. Our content covers a range of popular programming languages, so you can learn and practice using the language of your choice. From basic data structures to advanced topics, we've got you covered. At "LeetCode The Hard Way," we believe that learning should be fun and engaging. With plenty of LC problems and examples to work through, you'll have the opportunity to apply what you've learned and test your skills. So why wait? Start exploring our tutorials and solutions today, and take the first step towards becoming a DSA expert! Background LeetCode is a platform that offers a collection of coding problems and challenges that are designed to help developers improve their skills in data structures and algorithms. Initially, this project was created for my own personal education, however now it is publicly available and open for anyone to use. Grinding for Interviews? AlgoMonster is the key to unlock a successful technical interview. It won't dump a bunch of problems on you, but help you learn the key patterns necessary to solve any interview question. This focused approach will help you gain systematic knowledge, prepare in less time and stop the endless grinding of random problems. AlgoMonster is flexible so you can learn on your own terms through a highly-interactive format. Be more confident as you walk into that interview! Contributing Contributions are welcome. Please see CONTRIBUTING.md.  Discord Community https://discord.com/invite/Nqm4jJcyBf Sponsorship and Advertisement Our website provides free and comprehensive tutorials and solutions. As a free resource, we rely on the support of our community and advertisers to continue providing high-quality content. By sponsoring "LeetCode The Hard Way," you'll have the opportunity to connect with our audience through targeted advertisements and brand placement, while also supporting a valuable resource for programmers everywhere. If you are interested in sponsoring and advertising with us, please email us at here. Outro GLHF.   </readme><commitcount>1,977</commitcount><languages>JavaScript 72.2%	CSS 25.5%	MDX 1.2%	HTML 1.1%</languages><tags>javascript	python	golang	computer-science	algorithm	programming	algorithms	leetcode	cpp	data-structures	software-engineering	leetcode-solutions	problem-solving	hacktoberfest	coding-challenges	interview-preparation	technical-interviews	leetcode-tutorials	algo-monster</tags><about>LeetCode The Hard Way - From Absolute Beginner to Quitter. Join Discord: https://discord.com/invite/Nqm4jJcyBf</about><starcount>745</starcount><watchcount>0</watchcount></repository><repository><username>wingkwong</username><reponame>react-quiz-component</reponame><readme>               react-quiz-component Features Installing Importing react-quiz-component Defining Your Quiz Source Locale Customization Passing to Quiz container Shuffling Question Set Shuffling Answer Set Disabling Default Result Page Enabling Custom Result Page Enabling onComplete Action Example of Quiz Summary returned to customResultPage and onComplete Showing Instant Feedback Answering the same question till it is correct Props Contribution Demo License      README.md     react-quiz-component 📙 React Quiz Component    react-quiz-component is a ReactJS component allowing users to attempt a quiz. Features  JSON-based input Quiz landing page showing title, synopsis and number of questions Quiz Input Validator Multiple answers with single correct answer Multiple answers with multiple correct answers Support text and photo answers Continue till answered correctly Show explainations when answered correctly or not Quiz result page at the end with the dropdown filtering all questions or only those you answered correctly or incorrectly Support custom result page Return quiz summary at the page Allow Instant feedback Allow retry until the answer is selected correctly Allow markdown in Question Allow Picture in Question Scoring System Shuffling Questions / Answers  Installing npm i react-quiz-component  Importing react-quiz-component import Quiz from 'react-quiz-component'; Defining Your Quiz Source The quiz source is a JSON object. You can use react-quiz-form to generate it. export const quiz =  {   "quizTitle": "React Quiz Component Demo",   "quizSynopsis": "Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim",   "nrOfQuestions": "4",   "questions": [     {       "question": "How can you access the state of a component from inside of a member function?",       "questionType": "text",       "questionPic": "https://dummyimage.com/600x400/000/fff&amp;text=X", // if you need to display Picture in Question       "answerSelectionType": "single",       "answers": [         "this.getState()",         "this.prototype.stateValue",         "this.state",         "this.values"       ],       "correctAnswer": "3",       "messageForCorrectAnswer": "Correct answer. Good job.",       "messageForIncorrectAnswer": "Incorrect answer. Please try again.",       "explanation": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",       "point": "20"     },     {       "question": "ReactJS is developed by _____?",       "questionType": "text",       "answerSelectionType": "single",       "answers": [         "Google Engineers",         "Facebook Engineers"       ],       "correctAnswer": "2",       "messageForCorrectAnswer": "Correct answer. Good job.",       "messageForIncorrectAnswer": "Incorrect answer. Please try again.",       "explanation": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",       "point": "20"     },     {       "question": "ReactJS is an MVC based framework?",       "questionType": "text",       "answerSelectionType": "single",       "answers": [         "True",         "False"       ],       "correctAnswer": "2",       "messageForCorrectAnswer": "Correct answer. Good job.",       "messageForIncorrectAnswer": "Incorrect answer. Please try again.",       "explanation": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",       "point": "10"     },     {       "question": "Which of the following concepts is/are key to ReactJS?",       "questionType": "text",       "answerSelectionType": "single",       "answers": [         "Component-oriented design",         "Event delegation model",         "Both of the above",       ],       "correctAnswer": "3",       "messageForCorrectAnswer": "Correct answer. Good job.",       "messageForIncorrectAnswer": "Incorrect answer. Please try again.",       "explanation": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",       "point": "30"     },     {       "question": "Lorem ipsum dolor sit amet, consectetur adipiscing elit,",       "questionType": "photo",       "answerSelectionType": "single",       "answers": [         "https://dummyimage.com/600x400/000/fff&amp;text=A",         "https://dummyimage.com/600x400/000/fff&amp;text=B",         "https://dummyimage.com/600x400/000/fff&amp;text=C",         "https://dummyimage.com/600x400/000/fff&amp;text=D"       ],       "correctAnswer": "1",       "messageForCorrectAnswer": "Correct answer. Good job.",       "messageForIncorrectAnswer": "Incorrect answer. Please try again.",       "explanation": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",       "point": "20"     },     {       "question": "What are the advantages of React JS?",       "questionType": "text",       "answerSelectionType": "multiple",       "answers": [         "React can be used on client and as well as server side too",         "Using React increases readability and makes maintainability easier. Component, Data patterns improves readability and thus makes it easier for manitaining larger apps",         "React components have lifecycle events that fall into State/Property Updates",         "React can be used with any other framework (Backbone.js, Angular.js) as it is only a view layer"       ],       "correctAnswer": [1, 2, 4],       "messageForCorrectAnswer": "Correct answer. Good job.",       "messageForIncorrectAnswer": "Incorrect answer. Please try again.",       "explanation": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",       "point": "20"     },   ] }  Locale Customization If you want to use your customized text, you can add appLocale into your quiz source. Below is the default one.  and  will be replaced dynamically. "appLocale": {   "landingHeaderText": "&lt;questionLength&gt; Questions",   "question": "Question",   "startQuizBtn": "Start Quiz",   "resultFilterAll": "All",   "resultFilterCorrect": "Correct",   "resultFilterIncorrect": "Incorrect",   "prevQuestionBtn": "Prev",   "nextQuestionBtn": "Next",   "resultPageHeaderText": "You have completed the quiz. You got &lt;correctIndexLength&gt; out of &lt;questionLength&gt; questions." }  Passing to Quiz container import { quiz } from './quiz'; ... &lt;Quiz quiz={quiz}/&gt; Shuffling Question Set import { quiz } from './quiz'; ... &lt;Quiz quiz={quiz} shuffle={true}/&gt; Shuffling Answer Set import { quiz } from './quiz'; ... &lt;Quiz quiz={quiz} shuffleAnswer={true}/&gt; Disabling Default Result Page import { quiz } from './quiz'; ... &lt;Quiz quiz={quiz} showDefaultResult={false}/&gt; Enabling Custom Result Page  In order to enable custom result page, showDefaultResult has to be false.  import { quiz } from './quiz'; ... const renderCustomResultPage = (obj) =&gt; {   console.log(obj);   return (     &lt;div&gt;       This is a custom result page. You can use obj to render your custom result page     &lt;/div&gt;   ) }  ``` Enabling onComplete Action import { quiz } from './quiz'; ... const setQuizResult = (obj) =&gt; {   console.log(obj);   // YOUR LOGIC GOES HERE } ... &lt;Quiz quiz={quiz} showDefaultResult={false} onComplete={setQuizResult}/&gt; Example of Quiz Summary returned to customResultPage and onComplete Object   numberOfCorrectAnswers: 4   numberOfIncorrectAnswers: 1   numberOfQuestions: 5   questions: Array(5)     0: {question: "Which of the following concepts is/are key to ReactJS?", questionType: "text", answers: Array(3), correctAnswer: "3", messageForCorrectAnswer: "Correct answer. Good job.", …}     1: {question: "ReactJS is developed by _____?", questionType: "text", answers: Array(2), correctAnswer: "2", messageForCorrectAnswer: "Correct answer. Good job.", …}     2: {question: "How can you access the state of a component from inside of a member function?", questionType: "text", answers: Array(4), correctAnswer: "3", messageForCorrectAnswer: "Correct answer. Good job.", …}     3: {question: "Lorem ipsum dolor sit amet, consectetur adipiscing elit,", questionType: "photo", answers: Array(4), correctAnswer: "1", messageForCorrectAnswer: "Correct answer. Good job.", …}     4: {question: "ReactJS is an MVC based framework?", questionType: "text", answers: Array(2), correctAnswer: "2", messageForCorrectAnswer: "Correct answer. Good job.", …}   userInput: (5) [1, 2, 1, 2, 3]   totalPoints: 100   correctPoints: 40  Showing Instant Feedback import { quiz } from './quiz'; ... &lt;Quiz quiz={quiz} showInstantFeedback={true}/&gt; Answering the same question till it is correct import { quiz } from './quiz'; ... &lt;Quiz quiz={quiz} continueTillCorrect={true}/&gt; Props    Name Type Default Required Description     quiz object null Y Quiz Json Object   shuffle boolean false N Shuffle the questions   shuffleAnswer boolean false N Shuffle the answers   showDefaultResult boolean true N Show the default result page   customResultPage function null N A quiz summary object will be returned to the function and users can use it to render its custom result page   onComplete function null N A quiz summary object will be returned to the function   showInstantFeedback boolean false N Show instant feedback when it is true   continueTillCorrect boolean false N Continue to select an answer until it is correct   onQuestionSubmit function null N A user response for a question will be returned   disableSynopsis boolean false N Disable synopsis before quiz    Contribution  Clone the repository Run npm install Run npm run dev Run npm run lint Make a PR to develop and describe the changes  Demo The demo is available at https://wingkwong.github.io/react-quiz-component/ License This project is licensed under the MIT License - see the LICENSE file for details   </readme><commitcount>239</commitcount><languages>JavaScript 90.6%	CSS 8.5%	HTML 0.9%</languages><tags>react	quiz	hacktoberfest	react-plugin	react-quiz	react-quiz-component</tags><about>📙 React Quiz Component</about><starcount>205</starcount><watchcount>0</watchcount></repository><repository><username>wingkwong</username><reponame>geodesy</reponame><readme>               Geodesy About How to Use Geodesy Commands For Dart For Flutter Import Geodesy Class Static Methods License Contributors      README.md     Geodesy  About A Dart library for implementing geodesic and trigonometric calculations based on a spherical Earth model for working with points and paths such as distances, bearings and destinations. How to Use Geodesy Commands For Dart dart pub add geodesy For Flutter flutter pub add geodesy Import Geodesy import 'package:geodesy/geodesy.dart'; Class The Geodesy class provides a collection of methods for performing various geodetic calculations, including distance calculations, point intersections, and more. This class is designed to work with geographical coordinates in the form of latitude and longitude. Please see the Usage and Example. Static Methods Static methods are available without using Geodesy instance. Please see the Usage and Example. License This project is licensed under MIT. Contributors Thank you for your contributions! We appreciate all the contributions from the community.      </readme><commitcount>122</commitcount><languages>Dart 100.0%</languages><tags>dart	geometry	distance	geodesy	geography	hacktoberfest	distance-calculation	latitude-and-longitude	coordinate	bearing	geodesy-functions	geodetic-point	trigonometric-calculation</tags><about>A Dart library for geodesic and trigonometric calculations working with points and paths</about><starcount>41</starcount><watchcount>0</watchcount></repository><repository><username>watermarkify</username><reponame>vue-watermark</reponame><readme>            @watermarkify/vue-watermark Features Installation Usage Options License Contributing      README.md     @watermarkify/vue-watermark    @watermarkify/vue-watermark is a lightweight and customizable Vue.js component that allows you to easily add watermarks to your web applications. With Vue Watermark, you can overlay images or text on top of images, videos, or any other HTML elements to protect your content or add branding.    Features  Simple integration: Easily add watermarks to your Vue.js applications with just a few lines of code. Customizable options: Customize the watermark's appearance, position, size, and more to suit your needs. Support for images and text: Add watermark images or text, and control various properties such as font style, color, and size. Responsive design: The watermark adapts to different screen sizes and device orientations.  Installation You can install @watermarkify/vue-watermark via npm or yarn: npm install @watermarkify/vue-watermark # or yarn add @watermarkify/vue-watermark # or  pnpm install @watermarkify/vue-watermark Usage The following code snippet is retrieved from playground. You may also try the interactive playground here. &lt;script setup lang="ts"&gt; // step 1: import Watermark from @watermarkify/vue-watermark import { Watermark } from '@watermarkify/vue-watermark'  // step 2: define watermark options // see https://github.com/watermarkify/vue-watermark#options const watermarkOptions = ref({   content: 'watermark',   gap: [20, 20],   offset: [10, 10],   zIndex: 5,   rotate: -20, }) &lt;/script&gt; Pass watermarkOptions to options and define your slot. &lt;Watermark :options="watermarkOptions"&gt;   &lt;div&gt;This is the content of the slot.&lt;/div&gt; &lt;/Watermark&gt; Options    Property Description Type Default Value     width The width of the watermark in pixels. If not specified, defaults to the width of the container element. number 120   height The height of the watermark in pixels. If not specified, defaults to the height of the container element. number 64   content The text content to be used as the watermark. Can be a string or an array of strings. If image is provided, this property will be ignored. string or string[] undefined   gap The gap between each instance of the watermark in pixels. Can be an array with two values for horizontal and vertical gap, respectively. [number, number] [20, 20]   offset The offset of the watermark from the top-left corner of the container element in pixels. Can be an array with two values for horizontal and vertical offset, respectively. [number, number] [gap[0]/2, gap[1]/2]   zIndex The z-index of the watermark relative to other elements on the page. number 5   rotate The rotation angle in degrees of the watermark. Can be a value in the range [-180, 180]. number -20   font The font properties for the watermark text, including family, size, style, weight, and color. WatermarkFont { family: "sans-serif", size: 18, style: "normal", weight: "normal", color: "rgba(0,0,0,.2)" }    License @watermarkify/vue-watermark is licensed under the MIT License. See the LICENSE file for more information. Contributing Contributions are welcome! To contribute to @watermarkify/vue-watermark, please fork the repository and submit a pull request.   </readme><commitcount>148</commitcount><languages>TypeScript 55.8%	Vue 35.6%	CSS 5.4%	JavaScript 1.7%	HTML 1.5%</languages><tags>vuejs	typescript	hacktoberfest	watermark	vuejs-components	image-protection	vue3	vue-watermark</tags><about>Vue Watermark ◉ 워터마크 ◉ 水印</about><starcount>13</starcount><watchcount>0</watchcount></repository><repository><username>nocodb</username><reponame>nocodb-seed</reponame><readme>            Clone &amp; install Start To Upgrade Using environment file      README.md      Clone &amp; install git clone https://github.com/nocodb/nocodb-seed  cd nocodb-seed   npm install  Start npm run start  To Upgrade npm run upgrade  Using environment file Copy the .env.example file to .env and change as needed.   </readme><commitcount>52</commitcount><languages>JavaScript 100.0%</languages><tags /><about>Vue Watermark ◉ 워터마크 ◉ 水印</about><starcount>36</starcount><watchcount>0</watchcount><tokens>, C#, Ruby, Fluent, Astro, Smarty, ASP.NET, HCL, Lua, Assembly, Clojure, Perl, Handlebars, PowerShell, MDX, Svelte, Jinja, F#, Solidity, Jupyter, PLpgSQL, JavaScript, C++, Bicep, Dockerfile, HLSL, Go, Shell, CSS, Starlark, Python, Java, Swift, SCSS, Batchfile, HTML, C, Scala, Groovy, TypeScript, Rust, PHP, Dart, XSLT, ShaderLab, Vue, Standard, Makefile, Mustache</tokens></repository></root>